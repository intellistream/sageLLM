# Control Plane Quick Start Guide

## 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### 1. å¯¼å…¥æ‰€éœ€æ¨¡å—

```python
import asyncio
from vllm.control_plane import (
    ControlPlaneManager,
    ExecutionInstance,
    RequestMetadata,
    RequestPriority,
)
```

### 2. åˆ›å»ºControl Planeå®ä¾‹

```python
# ä½¿ç”¨è‡ªé€‚åº”è°ƒåº¦ç­–ç•¥ï¼ˆæ¨èï¼‰
cp = ControlPlaneManager(
    scheduling_policy="adaptive",      # è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç­–ç•¥
    routing_strategy="load_balanced",  # è´Ÿè½½å‡è¡¡è·¯ç”±
    enable_monitoring=True,            # å¯ç”¨ç›‘æ§
)
```

### 3. æ³¨å†ŒvLLMå®ä¾‹

```python
# åˆ›å»ºç¬¬ä¸€ä¸ªå®ä¾‹ï¼ˆ4 GPUå¼ é‡å¹¶è¡Œï¼‰
instance1 = ExecutionInstance(
    instance_id="vllm-tp4",
    host="localhost",
    port=8000,
    model_name="llama-3-70b",
    tensor_parallel_size=4,
    gpu_count=4,
    gpu_memory_gb=80.0,
    max_concurrent_requests=100,
)
cp.register_instance(instance1)

# åˆ›å»ºç¬¬äºŒä¸ªå®ä¾‹ï¼ˆæ··åˆå¹¶è¡Œï¼‰
instance2 = ExecutionInstance(
    instance_id="vllm-hybrid",
    host="localhost",
    port=8001,
    model_name="llama-3-70b",
    tensor_parallel_size=2,
    pipeline_parallel_size=2,
    gpu_count=4,
    gpu_memory_gb=80.0,
    max_concurrent_requests=80,
)
cp.register_instance(instance2)
```

### 4. å¯åŠ¨Control Plane

```python
async def main():
    # å¯åŠ¨åå°ä»»åŠ¡
    await cp.start()
    print("âœ“ Control Plane started")
```

### 5. æäº¤æ¨ç†è¯·æ±‚

```python
    # åˆ›å»ºé«˜ä¼˜å…ˆçº§è¯·æ±‚
    request = RequestMetadata(
        request_id="req-001",
        user_id="user-123",
        priority=RequestPriority.HIGH,
        slo_deadline_ms=1000,      # 1ç§’SLO
        max_tokens=100,
        model_name="llama-3-70b",
    )
    
    # æäº¤è¯·æ±‚
    await cp.submit_request(request)
    print("âœ“ Request submitted")
```

### 6. ç›‘æ§æ€§èƒ½

```python
    # ç­‰å¾…å¤„ç†
    await asyncio.sleep(2)
    
    # è·å–æŒ‡æ ‡
    metrics = cp.get_metrics()
    print(f"Completed: {metrics.completed_requests}")
    print(f"Failed: {metrics.failed_requests}")
    print(f"Avg Latency: {metrics.avg_latency_ms:.2f}ms")
    print(f"SLO Compliance: {metrics.slo_compliance_rate:.2%}")
    
    # è·å–ç³»ç»ŸçŠ¶æ€
    status = cp.get_status()
    print(f"Running: {status['running_requests']}")
    print(f"Queued: {status['pending_requests']}")
```

### 7. åœæ­¢Control Plane

```python
    # åœæ­¢æ‰€æœ‰åå°ä»»åŠ¡
    await cp.stop()
    print("âœ“ Control Plane stopped")

# è¿è¡Œ
asyncio.run(main())
```

## å®Œæ•´ç¤ºä¾‹ä»£ç 

```python
#!/usr/bin/env python3
import asyncio
from vllm.control_plane import (
    ControlPlaneManager,
    ExecutionInstance,
    RequestMetadata,
    RequestPriority,
)

async def main():
    # 1. åˆ›å»ºControl Plane
    cp = ControlPlaneManager(
        scheduling_policy="adaptive",
        routing_strategy="load_balanced",
    )
    
    # 2. æ³¨å†Œå®ä¾‹
    instance = ExecutionInstance(
        instance_id="vllm-1",
        host="localhost",
        port=8000,
        model_name="llama-3-70b",
        tensor_parallel_size=4,
        gpu_count=4,
        gpu_memory_gb=80.0,
        max_concurrent_requests=100,
    )
    cp.register_instance(instance)
    
    # 3. å¯åŠ¨
    await cp.start()
    
    # 4. æäº¤è¯·æ±‚
    for i in range(5):
        request = RequestMetadata(
            request_id=f"req-{i:03d}",
            user_id=f"user-{i}",
            priority=RequestPriority.NORMAL if i % 2 else RequestPriority.HIGH,
            slo_deadline_ms=1000,
            max_tokens=100,
            model_name="llama-3-70b",
        )
        await cp.submit_request(request)
    
    # 5. ç›‘æ§
    await asyncio.sleep(2)
    metrics = cp.get_metrics()
    print(f"Completed: {metrics.completed_requests}")
    print(f"SLO Compliance: {metrics.slo_compliance_rate:.2%}")
    
    # 6. åœæ­¢
    await cp.stop()

if __name__ == "__main__":
    asyncio.run(main())
```

## å¸¸è§é…ç½®

### é…ç½®1: SaaSåº”ç”¨ï¼ˆä¸¥æ ¼SLOï¼‰

```python
cp = ControlPlaneManager(
    scheduling_policy="slo_aware",     # SLOæ„ŸçŸ¥è°ƒåº¦
    routing_strategy="affinity",        # ç”¨æˆ·äº²å’Œè·¯ç”±
    enable_monitoring=True,
)

# æäº¤å¸¦SLOçš„è¯·æ±‚
request = RequestMetadata(
    request_id="req-critical",
    priority=RequestPriority.HIGH,
    slo_deadline_ms=500,  # ä¸¥æ ¼ï¼š500ms
)
```

### é…ç½®2: æ‰¹å¤„ç†ï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰

```python
cp = ControlPlaneManager(
    scheduling_policy="cost_optimized",  # æˆæœ¬ä¼˜åŒ–
    routing_strategy="load_balanced",
)

# æäº¤å¸¦æˆæœ¬é¢„ç®—çš„è¯·æ±‚
request = RequestMetadata(
    request_id="req-batch",
    priority=RequestPriority.LOW,
    cost_budget=0.05,  # $0.05é¢„ç®—
    max_tokens=500,
)
```

### é…ç½®3: æ··åˆåœºæ™¯ï¼ˆæ¨èï¼‰

```python
cp = ControlPlaneManager(
    scheduling_policy="adaptive",        # è‡ªé€‚åº”ï¼ˆæ¨èï¼‰
    routing_strategy="load_balanced",
    enable_monitoring=True,
)
```

### é…ç½®4: ç¼“å­˜ä¼˜åŒ–

```python
cp = ControlPlaneManager(
    scheduling_policy="priority",
    routing_strategy="locality",  # å“ˆå¸Œè·¯ç”±æé«˜ç¼“å­˜å‘½ä¸­
)
```

## åŠ¨æ€ç­–ç•¥åˆ‡æ¢

```python
# åˆå§‹ç­–ç•¥
cp = ControlPlaneManager(scheduling_policy="fifo")

# åæœŸåˆ‡æ¢
cp.update_policy("adaptive")       # åˆ‡æ¢åˆ°è‡ªé€‚åº”
print(cp.get_status()['scheduling_policy'])  # æŸ¥çœ‹å½“å‰ç­–ç•¥
```

## æŸ¥è¯¢è¯·æ±‚çŠ¶æ€

```python
# æäº¤è¯·æ±‚
request_id = "req-001"
await cp.submit_request(request)

# æŸ¥è¯¢çŠ¶æ€
status = await cp.get_request_status(request_id)
print(status)  # RequestStatus.QUEUED or RUNNING

# å–æ¶ˆè¯·æ±‚ï¼ˆåªèƒ½å–æ¶ˆpendingçš„ï¼‰
await cp.cancel_request(request_id)
```

## å®ä¾‹ç®¡ç†

```python
# è·å–æ‰€æœ‰å®ä¾‹
instances = cp.get_instances()
for instance in instances:
    print(f"{instance.instance_id}: Load={instance.current_load:.2%}")

# è·å–å•ä¸ªå®ä¾‹æŒ‡æ ‡
metrics = cp.get_instance_metrics("vllm-1")
print(f"Active: {metrics['active_requests']}")
print(f"Latency: {metrics['avg_latency_ms']}ms")
print(f"Healthy: {metrics['is_healthy']}")

# æ³¨é”€å®ä¾‹
cp.unregister_instance("vllm-1")
```

## æ€§èƒ½è°ƒä¼˜å»ºè®®

### 1. è°ƒåº¦ç­–ç•¥é€‰æ‹©

```python
# å¦‚æœæœ‰ä¸¥æ ¼SLOè¦æ±‚
cp.update_policy("slo_aware")

# å¦‚æœè¦æœ€å°åŒ–æˆæœ¬
cp.update_policy("cost_optimized")

# ä¸ç¡®å®šæ—¶ç”¨è‡ªé€‚åº”
cp.update_policy("adaptive")
```

### 2. å¹¶è¡Œç­–ç•¥ä¼˜åŒ–

```python
# é€šè¿‡parallelism_hintæç¤º
request = RequestMetadata(
    request_id="req-001",
    parallelism_hint=ParallelismType.TENSOR_PARALLEL,  # æç¤ºTP
)

# æˆ–è®©ç³»ç»Ÿè‡ªåŠ¨é€‰æ‹©ï¼ˆæ¨èï¼‰
request = RequestMetadata(
    request_id="req-002",
    # ä¸è®¾ç½®hintï¼Œç³»ç»Ÿè‡ªåŠ¨ä¼˜åŒ–
)
```

### 3. è·¯ç”±ç­–ç•¥ä¼˜åŒ–

```python
# éœ€è¦ç¼“å­˜äº²å’Œæ€§
cp.router.routing_strategy = "locality"

# éœ€è¦ç”¨æˆ·ä¼šè¯ä¸€è‡´æ€§
cp.router.routing_strategy = "affinity"

# é€šç”¨è´Ÿè½½å‡è¡¡
cp.router.routing_strategy = "load_balanced"
```

## æ€§èƒ½æŒ‡æ ‡è§£è¯»

```python
metrics = cp.get_metrics()

# å…³é”®æŒ‡æ ‡
print(f"ååé‡: {metrics.requests_per_second:.2f} req/s")
print(f"å¹³å‡å»¶è¿Ÿ: {metrics.avg_latency_ms:.2f}ms")
print(f"å°¾å»¶è¿Ÿ(p95): {metrics.p95_latency_ms:.2f}ms")
print(f"å°¾å»¶è¿Ÿ(p99): {metrics.p99_latency_ms:.2f}ms")
print(f"SLOéµå®ˆç‡: {metrics.slo_compliance_rate:.2%}")

# å¦‚æœSLOéµå®ˆç‡ä½
if metrics.slo_compliance_rate < 0.95:
    cp.update_policy("slo_aware")  # åˆ‡æ¢åˆ°SLOæ„ŸçŸ¥
    
# å¦‚æœæˆæœ¬è¿‡é«˜
if metrics.cost_per_token > 0.001:
    cp.update_policy("cost_optimized")  # åˆ‡æ¢åˆ°æˆæœ¬ä¼˜åŒ–
```

## æ•…éšœæ’æŸ¥

### é—®é¢˜1: è¯·æ±‚å§‹ç»ˆåœ¨é˜Ÿåˆ—ä¸­

```python
# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨å®ä¾‹
status = cp.get_status()
print(f"Available instances: {status['available_instances']}")

# æ£€æŸ¥å®ä¾‹å¥åº·çŠ¶æ€
for instance in cp.get_instances():
    print(f"{instance.instance_id}: healthy={instance.is_healthy}")
    
# å¯èƒ½éœ€è¦æ³¨å†Œæ–°å®ä¾‹æˆ–å¯åŠ¨vLLM
```

### é—®é¢˜2: å»¶è¿Ÿè¿‡é«˜

```python
# æ£€æŸ¥SLOåˆè§„
metrics = cp.get_metrics()
print(f"SLO violations: {metrics.slo_violations}")

# å°è¯•ä¼˜å…ˆçº§è°ƒåº¦
cp.update_policy("priority")

# æˆ–å¢åŠ å®ä¾‹
new_instance = ExecutionInstance(...)
cp.register_instance(new_instance)
```

### é—®é¢˜3: æŸäº›å®ä¾‹è´Ÿè½½ä¸å‡è¡¡

```python
# æ£€æŸ¥å„å®ä¾‹è´Ÿè½½
for instance in cp.get_instances():
    inst_metrics = cp.get_instance_metrics(instance.instance_id)
    print(f"{instance.instance_id}: load={inst_metrics['current_load']:.2%}")

# åˆ‡æ¢è·¯ç”±ç­–ç•¥
cp.router.routing_strategy = "power_of_two"  # æ›´å¥½çš„è´Ÿè½½å‡è¡¡

# æˆ–ä½¿ç”¨weighted_round_robin
cp.load_balancer.select_instance(
    cp.get_instances(),
    algorithm="weighted_round_robin"
)
```

## APIå¿«é€Ÿå‚è€ƒ

### ControlPlaneManager æ–¹æ³•

```python
# ç”Ÿå‘½å‘¨æœŸ
await cp.start()
await cp.stop()

# å®ä¾‹ç®¡ç†
cp.register_instance(instance)
cp.unregister_instance(instance_id)
cp.get_instances()
cp.get_instance_metrics(instance_id)

# è¯·æ±‚ç®¡ç†
await cp.submit_request(request)
await cp.get_request_status(request_id)
await cp.cancel_request(request_id)

# ç›‘æ§
cp.get_metrics()
cp.get_status()

# ç­–ç•¥
cp.update_policy(policy_name)
```

### RequestMetadata å¸¸ç”¨å­—æ®µ

```python
RequestMetadata(
    request_id="unique-id",              # å¿…éœ€
    user_id="user-123",                  # å¯é€‰
    priority=RequestPriority.NORMAL,     # ä¼˜å…ˆçº§
    slo_deadline_ms=1000,                # SLOæˆªæ­¢æ—¶é—´
    max_tokens=100,                      # æœ€å¤§ç”Ÿæˆtokenæ•°
    model_name="llama-3-70b",            # æ¨¡å‹å
    cost_budget=0.01,                    # æˆæœ¬é¢„ç®—
    parallelism_hint=ParallelismType.TP, # å¹¶è¡Œç­–ç•¥æç¤º
)
```

## æ›´å¤šä¿¡æ¯

- è¯¦ç»†æ¶æ„è®¾è®¡: è§ `DESIGN.md`
- ä¸­æ–‡è®¾è®¡æ–‡æ¡£: è§ `DESIGN_SUMMARY_CN.md`
- å®Œæ•´ç¤ºä¾‹: è§ `example.py`
- APIæ–‡æ¡£: å„æ¨¡å—æºä»£ç ä¸­çš„docstrings

ç¥ä½¿ç”¨æ„‰å¿«ï¼ğŸš€
