# Code Review Report: Tasks 1-5 Implementation

## æ¦‚è¿°

å¯¹ Tasks 1-5 çš„è¯¦ç»†ä»£ç å®¡æŸ¥ï¼Œå…³æ³¨ bugã€ä¸åˆç†è®¾è®¡å’ŒæŠ€æœ¯å€ºåŠ¡ã€‚æ‰€æœ‰ 89 ä¸ªæµ‹è¯•é€šè¿‡ï¼Œruff æ£€æŸ¥é€šè¿‡ã€‚

## ğŸ”´ é«˜ä¼˜å…ˆçº§é—®é¢˜ï¼ˆéœ€è¦ç«‹å³ä¿®å¤ï¼‰

### 1. **Task 2: æœªå®ç°çš„ prefix_reuse é›†æˆ**

**ä½ç½®**: `kv_runtime/blocks/multi_granular.py:425`

```python
def query_by_prefix(self, token_ids: list[int], min_match_length: int = 1) -> list[KVBlockDescriptor] | None:
    # TODO: Integrate with prefix_reuse module for actual matching
    self._stats["cache_misses"] += 1
    return None  # æ€»æ˜¯è¿”å› None!
```

**é—®é¢˜ä¸¥é‡æ€§**: ğŸ”´ é«˜
- `query_by_prefix` æ˜¯ KV cache å¤ç”¨çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œä½†å½“å‰åªæ˜¯å ä½ç¬¦
- æ€»æ˜¯è¿”å› `None` æ„å‘³ç€æ‰€æœ‰ prefix æŸ¥è¯¢éƒ½å¤±è´¥
- ç»Ÿè®¡ä¸­çš„ `cache_misses` ä¼šæŒç»­å¢é•¿ï¼Œä½†å®é™…ä¸Šä»æœªå°è¯•åŒ¹é…

**å»ºè®®ä¿®å¤**:
```python
def query_by_prefix(self, token_ids: list[int], min_match_length: int = 1) -> list[KVBlockDescriptor] | None:
    """Query for reusable KV blocks by prefix."""
    if len(token_ids) < min_match_length:
        self._stats["cache_misses"] += 1
        return None
    
    # 1. è®¡ç®— token_ids çš„ hash
    import hashlib
    token_hash = hashlib.sha256(",".join(map(str, token_ids)).encode()).hexdigest()[:16]
    
    # 2. åœ¨å·²æœ‰å—ä¸­æŸ¥æ‰¾åŒ¹é…å‰ç¼€
    candidates = []
    for block in self._blocks.values():
        if block.token_hash and block.token_hash == token_hash[:len(block.token_hash)]:
            candidates.append(block)
    
    if candidates:
        self._stats["cache_hits"] += 1
        return candidates
    
    self._stats["cache_misses"] += 1
    return None
```

**å½±å“èŒƒå›´**: 
- `CrossRequestKVCache.try_reuse()` ä¾èµ–æ­¤å‡½æ•°
- ç³»ç»Ÿ prompt ç­‰å¸¸è§å‰ç¼€æ— æ³•å¤ç”¨ï¼Œæ€§èƒ½æŸå¤±æ˜¾è‘—

---

### 2. **Task 2: HBM/DDR/NVMe deallocate å®ç°ä¸æ­£ç¡®**

**ä½ç½®**: `kv_runtime/hierarchy/tiered_storage.py`

```python
def deallocate(self, size: int) -> None:
    """Mark space as deallocated."""
    self._allocated = max(0, self._allocated - size)
```

**é—®é¢˜ä¸¥é‡æ€§**: ğŸ”´ é«˜
- **å†…å­˜æ³„æ¼é£é™©**: ç®€å•åœ°å‡å°‘ `_allocated` ä¸ä¼šçœŸæ­£å›æ”¶ç©ºé—´
- **ç¢ç‰‡åŒ–**: çº¿æ€§åˆ†é…å™¨æ— æ³•é‡ç”¨ä¸­é—´é‡Šæ”¾çš„å—
- **offset ä¸¢å¤±**: deallocate åªæ¥å— sizeï¼Œä¸çŸ¥é“å“ªä¸ª offset è¢«é‡Šæ”¾

**ç¤ºä¾‹é—®é¢˜åœºæ™¯**:
```python
# åˆ†é… 100 bytes at offset 0
offset1 = backend.allocate(100)  # offset1 = 0, _allocated = 100

# åˆ†é… 50 bytes at offset 100
offset2 = backend.allocate(50)   # offset2 = 100, _allocated = 150

# é‡Šæ”¾ç¬¬ä¸€ä¸ªå—
backend.deallocate(100)          # _allocated = 50

# å†åˆ†é… 80 bytes
offset3 = backend.allocate(80)   # offset3 = 50 (è¦†ç›–äº†ä»åœ¨ä½¿ç”¨çš„å—!)
```

**å»ºè®®ä¿®å¤**:
```python
class HBMBackend:
    def __init__(self, ...):
        # ...
        self._free_chunks: list[tuple[int, int]] = [(0, self.capacity_bytes)]  # (offset, size)
        self._allocated_chunks: dict[int, int] = {}  # offset -> size
    
    def allocate(self, size: int) -> int:
        """First-fit allocation."""
        for i, (offset, chunk_size) in enumerate(self._free_chunks):
            if chunk_size >= size:
                # åˆ†é…
                self._allocated_chunks[offset] = size
                # æ›´æ–° free list
                if chunk_size == size:
                    del self._free_chunks[i]
                else:
                    self._free_chunks[i] = (offset + size, chunk_size - size)
                return offset
        raise MemoryError(f"Insufficient space: need {size}")
    
    def deallocate(self, offset: int) -> None:
        """Free and try to merge adjacent chunks."""
        if offset not in self._allocated_chunks:
            return
        size = self._allocated_chunks.pop(offset)
        self._free_chunks.append((offset, size))
        self._free_chunks.sort()
        # Merge adjacent free chunks
        self._merge_free_chunks()
    
    def _merge_free_chunks(self):
        """Merge contiguous free chunks."""
        merged = []
        for offset, size in sorted(self._free_chunks):
            if merged and merged[-1][0] + merged[-1][1] == offset:
                merged[-1] = (merged[-1][0], merged[-1][1] + size)
            else:
                merged.append((offset, size))
        self._free_chunks = merged
```

**å½±å“èŒƒå›´**: æ‰€æœ‰ä¸‰å±‚å­˜å‚¨ï¼ˆHBM/DDR/NVMeï¼‰éƒ½æœ‰ç›¸åŒé—®é¢˜

---

### 3. **Task 3: FP8/INT4 é‡åŒ–ç¼ºå°‘è¾¹ç•Œæ£€æŸ¥**

**ä½ç½®**: `accel/quantize/fp8.py`, `accel/quantize/int4.py`

**é—®é¢˜**: 
```python
# FP8 E4M3: max_value = 448.0
scaled_weight = weight / scales.view(-1, 1)
clipped = torch.clamp(scaled_weight, -448.0 * clip_ratio, 448.0 * clip_ratio)
```

**ç¼ºå°‘çš„æ£€æŸ¥**:
1. **Zero scale å¤„ç†**: å¦‚æœ `scales` æ¥è¿‘ 0ï¼Œé™¤æ³•ä¼šå¯¼è‡´ inf/nan
2. **Input validation**: æ²¡æœ‰æ£€æŸ¥ `weight` æ˜¯å¦åŒ…å« nan/inf
3. **Shape validation**: `scales.view(-1, 1)` å‡è®¾ scales æ˜¯ 1Dï¼Œä½† per-group æ—¶å¯èƒ½æ˜¯ 2D

**å»ºè®®ä¿®å¤**:
```python
def quantize(self, weight, config: QuantizationConfig) -> QuantizationOutput:
    import torch
    
    # Validate input
    if torch.isnan(weight).any() or torch.isinf(weight).any():
        raise ValueError("Weight contains NaN or Inf")
    
    # Compute scales
    scales = self._compute_scale_per_tensor(weight)
    
    # Protect against zero division
    scales = scales.clamp(min=1e-8)
    
    # Ensure scale shape matches weight
    if scales.dim() == 1 and weight.dim() == 2:
        scales = scales.view(-1, 1)
    elif scales.dim() == 2 and weight.dim() == 2:
        # per-group: scales shape is [out_features, num_groups]
        # need broadcasting logic
        pass
    
    # Scale and clip
    scaled_weight = weight / scales
    clipped = torch.clamp(
        scaled_weight,
        -self.format.max_value * config.clip_ratio,
        self.format.max_value * config.clip_ratio
    )
    
    # ... rest
```

---

## ğŸŸ¡ ä¸­ä¼˜å…ˆçº§é—®é¢˜ï¼ˆå»ºè®®ä¿®å¤ï¼‰

### 4. **Task 2: çƒ­åº¦åˆ†ç±»é˜ˆå€¼ç¡¬ç¼–ç **

**ä½ç½®**: `kv_runtime/migration/hot_cold.py:130`

```python
def classify(self, block: KVBlockDescriptor) -> str:
    if block.access_frequency >= self.hot_frequency_threshold:
        return "hot"
    if time_since_access > self.cold_timeout_s:
        return "cold"
    if block.access_frequency < self.warm_frequency_threshold:
        return "cold"
    return "warm"
```

**é—®é¢˜**:
- **é€»è¾‘å†²çª**: åŒæ—¶æ»¡è¶³"æ—¶é—´ä¹…"å’Œ"é¢‘ç‡ä½"éƒ½ä¼šè¿”å› coldï¼Œä½†ä¸¤è€…åº”æœ‰ä¸åŒä¼˜å…ˆçº§
- **è¾¹ç•Œæƒ…å†µ**: `warm_frequency_threshold <= freq < hot_frequency_threshold` ä¸” `time_since_access <= cold_timeout_s` çš„å—ä¼šè¢«é”™è¯¯åˆ†ç±»
- **ç¼ºå°‘è‡ªé€‚åº”**: é˜ˆå€¼å›ºå®šï¼Œæ— æ³•é€‚åº”ä¸åŒå·¥ä½œè´Ÿè½½

**å»ºè®®æ”¹è¿›**:
```python
def classify(self, block: KVBlockDescriptor) -> str:
    """Enhanced classification with clear priority."""
    now = time.time()
    time_since_access = now - block.last_access_time
    freq = block.access_frequency
    
    # Priority 1: Very recent access = hot (regardless of frequency)
    if time_since_access < 1.0:  # Within 1 second
        return "hot"
    
    # Priority 2: High frequency = hot
    if freq >= self.hot_frequency_threshold:
        return "hot"
    
    # Priority 3: Very old = cold (regardless of frequency)
    if time_since_access > self.cold_timeout_s:
        return "cold"
    
    # Priority 4: Low frequency and moderate age = cold
    if freq < self.warm_frequency_threshold and time_since_access > self.cold_timeout_s / 2:
        return "cold"
    
    # Default: warm
    return "warm"
```

---

### 5. **Task 3: N:M ç¨€ç–æ€§æœªéªŒè¯ç¡¬ä»¶æ”¯æŒ**

**ä½ç½®**: `accel/sparsity/structured.py:68`

```python
def prune(self, weight) -> SparseOutput:
    # Find top-N magnitudes in each M-group
    _, indices = torch.topk(abs_weight, self.n, dim=-1)
```

**é—®é¢˜**:
- **ç¡¬ä»¶é™åˆ¶**: NVIDIA Ampere åªæ”¯æŒ 2:4 ç¨€ç–æ€§ï¼Œ4:8 å’Œ 1:4 å¯èƒ½æ— åŠ é€Ÿ
- **Shape çº¦æŸ**: æƒé‡ shape å¿…é¡»æ˜¯ M çš„å€æ•°ï¼Œå¦åˆ™ reshape ä¼šå¤±è´¥
- **æœªæ£€æŸ¥ CUDA Capability**: æ²¡æœ‰è¿è¡Œæ—¶æ£€æŸ¥ GPU æ˜¯å¦æ”¯æŒç¨€ç–å¼ é‡æ ¸

**å»ºè®®ä¿®å¤**:
```python
def prune(self, weight) -> SparseOutput:
    import torch
    
    # Check shape compatibility
    if weight.numel() % self.m != 0:
        raise ValueError(
            f"Weight size {weight.numel()} is not divisible by M={self.m}. "
            f"Consider padding to nearest multiple of {self.m}."
        )
    
    # Check hardware support (optional warning)
    if torch.cuda.is_available():
        capability = torch.cuda.get_device_capability()
        if capability < (8, 0):  # Ampere = 8.0
            import warnings
            warnings.warn(
                f"GPU compute capability {capability} may not support structured "
                f"sparsity acceleration. Requires compute capability >= 8.0 (Ampere)."
            )
    
    # Rest of implementation...
```

---

### 6. **Task 5: MFU è®¡ç®—ä¸è€ƒè™‘ Attention FLOPS**

**ä½ç½®**: `benchmarks/metrics/mfu.py:89`

```python
# FLOPs per token per layer (simplified Transformer)
# Attention: 4 * hidden_size^2 (QKV + Output projection)
flops_per_token_per_layer = (
    4 * hidden_size * hidden_size  # Attention
    + 2 * hidden_size * intermediate_size  # MLP
)
```

**é—®é¢˜**:
- **æ³¨æ„åŠ›è®¡ç®—è¢«ç®€åŒ–**: å®é™… Attention åŒ…æ‹¬ QKV matmulã€æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—ã€softmaxã€output matmul
- **æ­£ç¡®å…¬å¼**:
  ```
  QKV projection:     3 * 2 * seq_len * hidden^2 = 6 * seq_len * hidden^2
  Attention scores:   2 * seq_len^2 * hidden (Q @ K^T)
  Attention output:   2 * seq_len^2 * hidden (scores @ V)
  Output projection:  2 * seq_len * hidden^2
  Total Attention:    8 * seq_len * hidden^2 + 4 * seq_len^2 * hidden
  ```

**å»ºè®®ä¿®å¤**:
```python
def compute(self, num_tokens: int, seq_len: int, num_layers: int, hidden_size: int, ...) -> MFUResult:
    """Compute MFU with accurate FLOP counting.
    
    Args:
        num_tokens: Total tokens processed (batch_size * seq_len)
        seq_len: Sequence length (for attention complexity)
        ...
    """
    # More accurate Transformer FLOP formula
    # Reference: https://arxiv.org/abs/2001.08361 (Kaplan et al.)
    
    # Attention (per token, considering seq_len dependency)
    attention_flops = (
        6 * hidden_size * hidden_size  # QKV projection
        + 2 * seq_len * hidden_size     # Attention scores & output
        + 2 * hidden_size * hidden_size  # Output projection
    )
    
    # MLP
    mlp_flops = 2 * hidden_size * intermediate_size
    
    flops_per_token_per_layer = attention_flops + mlp_flops
    total_flops = num_tokens * num_layers * flops_per_token_per_layer
    
    # ... rest
```

---

## ğŸŸ¢ ä½ä¼˜å…ˆçº§é—®é¢˜ï¼ˆä»£ç æ”¹è¿›ï¼‰

### 7. **é€šç”¨: å¼‚å¸¸æ¶ˆæ¯ç¼ºå°‘ä¸Šä¸‹æ–‡**

å¾ˆå¤š `raise` è¯­å¥ç¼ºå°‘è¶³å¤Ÿçš„è°ƒè¯•ä¿¡æ¯ï¼š

```python
# ä¸å¥½
raise MemoryError(f"Insufficient HBM space: need {size}, have {free}")

# æ›´å¥½
raise MemoryError(
    f"Insufficient HBM space: need {size} bytes, have {free} bytes. "
    f"Allocated: {self._allocated}/{self.capacity_bytes}. "
    f"Consider increasing capacity or migrating to DDR."
)
```

---

### 8. **Task 2: ç¼ºå°‘å¹¶å‘å®‰å…¨ä¿æŠ¤**

`MultiGranularKVPool` å’Œ `TieredKVStorage` æ²¡æœ‰çº¿ç¨‹é”ï¼š

```python
class MultiGranularKVPool:
    def __init__(self, config: KVPoolConfig):
        self._blocks: dict[int, KVBlockDescriptor] = {}
        self._lock = threading.Lock()  # æ·»åŠ é”
    
    def allocate(self, ...) -> list[KVBlockDescriptor]:
        with self._lock:
            # ... allocation logic
```

---

### 9. **Task 5: æ—¶é—´æˆ³ç²¾åº¦ä¸ä¸€è‡´**

```python
# trace.py: ä½¿ç”¨ time.time() (ç§’çº§ï¼Œæµ®ç‚¹)
start_time = time.time()

# latency.py: ä½¿ç”¨ time.perf_counter() (é«˜ç²¾åº¦)
start = time.perf_counter()
```

**å»ºè®®**: ç»Ÿä¸€ä½¿ç”¨ `time.perf_counter()` ç”¨äºæ€§èƒ½æµ‹é‡ï¼Œ`time.time()` ç”¨äºç»å¯¹æ—¶é—´æˆ³

---

### 10. **Task 3: é‡åŒ–é…ç½®æœªæ ¡éªŒ**

```python
@dataclass
class QuantizationConfig:
    clip_ratio: float = 1.0
    group_size: int = 128
```

ç¼ºå°‘æ ¡éªŒ:
```python
def __post_init__(self):
    if not 0.0 < self.clip_ratio <= 1.0:
        raise ValueError(f"clip_ratio must be in (0, 1], got {self.clip_ratio}")
    if self.group_size < 1:
        raise ValueError(f"group_size must be >= 1, got {self.group_size}")
    if self.granularity == QuantizationGranularity.PER_GROUP and self.group_size is None:
        raise ValueError("group_size is required for PER_GROUP granularity")
```

---

## ğŸ“Š æµ‹è¯•è¦†ç›–é—®é¢˜

### Task 2 ç¼ºå¤±æµ‹è¯•:
1. **å¹¶å‘åˆ†é…/é‡Šæ”¾**: å¤šçº¿ç¨‹è®¿é—® KV pool
2. **è¾¹ç•Œæƒ…å†µ**: åˆ†é… 0 bytesï¼Œé‡Šæ”¾ä¸å­˜åœ¨çš„å—
3. **è·¨å±‚è¿ç§»å¤±è´¥**: DDR/NVMe æ»¡æ—¶çš„å›é€€ç­–ç•¥

### Task 3 ç¼ºå¤±æµ‹è¯•:
1. **NaN/Inf è¾“å…¥**: é‡åŒ–æ—¶è¾“å…¥åŒ…å«æ— æ•ˆå€¼
2. **ç¨€ç–æ€§éªŒè¯**: N:M æ¨¡å¼æ˜¯å¦çœŸçš„å¼ºåˆ¶æ‰§è¡Œï¼ˆæ¯ M ä¸ªå…ƒç´ ä¸­æ˜¯å¦æ°å¥½ N ä¸ªéé›¶ï¼‰
3. **ç¡¬ä»¶åŠ é€ŸéªŒè¯**: åœ¨æ”¯æŒç¨€ç–çš„ GPU ä¸ŠéªŒè¯åŠ é€Ÿæ¯”

### Task 5 ç¼ºå¤±æµ‹è¯•:
1. **Chrome Tracing æ ¼å¼**: è¾“å‡ºæ˜¯å¦ç¬¦åˆ `chrome://tracing` è§„èŒƒ
2. **é•¿æ—¶é—´è¿è¡Œ**: æ—¶é—´æˆ³æº¢å‡ºã€æµ®ç‚¹ç²¾åº¦æŸå¤±
3. **å¹¶å‘ trace**: å¤šçº¿ç¨‹åŒæ—¶å†™å…¥ trace

---

## æ€»ç»“

### å¿…é¡»ä¿®å¤ï¼ˆé˜»å¡ï¼‰:
1. âœ… Task 2: å®ç° `query_by_prefix` çš„å®é™…é€»è¾‘
2. âœ… Task 2: ä¿®å¤ deallocate çš„å†…å­˜ç®¡ç†
3. âœ… Task 3: æ·»åŠ é‡åŒ–è¾“å…¥æ ¡éªŒå’Œè¾¹ç•Œæ£€æŸ¥

### å»ºè®®ä¿®å¤ï¼ˆæå‡è´¨é‡ï¼‰:
4. Task 2: æ”¹è¿›çƒ­åº¦åˆ†ç±»é€»è¾‘
5. Task 3: æ·»åŠ ç¡¬ä»¶æ”¯æŒæ£€æŸ¥
6. Task 5: ä¿®æ­£ MFU FLOP è®¡ç®—å…¬å¼
7. å…¨å±€: æ”¹è¿›å¼‚å¸¸æ¶ˆæ¯
8. å…¨å±€: æ·»åŠ å¹¶å‘å®‰å…¨ä¿æŠ¤

### å¯é€‰æ”¹è¿›ï¼ˆé•¿æœŸï¼‰:
9. ç»Ÿä¸€æ—¶é—´æˆ³ API
10. æ·»åŠ é…ç½®æ ¡éªŒ
11. æ‰©å±•æµ‹è¯•è¦†ç›–

---

## æŠ€æœ¯å€ºåŠ¡è¯„ä¼°

| æ¨¡å— | å€ºåŠ¡ç¨‹åº¦ | ä¸»è¦é—®é¢˜ |
|-----|---------|---------|
| Task 2 - KV Runtime | ğŸ”´ é«˜ | å†…å­˜ç®¡ç†ã€prefix åŒ¹é…æœªå®ç° |
| Task 3 - Accel | ğŸŸ¡ ä¸­ | è¾¹ç•Œæ£€æŸ¥ã€ç¡¬ä»¶å…¼å®¹æ€§ |
| Task 5 - Benchmarks | ğŸŸ¢ ä½ | å…¬å¼å‡†ç¡®æ€§ã€æ ¼å¼å…¼å®¹æ€§ |
| Task 1 - ExecutionGraph | ğŸŸ¢ ä½ | å·²æˆç†Ÿï¼Œæ— é‡å¤§é—®é¢˜ |

**æ¨èè¡ŒåŠ¨**: ä¼˜å…ˆä¿®å¤ Task 2 çš„ 2 ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜ï¼ˆprefix_reuse å’Œ deallocateï¼‰ï¼Œå…¶ä»–é—®é¢˜å¯è¿­ä»£æ”¹è¿›ã€‚
