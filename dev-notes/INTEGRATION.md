# sageLLM Control Plane - é›†æˆæ¶æ„æ–‡æ¡£

## æ¦‚è¿°

Control Plane æ˜¯ sageLLM çš„æ ¸å¿ƒåè°ƒå±‚ï¼Œä½äºç”¨æˆ·åº”ç”¨å’Œå¤šä¸ª vLLM å®ä¾‹ä¹‹é—´ï¼Œæä¾›æ™ºèƒ½çš„è¯·æ±‚è°ƒåº¦ã€è·¯ç”±å’Œæ€§èƒ½ä¼˜åŒ–ã€‚

### æ ¸å¿ƒåŠŸèƒ½

1. **æ™ºèƒ½è¯·æ±‚è°ƒåº¦**: FIFOã€Priorityã€SLO-Awareã€Cost-Optimizedã€Adaptive äº”ç§ç­–ç•¥
1. **PD åˆ†ç¦»ä¼˜åŒ–**: Prefilling/Decoding åˆ†ç¦»è·¯ç”±ï¼Œæå‡ 50-80% ååï¼Œé™ä½ 50-60% å»¶è¿Ÿ
1. **å¤šå®ä¾‹ç®¡ç†**: ç»Ÿä¸€ç®¡ç†å¤šä¸ª vLLM å®ä¾‹ï¼Œæ”¯æŒä¸åŒå¹¶è¡Œç­–ç•¥
1. **åŠ¨æ€å¹¶è¡Œä¼˜åŒ–**: è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„ TP/PP/DP/EP/Hybrid å¹¶è¡Œæ–¹æ¡ˆ
1. **è´Ÿè½½å‡è¡¡**: å¤šç§è·¯ç”±ç®—æ³•ï¼ˆload_balancedã€affinityã€locality ç­‰ï¼‰
1. **æ€§èƒ½ç›‘æ§**: å®æ—¶ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†

## é›†æˆæ¶æ„

### Control Plane åœ¨ SAGE ä¸­çš„ä½ç½®

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SAGE Applications Layer                         â”‚
â”‚  â€¢ Chat Service (å¯¹è¯æœåŠ¡)                                   â”‚
â”‚  â€¢ Embedding Service (å‘é‡åŒ–æœåŠ¡)                            â”‚
â”‚  â€¢ Batch Processing (æ‰¹å¤„ç†æœåŠ¡)                             â”‚
â”‚  â€¢ Fine-tuning Service (å¾®è°ƒæœåŠ¡)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ RequestMetadata
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Control Plane Manager                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Request Queue Management                          â”‚      â”‚
â”‚  â”‚  â€¢ pending_queue: deque[RequestMetadata]           â”‚      â”‚
â”‚  â”‚  â€¢ running_requests: dict[str, RequestMetadata]    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Scheduling â”‚  â”‚ PD Routing   â”‚  â”‚ Request      â”‚         â”‚
â”‚  â”‚ Policies   â”‚  â”‚ Strategy     â”‚  â”‚ Router       â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Execution Coordinator                             â”‚      â”‚
â”‚  â”‚  â€¢ Instance Registry                               â”‚      â”‚
â”‚  â”‚  â€¢ Health Monitoring                               â”‚      â”‚
â”‚  â”‚  â€¢ HTTP Client Pool                                â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP API (OpenAI-compatible)
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              vLLM Instances (Multiple)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Instance â”‚  â”‚ Instance â”‚  â”‚ Instance â”‚  â”‚ Instance â”‚    â”‚
â”‚  â”‚    1     â”‚  â”‚    2     â”‚  â”‚    3     â”‚  â”‚    N     â”‚    â”‚
â”‚  â”‚ Prefill  â”‚  â”‚ Decode   â”‚  â”‚ Decode   â”‚  â”‚ General  â”‚    â”‚
â”‚  â”‚ TP=4     â”‚  â”‚ TP=1     â”‚  â”‚ DP=2     â”‚  â”‚ Hybrid   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### è¯·æ±‚å¤„ç†æµç¨‹

```python
# ============================================================
# å®Œæ•´çš„è¯·æ±‚å¤„ç†æµç¨‹ç¤ºä¾‹
# ============================================================

# 1. SAGE åº”ç”¨å±‚æäº¤è¯·æ±‚åˆ° Control Plane
from control_plane import ControlPlaneManager, RequestMetadata, RequestPriority

request = RequestMetadata(
    request_id="req-123",
    user_id="user-456",
    priority=RequestPriority.HIGH,
    slo_deadline_ms=1000,  # 1ç§’ SLO
    max_tokens=512,
    prompt="Explain quantum computing in simple terms.",
    model_name="meta-llama/Llama-2-7b",
)

# æäº¤è¯·æ±‚
request_id = await manager.submit_request(request)

# ============================================================
# 2. Control Plane Manager æ¥æ”¶è¯·æ±‚
# ============================================================
# manager.py: submit_request()
# â€¢ éªŒè¯è¯·æ±‚å‚æ•°
# â€¢ åˆ†é…å”¯ä¸€ request_id
# â€¢ æ·»åŠ åˆ° pending_queue
# â€¢ è§¦å‘è°ƒåº¦å¾ªç¯

# ============================================================
# 3. Scheduling Policy ç¡®å®šä¼˜å…ˆçº§/é¡ºåº
# ============================================================
# policies.py: get_next_request()
# â€¢ FIFO: æŒ‰åˆ°è¾¾æ—¶é—´æ’åº
# â€¢ Priority: æŒ‰ä¼˜å…ˆçº§æ’åº
# â€¢ SLO-Aware: è®¡ç®—ç´§è¿«åº¦ (deadline - current_time)
# â€¢ Cost-Optimized: è€ƒè™‘æˆæœ¬é¢„ç®—
# â€¢ Adaptive: åŠ¨æ€é€‰æ‹©ç­–ç•¥

scheduling_decision = await manager.scheduling_policy.schedule(
    request, available_instances
)

# ============================================================
# 4. PD Router ç¡®å®šè¯·æ±‚é˜¶æ®µ (å¦‚æœå¯ç”¨)
# ============================================================
# pd_routing.py: determine_request_phase()
if manager.enable_pd_separation:
    phase = manager.pd_router.determine_request_phase(request)
    # â€¢ PREFILLING: é•¿è¾“å…¥å¤„ç† (input_tokens > threshold)
    # â€¢ DECODING: ç”Ÿæˆè¾“å‡º (input_tokens <= threshold)

# ============================================================
# 5. Request Router é€‰æ‹©åˆé€‚çš„å®ä¾‹
# ============================================================
# router.py: select_instance()
instance = await manager.router.select_instance(
    request=request,
    available_instances=manager.executor.get_healthy_instances(),
    phase=phase,  # å¦‚æœå¯ç”¨ PD åˆ†ç¦»
)
# è·¯ç”±ç­–ç•¥:
# â€¢ load_balanced: é€‰æ‹©è´Ÿè½½æœ€ä½çš„å®ä¾‹
# â€¢ round_robin: è½®è¯¢
# â€¢ affinity: ç”¨æˆ·äº²å’Œæ€§ (user_id hash)
# â€¢ locality: è¯·æ±‚å±€éƒ¨æ€§ (request hash)

# ============================================================
# 6. Execution Coordinator é€šè¿‡ HTTP API æ‰§è¡Œ
# ============================================================
# executor.py: execute_request()
result = await manager.executor.execute_request(
    request=request,
    instance=instance,
    decision=scheduling_decision,
)
# HTTP è°ƒç”¨:
# â€¢ POST {instance.host}:{instance.port}/v1/completions
# â€¢ æˆ– POST {instance.host}:{instance.port}/v1/chat/completions
# â€¢ æ”¯æŒæµå¼ (stream=True) æˆ–æ‰¹é‡å“åº”

# ============================================================
# 7. vLLM Instance å¤„ç†è¯·æ±‚
# ============================================================
# vLLM å†…éƒ¨æ‰§è¡Œ:
# â€¢ AsyncLLMEngine æ¥æ”¶è¯·æ±‚
# â€¢ KV Cache ç®¡ç†
# â€¢ GPU è°ƒåº¦å’Œæ‰¹å¤„ç†
# â€¢ ç”Ÿæˆ tokens

# ============================================================
# 8. å“åº”è¿”å›åˆ° Control Plane
# ============================================================
# â€¢ æ”¶é›†ç”Ÿæˆçš„ tokens
# â€¢ è®¡ç®—å»¶è¿ŸæŒ‡æ ‡
# â€¢ æ›´æ–°è¯·æ±‚çŠ¶æ€

# ============================================================
# 9. æŒ‡æ ‡æ”¶é›†å’Œæ›´æ–°
# ============================================================
# manager.py: _update_metrics()
metrics = manager.get_metrics()
# â€¢ total_requests, completed_requests, failed_requests
# â€¢ avg_latency_ms, p95_latency_ms, p99_latency_ms
# â€¢ tokens_per_second, requests_per_second
# â€¢ slo_violations, slo_compliance_rate
# â€¢ gpu_utilization

# ============================================================
# 10. ç»“æœè¿”å›åˆ° SAGE åº”ç”¨
# ============================================================
status = await manager.get_request_status(request_id)
# RequestStatus.COMPLETED
# RequestStatus.FAILED
# RequestStatus.RUNNING
```

## é›†æˆæ–¹å¼

### æ–¹å¼ 1: ç›´æ¥ä½¿ç”¨ Control Plane API

è¿™æ˜¯æ¨èçš„é›†æˆæ–¹å¼ï¼Œé€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯ã€‚

```python
import asyncio
from control_plane import (
    ControlPlaneManager,
    ExecutionInstance,
    RequestMetadata,
    RequestPriority,
)


async def integrate_with_sage_app():
    """SAGE åº”ç”¨é›†æˆç¤ºä¾‹"""

    # 1. åˆå§‹åŒ– Control Plane
    manager = ControlPlaneManager(
        scheduling_policy="adaptive",
        routing_strategy="load_balanced",
        enable_pd_separation=True,
        enable_monitoring=True,
    )

    # 2. æ³¨å†Œ vLLM å®ä¾‹
    # æ³¨æ„: vLLM å®ä¾‹éœ€è¦é¢„å…ˆå¯åŠ¨å¹¶ç›‘å¬ HTTP ç«¯å£

    # Prefilling ä¼˜åŒ–å®ä¾‹
    prefilling_instance = ExecutionInstance(
        instance_id="prefill-1",
        host="localhost",  # æˆ–è¿œç¨‹ IP
        port=8000,
        model_name="meta-llama/Llama-2-7b",
        instance_type="prefilling",
        tensor_parallel_size=4,
        gpu_count=4,
    )
    manager.register_instance(prefilling_instance)

    # Decoding ä¼˜åŒ–å®ä¾‹
    decoding_instance = ExecutionInstance(
        instance_id="decode-1",
        host="localhost",
        port=8001,
        model_name="meta-llama/Llama-2-7b",
        instance_type="decoding",
        tensor_parallel_size=1,
        gpu_count=1,
    )
    manager.register_instance(decoding_instance)

    # 3. å¯åŠ¨ Control Plane
    await manager.start()

    # 4. åœ¨ SAGE åº”ç”¨ä¸­æäº¤è¯·æ±‚
    request = RequestMetadata(
        request_id="sage-req-001",
        user_id="user-123",
        priority=RequestPriority.HIGH,
        slo_deadline_ms=1000,
        max_tokens=512,
        prompt="ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šé‡å­è®¡ç®—ã€‚",
    )

    request_id = await manager.submit_request(request)

    # 5. ç­‰å¾…å®Œæˆå¹¶è·å–ç»“æœ
    while True:
        status = await manager.get_request_status(request_id)
        if status.state in ["completed", "failed"]:
            break
        await asyncio.sleep(0.1)

    # 6. è·å–æ€§èƒ½æŒ‡æ ‡
    metrics = manager.get_metrics()
    print(f"Throughput: {metrics.requests_per_second:.2f} req/s")
    print(f"Latency: {metrics.avg_latency_ms:.2f} ms")

    # 7. åœæ­¢ Control Plane
    await manager.stop()

    return status


# è¿è¡Œé›†æˆ
asyncio.run(integrate_with_sage_app())
```

### æ–¹å¼ 2: åµŒå…¥åˆ° SAGE Service ä¸­

å°† Control Plane ä½œä¸º SAGE Service çš„ä¸€éƒ¨åˆ†ï¼š

```python
# sage/services/llm_service.py

from control_plane import ControlPlaneManager
import logging

logger = logging.getLogger(__name__)


class LLMService:
    """SAGE LLM æœåŠ¡ï¼Œé›†æˆ Control Plane"""

    def __init__(self, config):
        self.config = config
        self.manager = None

    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        logger.info("Initializing LLM Service with Control Plane")

        # åˆ›å»º Control Plane
        self.manager = ControlPlaneManager(
            scheduling_policy=self.config.get("scheduling_policy", "adaptive"),
            enable_pd_separation=self.config.get("enable_pd_separation", True),
        )

        # ä»é…ç½®åŠ è½½ vLLM å®ä¾‹
        for instance_config in self.config.get("vllm_instances", []):
            instance = ExecutionInstance(**instance_config)
            self.manager.register_instance(instance)

        # å¯åŠ¨
        await self.manager.start()
        logger.info("LLM Service initialized successfully")

    async def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        request = RequestMetadata(
            prompt=prompt,
            max_tokens=kwargs.get("max_tokens", 512),
            priority=kwargs.get("priority", RequestPriority.NORMAL),
        )

        request_id = await self.manager.submit_request(request)

        # ç­‰å¾…å®Œæˆ
        status = await self._wait_for_completion(request_id)

        if status.state == "completed":
            return status.output
        else:
            raise RuntimeError(f"Request failed: {status.error}")

    async def _wait_for_completion(self, request_id: str, timeout: float = 30.0):
        """ç­‰å¾…è¯·æ±‚å®Œæˆ"""
        import time
        start_time = time.time()

        while time.time() - start_time < timeout:
            status = await self.manager.get_request_status(request_id)
            if status.state in ["completed", "failed"]:
                return status
            await asyncio.sleep(0.1)

        raise TimeoutError(f"Request {request_id} timed out")

    async def shutdown(self):
        """å…³é—­æœåŠ¡"""
        if self.manager:
            await self.manager.stop()
        logger.info("LLM Service shutdown complete")


# ä½¿ç”¨ç¤ºä¾‹
async def main():
    config = {
        "scheduling_policy": "adaptive",
        "enable_pd_separation": True,
        "vllm_instances": [
            {
                "instance_id": "vllm-1",
                "host": "localhost",
                "port": 8000,
                "model_name": "meta-llama/Llama-2-7b",
                "tensor_parallel_size": 2,
            }
        ]
    }

    service = LLMService(config)
    await service.initialize()

    try:
        output = await service.generate("Hello, how are you?")
        print(f"Output: {output}")
    finally:
        await service.shutdown()

asyncio.run(main())
```

## è¿è¡Œæµ‹è¯•

### æœ¬åœ°å¼€å‘æµ‹è¯•

```bash
# è¿›å…¥æµ‹è¯•ç›®å½•
cd tests/control_plane

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python -m pytest -v

# è¿è¡Œç‰¹å®šæµ‹è¯•æ¨¡å—
python -m pytest test_scheduling.py -v

# è¿è¡Œç‰¹å®šæµ‹è¯•å‡½æ•°
python -m pytest test_integration.py::test_control_plane_basic_flow -v

# æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
python -m pytest -v --tb=short

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
python -m pytest --cov=control_plane --cov-report=html
```

### CI/CD æµ‹è¯•

GitHub Actions å·¥ä½œæµä¼šè‡ªåŠ¨è¿è¡Œï¼š

```bash
cd tests/control_plane && python -m pytest -v --tb=short
```

**æµ‹è¯•éš”ç¦»é…ç½®ï¼š**

- ä½¿ç”¨ `tests/control_plane/pytest.ini` è¿›è¡Œéš”ç¦»é…ç½®
- é¿å…åŠ è½½ `tests/conftest.py`ï¼ˆæœ‰é‡å‹ vLLM ä¾èµ–ï¼‰
- æ¯ä¸ªæµ‹è¯•ç›®å½•æœ‰ç‹¬ç«‹çš„ `conftest.py`
- ä¸ä¾èµ–ç¼–è¯‘çš„ vLLM C æ‰©å±•

### æµ‹è¯•è¦†ç›–èŒƒå›´

| æµ‹è¯•æ–‡ä»¶                | æµ‹è¯•æ•°é‡ | è¦†ç›–èŒƒå›´                                            |
| ----------------------- | -------- | --------------------------------------------------- |
| `test_scheduling.py`    | 5        | è°ƒåº¦ç­–ç•¥éªŒè¯ï¼ˆFIFOã€Priorityã€SLOã€Costã€Adaptiveï¼‰ |
| `test_pd_separation.py` | 5        | PD è·¯ç”±å’Œå®ä¾‹ä¸“ä¸šåŒ–                                 |
| `test_executor.py`      | 5        | æ‰§è¡Œå™¨ç”Ÿå‘½å‘¨æœŸå’Œå®ä¾‹ç®¡ç†                            |
| `test_integration.py`   | 5        | å®Œæ•´ SAGE â†” Control Plane â†” vLLM æµç¨‹               |

**æ€»è®¡ï¼š20 ä¸ªæµ‹è¯•ï¼Œå…¨éƒ¨é€šè¿‡ âœ…**

## æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. Types (`types.py`)

å®šä¹‰äº†æ‰€æœ‰æ ¸å¿ƒæ•°æ®æ¨¡å‹ï¼š

```python
# è¯·æ±‚å…ƒæ•°æ®
@dataclass
class RequestMetadata:
    request_id: str
    user_id: Optional[str] = None
    priority: RequestPriority = RequestPriority.NORMAL
    slo_deadline_ms: Optional[float] = None
    max_tokens: int = 512
    prompt: str = ""
    model_name: Optional[str] = None
    cost_budget: Optional[float] = None
    parallelism_hint: Optional[ParallelismType] = None

# æ‰§è¡Œå®ä¾‹
@dataclass
class ExecutionInstance:
    instance_id: str
    host: str = "localhost"
    port: int = 8000
    model_name: str = ""
    tensor_parallel_size: int = 1
    pipeline_parallel_size: int = 1
    data_parallel_size: int = 1
    expert_parallel_size: int = 1
    gpu_count: int = 1
    instance_type: str = "general"  # prefilling, decoding, general
    health_status: str = "healthy"

# è°ƒåº¦å†³ç­–
@dataclass
class SchedulingDecision:
    instance_id: str
    request_id: str
    priority: float
    estimated_latency_ms: float
    parallelism_config: Optional[ParallelismConfig] = None
```

### 2. Manager (`manager.py`)

Control Plane çš„æ ¸å¿ƒåè°ƒå™¨ï¼š

```python
class ControlPlaneManager:
    def __init__(
        self,
        scheduling_policy: str = "adaptive",
        routing_strategy: str = "load_balanced",
        enable_pd_separation: bool = True,
        enable_monitoring: bool = True,
    ):
        # æ ¸å¿ƒç»„ä»¶
        self.executor = ExecutionCoordinator()
        self.router = RequestRouter(routing_strategy)
        self.scheduling_policy = self._create_policy(scheduling_policy)
        self.pd_router = PDRoutingStrategy() if enable_pd_separation else None

        # è¯·æ±‚é˜Ÿåˆ—
        self.pending_queue: deque[RequestMetadata] = deque()
        self.running_requests: dict[str, RequestMetadata] = {}

    async def submit_request(self, request: RequestMetadata) -> str:
        """æäº¤è¯·æ±‚åˆ°é˜Ÿåˆ—"""

    async def start(self):
        """å¯åŠ¨ Control Plane åå°ä»»åŠ¡"""

    async def stop(self):
        """åœæ­¢ Control Plane"""

    def get_metrics(self) -> PerformanceMetrics:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
```

### 3. Executor (`executor.py`)

ç®¡ç†æ‰€æœ‰ vLLM å®ä¾‹ï¼š

```python
class ExecutionCoordinator:
    def __init__(self):
        self.instances: dict[str, ExecutionInstance] = {}
        self.http_clients: dict[str, httpx.AsyncClient] = {}

    def register_instance(self, instance: ExecutionInstance):
        """æ³¨å†Œæ–°å®ä¾‹"""

    async def execute_request(
        self,
        request: RequestMetadata,
        instance: ExecutionInstance,
        decision: SchedulingDecision,
    ) -> dict:
        """æ‰§è¡Œè¯·æ±‚ (HTTP API è°ƒç”¨)"""

    async def health_check(self, instance_id: str) -> bool:
        """å¥åº·æ£€æŸ¥"""

    def get_healthy_instances(self) -> list[ExecutionInstance]:
        """è·å–å¥åº·çš„å®ä¾‹åˆ—è¡¨"""
```

### 4. Policies (`policies.py`)

äº”ç§è°ƒåº¦ç­–ç•¥ï¼š

```python
class SchedulingPolicy(ABC):
    @abstractmethod
    async def schedule(
        self,
        request: RequestMetadata,
        instances: list[ExecutionInstance],
    ) -> SchedulingDecision:
        """è°ƒåº¦é€»è¾‘"""

# å…·ä½“å®ç°
class FIFOPolicy(SchedulingPolicy): ...
class PriorityPolicy(SchedulingPolicy): ...
class SLOAwarePolicy(SchedulingPolicy): ...
class CostOptimizedPolicy(SchedulingPolicy): ...
class AdaptivePolicy(SchedulingPolicy): ...
```

### 5. PD Routing (`pd_routing.py`)

Prefilling/Decoding åˆ†ç¦»è·¯ç”±ï¼š

```python
class PDRoutingStrategy:
    def determine_request_phase(self, request: RequestMetadata) -> str:
        """ç¡®å®šè¯·æ±‚é˜¶æ®µ"""
        if len(request.prompt) > self.config.prefilling_threshold_input_tokens:
            return "prefilling"
        return "decoding"

    def get_instance_specialization(self, instance: ExecutionInstance) -> float:
        """è®¡ç®—å®ä¾‹ä¸“ä¸šåŒ–å¾—åˆ†"""
```

### 6. Router (`router.py`)

è¯·æ±‚è·¯ç”±å’Œè´Ÿè½½å‡è¡¡ï¼š

```python
class RequestRouter:
    def __init__(self, strategy: str = "load_balanced"):
        self.strategy = strategy

    async def select_instance(
        self,
        request: RequestMetadata,
        available_instances: list[ExecutionInstance],
        phase: Optional[str] = None,
    ) -> ExecutionInstance:
        """é€‰æ‹©å®ä¾‹"""

class LoadBalancer:
    def get_least_loaded_instance(
        self, instances: list[ExecutionInstance]
    ) -> ExecutionInstance:
        """è·å–è´Ÿè½½æœ€ä½çš„å®ä¾‹"""
```

### 7. Parallelism (`parallelism.py`)

å¹¶è¡Œç­–ç•¥ä¼˜åŒ–ï¼š

```python
class ParallelismOptimizer:
    def recommend_strategy(
        self,
        model_size_gb: float,
        gpu_count: int,
        gpu_memory_gb: float,
    ) -> ParallelismConfig:
        """æ¨èå¹¶è¡Œç­–ç•¥"""

    def optimize_hybrid_config(
        self, gpu_count: int
    ) -> tuple[int, int, int]:
        """ä¼˜åŒ–æ··åˆå¹¶è¡Œé…ç½® (TP, PP, DP)"""
```

## é…ç½®ç¤ºä¾‹

### åŸºç¡€é…ç½®

```python
from control_plane import ControlPlaneManager, ExecutionInstance

# æœ€ç®€é…ç½®
manager = ControlPlaneManager(
    scheduling_policy="fifo",
    enable_pd_separation=False
)

instance = ExecutionInstance(
    instance_id="vllm-1",
    host="localhost",
    port=8000,
    model_name="meta-llama/Llama-2-7b",
    tensor_parallel_size=1,
    gpu_count=1
)

manager.register_instance(instance)
```

### PD åˆ†ç¦»é…ç½®

```python
from control_plane import (
    ControlPlaneManager,
    ExecutionInstance,
    ExecutionInstanceType,
    PDSeparationConfig,
    PrefillingConfig,
    DecodingConfig,
)

# å¯ç”¨ PD åˆ†ç¦»
pd_config = PDSeparationConfig(
    enabled=True,
    routing_policy="adaptive",
    prefilling_threshold_input_tokens=2048,  # è¶…è¿‡ 2048 tokens è§†ä¸º prefilling
)

manager = ControlPlaneManager(
    scheduling_policy="adaptive",
    enable_pd_separation=True,
    pd_config=pd_config,
)

# Prefilling ä¼˜åŒ–å®ä¾‹
prefilling_instance = ExecutionInstance(
    instance_id="prefilling-1",
    host="localhost",
    port=8000,
    model_name="meta-llama/Llama-2-70b",
    instance_type=ExecutionInstanceType.PREFILLING,
    tensor_parallel_size=8,  # é«˜ TP ä»¥æé«˜åå
    gpu_count=8,
    prefilling_config=PrefillingConfig(
        target_batch_size=64,
        tensor_parallel_size=8,
        enable_chunked_prefill=True,
    ),
)

# Decoding ä¼˜åŒ–å®ä¾‹
decoding_instance = ExecutionInstance(
    instance_id="decoding-1",
    host="localhost",
    port=8001,
    model_name="meta-llama/Llama-2-70b",
    instance_type=ExecutionInstanceType.DECODING,
    tensor_parallel_size=2,  # ä½ TP ä»¥é™ä½å»¶è¿Ÿ
    gpu_count=2,
    decoding_config=DecodingConfig(
        target_latency_ms=50,
        max_parallel_requests=200,
    ),
)

manager.register_instance(prefilling_instance)
manager.register_instance(decoding_instance)
```

### ç”Ÿäº§ç¯å¢ƒé…ç½®

```python
# ç”Ÿäº§ç¯å¢ƒå®Œæ•´é…ç½®
manager = ControlPlaneManager(
    scheduling_policy="adaptive",
    routing_strategy="affinity",  # ç”¨æˆ·äº²å’Œæ€§ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ç‡
    enable_pd_separation=True,
    enable_monitoring=True,
    enable_auto_scaling=False,  # ç›®å‰ä¸æ”¯æŒï¼Œæœªæ¥ç‰ˆæœ¬
)

# å¤šä¸ªå®ä¾‹ï¼Œä¸åŒå¹¶è¡Œç­–ç•¥
instances = [
    # TP=4 å®ä¾‹ (é«˜åå prefilling)
    ExecutionInstance(
        instance_id="vllm-tp4",
        host="192.168.1.100",
        port=8000,
        model_name="llama-3-70b",
        tensor_parallel_size=4,
        gpu_count=4,
        instance_type="prefilling",
    ),
    # TP=2, PP=2 æ··åˆå¹¶è¡Œ (å¤§æ¨¡å‹)
    ExecutionInstance(
        instance_id="vllm-hybrid",
        host="192.168.1.101",
        port=8000,
        model_name="llama-3-70b",
        tensor_parallel_size=2,
        pipeline_parallel_size=2,
        gpu_count=4,
        instance_type="general",
    ),
    # DP=2 æ•°æ®å¹¶è¡Œ (é«˜å¹¶å‘ decoding)
    ExecutionInstance(
        instance_id="vllm-dp2",
        host="192.168.1.102",
        port=8000,
        model_name="llama-3-70b",
        data_parallel_size=2,
        tensor_parallel_size=2,
        gpu_count=4,
        instance_type="decoding",
    ),
]

for instance in instances:
    manager.register_instance(instance)
```

## å¸¸è§é—®é¢˜ (FAQ)

**Q: Control Plane ä¸ vLLM å†…ç½®çš„è¯·æ±‚é˜Ÿåˆ—æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**

A: Control Plane åœ¨ vLLM é˜Ÿåˆ—ä¹‹ä¸Šæ·»åŠ äº†é«˜çº§è·¯ç”±å’Œè°ƒåº¦ç­–ç•¥ã€‚å®ƒåè°ƒå¤šä¸ª vLLM å®ä¾‹ï¼Œåº”ç”¨ç‰¹å®šé¢†åŸŸçš„ç­–ç•¥ï¼ˆå¦‚ PD åˆ†ç¦»ï¼‰ã€‚vLLM è´Ÿè´£å•å®ä¾‹å†…çš„åŠ¨æ€æ‰¹å¤„ç†ï¼ŒControl
Plane è´Ÿè´£è·¨å®ä¾‹çš„æ™ºèƒ½è°ƒåº¦ã€‚

**Q: æ˜¯å¦æ”¯æŒåŠ¨æ€æ‰¹å¤„ç†ï¼Ÿ**

A: æ˜¯çš„ã€‚Control Plane ä¸ vLLM çš„åŠ¨æ€æ‰¹å¤„ç†ååŒå·¥ä½œã€‚è°ƒåº¦å™¨å†³å®šå“ªä¸ªå®ä¾‹å¤„ç†è¯·æ±‚ï¼Œç„¶å vLLM åœ¨è¯¥å®ä¾‹ä¸Šè¿›è¡Œæ‰¹å¤„ç†ã€‚

**Q: å¯ä»¥ç”¨äºæ¨ç†è¿˜æ˜¯è®­ç»ƒï¼Ÿ**

A: å½“å‰ç‰ˆæœ¬ä¸“æ³¨äºæ¨ç†åœºæ™¯ã€‚æ¶æ„æ”¯æŒæ‰©å±•åˆ°å¾®è°ƒå·¥ä½œæµï¼Œä½†å°šæœªå®ç°ã€‚

**Q: æ•…éšœæ¢å¤å¦‚ä½•å·¥ä½œï¼Ÿ**

A: å®ä¾‹å…·æœ‰å¥åº·çŠ¶æ€ç›‘æ§ã€‚å¦‚æœå®ä¾‹å˜å¾—ä¸å¥åº·ï¼Œè¯·æ±‚ä¼šæ•…éšœè½¬ç§»åˆ°å¥åº·çš„å®ä¾‹ã€‚æ”¯æŒè‡ªåŠ¨å¥åº·æ£€æŸ¥å’Œå®ä¾‹æ ‡è®°ã€‚

**Q: å¦‚ä½•å¤„ç†æ¨¡å‹åŠ è½½å’Œç¼“å­˜ï¼Ÿ**

A: vLLM å®ä¾‹ç‹¬ç«‹ç®¡ç†æ¨¡å‹åŠ è½½ã€‚Control Plane ä¸å‚ä¸æ¨¡å‹ç®¡ç†ï¼Œåªè´Ÿè´£è¯·æ±‚è·¯ç”±ã€‚å»ºè®®é¢„çƒ­æ‰€æœ‰å®ä¾‹ã€‚

**Q: æ”¯æŒæµå¼è¾“å‡ºå—ï¼Ÿ**

A: æ”¯æŒã€‚é€šè¿‡ HTTP API çš„ `stream=True` å‚æ•°å¯ä»¥å¯ç”¨æµå¼è¾“å‡ºã€‚Control Plane ä¼šå°†æµå¼å“åº”é€ä¼ ç»™å®¢æˆ·ç«¯ã€‚

**Q: å¦‚ä½•ç›‘æ§æ€§èƒ½ï¼Ÿ**

A: ä½¿ç”¨ `manager.get_metrics()` è·å–å®æ—¶æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ï¼š

- è¯·æ±‚ç»Ÿè®¡ï¼ˆæ€»æ•°ã€å®Œæˆã€å¤±è´¥ï¼‰
- å»¶è¿Ÿåˆ†å¸ƒï¼ˆå¹³å‡ã€P95ã€P99ï¼‰
- ååé‡ï¼ˆtokens/sã€requests/sï¼‰
- SLO åˆè§„ç‡
- GPU åˆ©ç”¨ç‡

**Q: æ˜¯å¦æ”¯æŒå¤šæ¨¡å‹ï¼Ÿ**

A: å½“å‰ç‰ˆæœ¬æ¯ä¸ªå®ä¾‹è¿è¡Œä¸€ä¸ªæ¨¡å‹ã€‚å¯ä»¥æ³¨å†Œè¿è¡Œä¸åŒæ¨¡å‹çš„å¤šä¸ªå®ä¾‹ï¼ŒControl Plane ä¼šæ ¹æ®è¯·æ±‚ä¸­çš„ `model_name` è¿›è¡Œè·¯ç”±ã€‚

**Q: å¦‚ä½•é€‰æ‹©è°ƒåº¦ç­–ç•¥ï¼Ÿ**

A:

- **ç”Ÿäº§ç¯å¢ƒ**: ä½¿ç”¨ `adaptive`ï¼ˆè‡ªåŠ¨é€‚åº”ï¼‰
- **ä¸¥æ ¼ SLO**: ä½¿ç”¨ `slo_aware`
- **æˆæœ¬æ•æ„Ÿ**: ä½¿ç”¨ `cost_optimized`
- **ç®€å•åœºæ™¯**: ä½¿ç”¨ `fifo`
- **å¤šä¼˜å…ˆçº§**: ä½¿ç”¨ `priority`

**Q: PD åˆ†ç¦»é€‚åˆæ‰€æœ‰åœºæ™¯å—ï¼Ÿ**

A: ä¸ä¸€å®šã€‚PD åˆ†ç¦»åœ¨ä»¥ä¸‹åœºæ™¯æœ€æœ‰æ•ˆï¼š

- âœ… è¾“å…¥é•¿åº¦å·®å¼‚å¤§ï¼ˆæœ‰äº›å¾ˆé•¿ï¼Œæœ‰äº›å¾ˆçŸ­ï¼‰
- âœ… æœ‰è¶³å¤Ÿ GPU èµ„æºè¿è¡Œå¤šä¸ªå®ä¾‹
- âœ… éœ€è¦åŒæ—¶ä¼˜åŒ–ååå’Œå»¶è¿Ÿ
- âŒ è¾“å…¥é•¿åº¦å‡åŒ€çš„åœºæ™¯å¯èƒ½ä¸éœ€è¦

## ä¸‹ä¸€æ­¥è®¡åˆ’

### å·²å®Œæˆ âœ…

- [x] 5 ç§è°ƒåº¦ç­–ç•¥
- [x] PD åˆ†ç¦»è·¯ç”±
- [x] å¤šç§å¹¶è¡Œç­–ç•¥æ”¯æŒ
- [x] è´Ÿè½½å‡è¡¡å’Œè·¯ç”±
- [x] å¥åº·æ£€æŸ¥
- [x] æ€§èƒ½ç›‘æ§
- [x] å®Œæ•´çš„å•å…ƒæµ‹è¯•

### è§„åˆ’ä¸­ ğŸš€

1. **è‡ªåŠ¨ä¼¸ç¼©**: æ ¹æ®è´Ÿè½½è‡ªåŠ¨æ‰©ç¼©å®¹ vLLM å®ä¾‹
1. **åˆ†å¸ƒå¼åè°ƒ**: æ”¯æŒå¤šèŠ‚ç‚¹éƒ¨ç½²çš„åˆ†å¸ƒå¼åè°ƒ
1. **è¯·æ±‚è¿½è¸ª**: æ·»åŠ åˆ†å¸ƒå¼è¿½è¸ªï¼ˆOpenTelemetryï¼‰
1. **æŒ‡æ ‡å¯¼å‡º**: Prometheus/CloudWatch é›†æˆ
1. **æ™ºèƒ½ç¼“å­˜**: KV cache è·¨å®ä¾‹å…±äº«
1. **å¤šæ¨¡å‹æ”¯æŒ**: åŒæ—¶ç®¡ç†å¤šä¸ªä¸åŒæ¨¡å‹çš„é«˜çº§ç‰¹æ€§
1. **æˆæœ¬é¢„æµ‹**: åŸºäºå†å²æ•°æ®çš„æˆæœ¬é¢„æµ‹
1. **A/B æµ‹è¯•**: ç­–ç•¥å¯¹æ¯”å’Œæ€§èƒ½æµ‹è¯•æ¡†æ¶

## å‚è€ƒèµ„æº

- **[sageLLM ä¸» README](../README.md)** - é¡¹ç›®æ¦‚è¿°
- **[éƒ¨ç½²æŒ‡å—](./DEPLOYMENT.md)** - vLLM å®ä¾‹éƒ¨ç½²
- **[vLLM æ–‡æ¡£](https://docs.vllm.ai/)** - vLLM å®˜æ–¹æ–‡æ¡£
- **[ç¤ºä¾‹ä»£ç ](../control_plane/examples/)** - å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
  - [HTTP å®¢æˆ·ç«¯æ¨¡å¼](../control_plane/examples/example_http_client.py) - å®é™…éƒ¨ç½²åœºæ™¯
  - [å®Œæ•´æ¼”ç¤º](../control_plane/examples/demo_control_plane.py) - åŠŸèƒ½æ¼”ç¤º
