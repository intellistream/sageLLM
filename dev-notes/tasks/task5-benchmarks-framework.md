# Task 5: benchmarks/ ç»Ÿä¸€è¯„æµ‹æ¡†æ¶

**çŠ¶æ€**: ğŸ”² å¾…å¼€å§‹  
**é¢„è®¡æ—¶é—´**: 3h  
**è¯¾é¢˜å¯¹åº”**: 4.1-4.3 è¯„æµ‹æŒ‡æ ‡  
**å¯å¹¶è¡Œ**: âœ… æ˜¯ï¼ˆä¸ Task 1-4 å¹¶è¡Œï¼‰

---

## èƒŒæ™¯

è¯¾é¢˜ 4.1-4.3 éœ€è¦ç»Ÿä¸€çš„è¯„æµ‹æŒ‡æ ‡ï¼š
- **4.1**: é€šä¿¡æ•ˆç‡ã€PD åˆ†ç¦»æ”¶ç›Š
- **4.2**: KV å‘½ä¸­ç‡ã€è¿ç§»æµé‡
- **4.3**: é‡åŒ–è¯¯å·®ã€ç¨€ç–åŠ é€Ÿæ¯”ã€MFU

æœ¬ä»»åŠ¡åˆ›å»º `benchmarks/` æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„è¯„æµ‹æ¡†æ¶ã€‚

---

## å·¥ä½œç›®å½•

```
/home/shuhao/SAGE/packages/sage-common/src/sage/common/components/sage_llm/sageLLM/benchmarks/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ metrics/                 # æŒ‡æ ‡å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ throughput.py       # ååé‡æŒ‡æ ‡
â”‚   â”œâ”€â”€ latency.py          # å»¶è¿ŸæŒ‡æ ‡
â”‚   â”œâ”€â”€ memory.py           # å†…å­˜æŒ‡æ ‡
â”‚   â”œâ”€â”€ kv_cache.py         # KV ç¼“å­˜æŒ‡æ ‡
â”‚   â””â”€â”€ mfu.py              # MFU è®¡ç®—
â”œâ”€â”€ profiler/               # æ€§èƒ½åˆ†æ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ trace.py            # æ‰§è¡Œè¿½è¸ª
â”œâ”€â”€ ci/                     # CI é›†æˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ gates.py            # æ€§èƒ½é—¨æ§
â””â”€â”€ reporters/              # æŠ¥å‘Šç”Ÿæˆ
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ console.py          # æ§åˆ¶å°è¾“å‡º
    â””â”€â”€ json_reporter.py    # JSON æŠ¥å‘Š
```

---

## å‚è€ƒèµ„æ–™

- vLLM Benchmarks: https://github.com/vllm-project/vllm/tree/main/benchmarks
- MLPerf Inference: https://github.com/mlcommons/inference
- LLMPerf: https://github.com/ray-project/llmperf
- SAGE benchmark_control_plane: `packages/sage-benchmark/`

---

## ä»»åŠ¡æ¸…å•

### 1. åŸºç¡€æŒ‡æ ‡å®šä¹‰ (`metrics/__init__.py`)

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Generic, TypeVar
from enum import Enum, auto
import time

T = TypeVar("T")


class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹"""
    THROUGHPUT = auto()      # ååé‡
    LATENCY = auto()         # å»¶è¿Ÿ
    MEMORY = auto()          # å†…å­˜
    KV_CACHE = auto()        # KV ç¼“å­˜
    COMPUTE = auto()         # è®¡ç®—æ•ˆç‡
    COMMUNICATION = auto()   # é€šä¿¡æ•ˆç‡


@dataclass
class MetricValue:
    """æŒ‡æ ‡å€¼"""
    name: str
    value: float
    unit: str
    timestamp: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def __str__(self) -> str:
        return f"{self.name}: {self.value:.4f} {self.unit}"


@dataclass
class MetricSummary:
    """æŒ‡æ ‡æ‘˜è¦ï¼ˆèšåˆå¤šæ¬¡æµ‹é‡ï¼‰"""
    name: str
    mean: float
    std: float
    min: float
    max: float
    p50: float
    p90: float
    p99: float
    count: int
    unit: str
    
    @classmethod
    def from_values(cls, name: str, values: List[float], unit: str) -> "MetricSummary":
        """ä»å€¼åˆ—è¡¨åˆ›å»ºæ‘˜è¦"""
        import numpy as np
        arr = np.array(values)
        return cls(
            name=name,
            mean=float(arr.mean()),
            std=float(arr.std()),
            min=float(arr.min()),
            max=float(arr.max()),
            p50=float(np.percentile(arr, 50)),
            p90=float(np.percentile(arr, 90)),
            p99=float(np.percentile(arr, 99)),
            count=len(values),
            unit=unit,
        )


class Metric(ABC, Generic[T]):
    """æŒ‡æ ‡åŸºç±»"""
    
    @property
    @abstractmethod
    def name(self) -> str:
        """æŒ‡æ ‡åç§°"""
        ...
    
    @property
    @abstractmethod
    def unit(self) -> str:
        """æŒ‡æ ‡å•ä½"""
        ...
    
    @property
    @abstractmethod
    def metric_type(self) -> MetricType:
        """æŒ‡æ ‡ç±»å‹"""
        ...
    
    @abstractmethod
    def compute(self, *args, **kwargs) -> T:
        """è®¡ç®—æŒ‡æ ‡å€¼"""
        ...
    
    def to_metric_value(self, value: float) -> MetricValue:
        """è½¬æ¢ä¸º MetricValue"""
        return MetricValue(
            name=self.name,
            value=value,
            unit=self.unit,
        )


class MetricRegistry:
    """æŒ‡æ ‡æ³¨å†Œè¡¨"""
    
    _metrics: Dict[str, type] = {}
    
    @classmethod
    def register(cls, name: str):
        """è£…é¥°å™¨ï¼šæ³¨å†ŒæŒ‡æ ‡"""
        def decorator(metric_cls):
            cls._metrics[name] = metric_cls
            return metric_cls
        return decorator
    
    @classmethod
    def get(cls, name: str) -> Metric:
        """è·å–æŒ‡æ ‡å®ä¾‹"""
        if name not in cls._metrics:
            raise ValueError(f"Unknown metric: {name}")
        return cls._metrics[name]()
    
    @classmethod
    def list_all(cls) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æŒ‡æ ‡"""
        return list(cls._metrics.keys())
```

### 2. ååé‡æŒ‡æ ‡ (`metrics/throughput.py`)

```python
from dataclasses import dataclass
from typing import Optional
import time

from . import Metric, MetricType, MetricRegistry, MetricValue


@dataclass
class ThroughputResult:
    """ååé‡ç»“æœ"""
    tokens_per_second: float
    requests_per_second: float
    total_tokens: int
    total_requests: int
    duration_s: float


@MetricRegistry.register("throughput")
class ThroughputMetric(Metric[ThroughputResult]):
    """ååé‡æŒ‡æ ‡
    
    æµ‹é‡ï¼š
    - Tokens/s (TPS)
    - Requests/s (QPS)
    """
    
    def __init__(self):
        self._start_time: Optional[float] = None
        self._total_tokens = 0
        self._total_requests = 0
    
    @property
    def name(self) -> str:
        return "throughput"
    
    @property
    def unit(self) -> str:
        return "tokens/s"
    
    @property
    def metric_type(self) -> MetricType:
        return MetricType.THROUGHPUT
    
    def start(self) -> None:
        """å¼€å§‹è®¡æ—¶"""
        self._start_time = time.perf_counter()
        self._total_tokens = 0
        self._total_requests = 0
    
    def record(self, tokens: int, requests: int = 1) -> None:
        """è®°å½•ç”Ÿæˆçš„ token æ•°"""
        self._total_tokens += tokens
        self._total_requests += requests
    
    def compute(self) -> ThroughputResult:
        """è®¡ç®—ååé‡"""
        if self._start_time is None:
            raise RuntimeError("Call start() first")
        
        duration = time.perf_counter() - self._start_time
        
        return ThroughputResult(
            tokens_per_second=self._total_tokens / duration if duration > 0 else 0,
            requests_per_second=self._total_requests / duration if duration > 0 else 0,
            total_tokens=self._total_tokens,
            total_requests=self._total_requests,
            duration_s=duration,
        )


@MetricRegistry.register("decode_throughput")
class DecodeThroughputMetric(Metric[float]):
    """Decode é˜¶æ®µååé‡
    
    å•ç‹¬æµ‹é‡ decodeï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰é˜¶æ®µçš„ååé‡ï¼Œ
    æ’é™¤ prefill çš„å½±å“ã€‚
    """
    
    @property
    def name(self) -> str:
        return "decode_throughput"
    
    @property
    def unit(self) -> str:
        return "tokens/s"
    
    @property
    def metric_type(self) -> MetricType:
        return MetricType.THROUGHPUT
    
    def compute(
        self,
        decode_tokens: int,
        decode_time_s: float,
    ) -> float:
        """è®¡ç®— decode ååé‡"""
        if decode_time_s <= 0:
            return 0.0
        return decode_tokens / decode_time_s
```

### 3. å»¶è¿ŸæŒ‡æ ‡ (`metrics/latency.py`)

```python
from dataclasses import dataclass, field
from typing import List, Optional
import time

from . import Metric, MetricType, MetricRegistry, MetricSummary


@dataclass
class LatencyResult:
    """å»¶è¿Ÿç»“æœ"""
    ttft_ms: float      # Time To First Token
    tpot_ms: float      # Time Per Output Token
    e2e_ms: float       # End-to-End latency
    prefill_ms: float   # Prefill é˜¶æ®µå»¶è¿Ÿ
    decode_ms: float    # Decode é˜¶æ®µæ€»å»¶è¿Ÿ


@MetricRegistry.register("latency")
class LatencyMetric(Metric[LatencyResult]):
    """å»¶è¿ŸæŒ‡æ ‡
    
    æµ‹é‡ï¼š
    - TTFT (Time To First Token): é¦– token å»¶è¿Ÿ
    - TPOT (Time Per Output Token): å¹³å‡æ¯ token å»¶è¿Ÿ
    - E2E (End-to-End): ç«¯åˆ°ç«¯å»¶è¿Ÿ
    """
    
    def __init__(self):
        self._request_start: Optional[float] = None
        self._first_token_time: Optional[float] = None
        self._prefill_end: Optional[float] = None
        self._decode_token_count = 0
    
    @property
    def name(self) -> str:
        return "latency"
    
    @property
    def unit(self) -> str:
        return "ms"
    
    @property
    def metric_type(self) -> MetricType:
        return MetricType.LATENCY
    
    def request_start(self) -> None:
        """è¯·æ±‚å¼€å§‹"""
        self._request_start = time.perf_counter()
        self._first_token_time = None
        self._prefill_end = None
        self._decode_token_count = 0
    
    def prefill_done(self) -> None:
        """Prefill å®Œæˆ"""
        self._prefill_end = time.perf_counter()
    
    def first_token(self) -> None:
        """é¦– token ç”Ÿæˆ"""
        if self._first_token_time is None:
            self._first_token_time = time.perf_counter()
    
    def token_generated(self) -> None:
        """Token ç”Ÿæˆ"""
        self._decode_token_count += 1
    
    def compute(self) -> LatencyResult:
        """è®¡ç®—å»¶è¿ŸæŒ‡æ ‡"""
        now = time.perf_counter()
        
        if self._request_start is None:
            raise RuntimeError("Call request_start() first")
        
        # TTFT
        ttft = (self._first_token_time - self._request_start) * 1000 if self._first_token_time else 0
        
        # Prefill
        prefill = (self._prefill_end - self._request_start) * 1000 if self._prefill_end else 0
        
        # Decode
        decode_start = self._first_token_time or self._request_start
        decode = (now - decode_start) * 1000
        
        # TPOT
        tpot = decode / self._decode_token_count if self._decode_token_count > 0 else 0
        
        # E2E
        e2e = (now - self._request_start) * 1000
        
        return LatencyResult(
            ttft_ms=ttft,
            tpot_ms=tpot,
            e2e_ms=e2e,
            prefill_ms=prefill,
            decode_ms=decode,
        )


@MetricRegistry.register("latency_percentiles")
class LatencyPercentilesMetric(Metric[MetricSummary]):
    """å»¶è¿Ÿåˆ†ä½æ•°æŒ‡æ ‡
    
    èšåˆå¤šæ¬¡æµ‹é‡ï¼Œè®¡ç®— P50/P90/P99ã€‚
    """
    
    def __init__(self):
        self._values: List[float] = []
    
    @property
    def name(self) -> str:
        return "latency_percentiles"
    
    @property
    def unit(self) -> str:
        return "ms"
    
    @property
    def metric_type(self) -> MetricType:
        return MetricType.LATENCY
    
    def record(self, latency_ms: float) -> None:
        """è®°å½•ä¸€æ¬¡å»¶è¿Ÿ"""
        self._values.append(latency_ms)
    
    def reset(self) -> None:
        """é‡ç½®"""
        self._values.clear()
    
    def compute(self) -> MetricSummary:
        """è®¡ç®—åˆ†ä½æ•°"""
        if not self._values:
            raise RuntimeError("No values recorded")
        
        return MetricSummary.from_values(
            name=self.name,
            values=self._values,
            unit=self.unit,
        )
```

### 4. KV ç¼“å­˜æŒ‡æ ‡ (`metrics/kv_cache.py`)

```python
from dataclasses import dataclass
from typing import Dict

from . import Metric, MetricType, MetricRegistry


@dataclass
class KVCacheResult:
    """KV ç¼“å­˜æŒ‡æ ‡ç»“æœ"""
    # å‘½ä¸­ç‡
    hit_rate: float              # æ€»ä½“å‘½ä¸­ç‡
    prefix_hit_rate: float       # å‰ç¼€å‘½ä¸­ç‡
    
    # å†…å­˜ä½¿ç”¨
    hbm_used_gb: float
    ddr_used_gb: float
    nvme_used_gb: float
    total_used_gb: float
    
    # è¿ç§»
    migration_count: int
    migration_bytes: int
    
    # å¤ç”¨
    reused_tokens: int
    total_tokens: int
    reuse_ratio: float


@MetricRegistry.register("kv_cache")
class KVCacheMetric(Metric[KVCacheResult]):
    """KV ç¼“å­˜æŒ‡æ ‡
    
    æµ‹é‡ï¼š
    - å‘½ä¸­ç‡
    - å†…å­˜ä½¿ç”¨ï¼ˆæŒ‰å±‚çº§ï¼‰
    - è¿ç§»æµé‡
    - å¤ç”¨ç‡
    """
    
    def __init__(self):
        self._hits = 0
        self._misses = 0
        self._prefix_hits = 0
        self._prefix_lookups = 0
        self._migrations = 0
        self._migration_bytes = 0
        self._reused_tokens = 0
        self._total_tokens = 0
        self._tier_usage: Dict[str, float] = {}
    
    @property
    def name(self) -> str:
        return "kv_cache"
    
    @property
    def unit(self) -> str:
        return ""  # å¤šç§å•ä½
    
    @property
    def metric_type(self) -> MetricType:
        return MetricType.KV_CACHE
    
    def record_hit(self) -> None:
        """è®°å½•ç¼“å­˜å‘½ä¸­"""
        self._hits += 1
    
    def record_miss(self) -> None:
        """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
        self._misses += 1
    
    def record_prefix_lookup(self, hit: bool) -> None:
        """è®°å½•å‰ç¼€æŸ¥æ‰¾"""
        self._prefix_lookups += 1
        if hit:
            self._prefix_hits += 1
    
    def record_migration(self, bytes_migrated: int) -> None:
        """è®°å½•è¿ç§»"""
        self._migrations += 1
        self._migration_bytes += bytes_migrated
    
    def record_reuse(self, reused: int, total: int) -> None:
        """è®°å½• token å¤ç”¨"""
        self._reused_tokens += reused
        self._total_tokens += total
    
    def update_tier_usage(self, tier: str, used_gb: float) -> None:
        """æ›´æ–°å±‚çº§ä½¿ç”¨"""
        self._tier_usage[tier] = used_gb
    
    def compute(self) -> KVCacheResult:
        """è®¡ç®— KV ç¼“å­˜æŒ‡æ ‡"""
        total_lookups = self._hits + self._misses
        hit_rate = self._hits / total_lookups if total_lookups > 0 else 0
        
        prefix_hit_rate = self._prefix_hits / self._prefix_lookups if self._prefix_lookups > 0 else 0
        
        reuse_ratio = self._reused_tokens / self._total_tokens if self._total_tokens > 0 else 0
        
        return KVCacheResult(
            hit_rate=hit_rate,
            prefix_hit_rate=prefix_hit_rate,
            hbm_used_gb=self._tier_usage.get("HBM", 0),
            ddr_used_gb=self._tier_usage.get("DDR", 0),
            nvme_used_gb=self._tier_usage.get("NVME", 0),
            total_used_gb=sum(self._tier_usage.values()),
            migration_count=self._migrations,
            migration_bytes=self._migration_bytes,
            reused_tokens=self._reused_tokens,
            total_tokens=self._total_tokens,
            reuse_ratio=reuse_ratio,
        )
```

### 5. MFU è®¡ç®— (`metrics/mfu.py`)

```python
from dataclasses import dataclass
from typing import Optional

from . import Metric, MetricType, MetricRegistry


@dataclass
class MFUResult:
    """MFU ç»“æœ"""
    mfu: float              # Model FLOPs Utilization (0-1)
    achieved_tflops: float  # å®é™…è¾¾åˆ°çš„ TFLOPS
    peak_tflops: float      # å³°å€¼ TFLOPS
    model_flops: int        # æ¨¡å‹ FLOPs
    duration_s: float       # æµ‹é‡æ—¶é—´


@MetricRegistry.register("mfu")
class MFUMetric(Metric[MFUResult]):
    """Model FLOPs Utilization (MFU) æŒ‡æ ‡
    
    MFU = å®é™… FLOPs / ç†è®ºå³°å€¼ FLOPs
    
    å¯¹äº Transformer:
    - Forward FLOPs â‰ˆ 2 * params * tokens
    - Backward FLOPs â‰ˆ 4 * params * tokens
    - Attention FLOPs = 4 * n_layers * n_heads * d_head * seq_len^2
    """
    
    @property
    def name(self) -> str:
        return "mfu"
    
    @property
    def unit(self) -> str:
        return "%"
    
    @property
    def metric_type(self) -> MetricType:
        return MetricType.COMPUTE
    
    def compute(
        self,
        model_params: int,
        tokens_processed: int,
        duration_s: float,
        peak_tflops: float,
        n_layers: Optional[int] = None,
        n_heads: Optional[int] = None,
        d_head: Optional[int] = None,
        seq_len: Optional[int] = None,
        include_attention: bool = True,
    ) -> MFUResult:
        """è®¡ç®— MFU
        
        Args:
            model_params: æ¨¡å‹å‚æ•°é‡
            tokens_processed: å¤„ç†çš„ token æ•°
            duration_s: è€—æ—¶ï¼ˆç§’ï¼‰
            peak_tflops: ç¡¬ä»¶å³°å€¼ TFLOPS
            n_layers: å±‚æ•°ï¼ˆç”¨äºæ³¨æ„åŠ›è®¡ç®—ï¼‰
            n_heads: æ³¨æ„åŠ›å¤´æ•°
            d_head: å¤´ç»´åº¦
            seq_len: åºåˆ—é•¿åº¦
            include_attention: æ˜¯å¦åŒ…å«æ³¨æ„åŠ› FLOPs
        """
        # åŸºç¡€ FLOPsï¼ˆçº¿æ€§å±‚ï¼‰
        # Forward: 2 * params * tokens
        linear_flops = 2 * model_params * tokens_processed
        
        # æ³¨æ„åŠ› FLOPs
        attention_flops = 0
        if include_attention and all([n_layers, n_heads, d_head, seq_len]):
            # QKV projection: 3 * 4 * n_heads * d_head * seq_len (per layer)
            # Attention: 2 * n_heads * seq_len^2 * d_head (per layer)
            # Output projection: 4 * n_heads * d_head * seq_len (per layer)
            attention_flops = n_layers * (
                3 * 4 * n_heads * d_head * seq_len +
                2 * n_heads * seq_len * seq_len * d_head +
                4 * n_heads * d_head * seq_len
            )
        
        total_flops = linear_flops + attention_flops
        
        # è®¡ç®— achieved TFLOPS
        achieved_tflops = (total_flops / duration_s) / 1e12 if duration_s > 0 else 0
        
        # è®¡ç®— MFU
        mfu = achieved_tflops / peak_tflops if peak_tflops > 0 else 0
        
        return MFUResult(
            mfu=mfu,
            achieved_tflops=achieved_tflops,
            peak_tflops=peak_tflops,
            model_flops=total_flops,
            duration_s=duration_s,
        )


@MetricRegistry.register("mbu")
class MBUMetric(Metric[float]):
    """Memory Bandwidth Utilization (MBU) æŒ‡æ ‡
    
    MBU = å®é™…å†…å­˜å¸¦å®½ / å³°å€¼å†…å­˜å¸¦å®½
    
    å¯¹äºæ¨ç†ï¼ˆmemory-boundï¼‰ï¼š
    - è¯»å–æ‰€æœ‰æƒé‡
    - è¯»å†™ KV cache
    - è¯»å†™æ¿€æ´»
    """
    
    @property
    def name(self) -> str:
        return "mbu"
    
    @property
    def unit(self) -> str:
        return "%"
    
    @property
    def metric_type(self) -> MetricType:
        return MetricType.MEMORY
    
    def compute(
        self,
        bytes_accessed: int,
        duration_s: float,
        peak_bandwidth_gbps: float,
    ) -> float:
        """è®¡ç®— MBU
        
        Args:
            bytes_accessed: è®¿é—®çš„æ€»å­—èŠ‚æ•°
            duration_s: è€—æ—¶ï¼ˆç§’ï¼‰
            peak_bandwidth_gbps: å³°å€¼å¸¦å®½ï¼ˆGB/sï¼‰
        """
        achieved_bandwidth = (bytes_accessed / duration_s) / 1e9 if duration_s > 0 else 0
        mbu = achieved_bandwidth / peak_bandwidth_gbps if peak_bandwidth_gbps > 0 else 0
        return mbu
```

### 6. CI æ€§èƒ½é—¨æ§ (`ci/gates.py`)

```python
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from enum import Enum, auto


class GateStatus(Enum):
    """é—¨æ§çŠ¶æ€"""
    PASSED = auto()
    FAILED = auto()
    SKIPPED = auto()


@dataclass
class GateResult:
    """é—¨æ§æ£€æŸ¥ç»“æœ"""
    name: str
    status: GateStatus
    expected: float
    actual: float
    threshold: float
    message: str
    
    @property
    def passed(self) -> bool:
        return self.status == GateStatus.PASSED


@dataclass
class PerformanceGateConfig:
    """æ€§èƒ½é—¨æ§é…ç½®"""
    # ååé‡é—¨æ§
    min_throughput_tps: Optional[float] = None      # æœ€å°ååé‡
    
    # å»¶è¿Ÿé—¨æ§
    max_ttft_ms: Optional[float] = None             # æœ€å¤§ TTFT
    max_tpot_ms: Optional[float] = None             # æœ€å¤§ TPOT
    max_p99_latency_ms: Optional[float] = None      # æœ€å¤§ P99 å»¶è¿Ÿ
    
    # å†…å­˜é—¨æ§
    max_memory_gb: Optional[float] = None           # æœ€å¤§å†…å­˜ä½¿ç”¨
    
    # KV ç¼“å­˜é—¨æ§
    min_kv_hit_rate: Optional[float] = None         # æœ€å°å‘½ä¸­ç‡
    
    # MFU é—¨æ§
    min_mfu: Optional[float] = None                 # æœ€å° MFU
    
    # å›å½’é—¨æ§ï¼ˆä¸åŸºå‡†æ¯”è¾ƒï¼‰
    max_regression_pct: float = 5.0                 # æœ€å¤§å…è®¸æ€§èƒ½ä¸‹é™ %
    
    # å…ƒæ•°æ®
    baseline_commit: Optional[str] = None           # åŸºå‡† commit
    tags: List[str] = field(default_factory=list)


class PerformanceGate:
    """æ€§èƒ½é—¨æ§
    
    ç”¨äº CI ä¸­çš„æ€§èƒ½æ£€æŸ¥ï¼Œç¡®ä¿æ€§èƒ½ä¸å‘ç”Ÿå›å½’ã€‚
    """
    
    def __init__(self, config: PerformanceGateConfig):
        self.config = config
        self._results: List[GateResult] = []
    
    def check_throughput(self, actual_tps: float) -> GateResult:
        """æ£€æŸ¥ååé‡"""
        if self.config.min_throughput_tps is None:
            return GateResult(
                name="throughput",
                status=GateStatus.SKIPPED,
                expected=0,
                actual=actual_tps,
                threshold=0,
                message="No throughput gate configured",
            )
        
        passed = actual_tps >= self.config.min_throughput_tps
        result = GateResult(
            name="throughput",
            status=GateStatus.PASSED if passed else GateStatus.FAILED,
            expected=self.config.min_throughput_tps,
            actual=actual_tps,
            threshold=self.config.min_throughput_tps,
            message=f"Throughput {actual_tps:.1f} TPS {'>='}{'<'} {self.config.min_throughput_tps:.1f} TPS",
        )
        self._results.append(result)
        return result
    
    def check_latency(
        self,
        ttft_ms: Optional[float] = None,
        tpot_ms: Optional[float] = None,
        p99_ms: Optional[float] = None,
    ) -> List[GateResult]:
        """æ£€æŸ¥å»¶è¿Ÿ"""
        results = []
        
        if ttft_ms is not None and self.config.max_ttft_ms is not None:
            passed = ttft_ms <= self.config.max_ttft_ms
            results.append(GateResult(
                name="ttft",
                status=GateStatus.PASSED if passed else GateStatus.FAILED,
                expected=self.config.max_ttft_ms,
                actual=ttft_ms,
                threshold=self.config.max_ttft_ms,
                message=f"TTFT {ttft_ms:.1f} ms {'<='}{'>'} {self.config.max_ttft_ms:.1f} ms",
            ))
        
        if tpot_ms is not None and self.config.max_tpot_ms is not None:
            passed = tpot_ms <= self.config.max_tpot_ms
            results.append(GateResult(
                name="tpot",
                status=GateStatus.PASSED if passed else GateStatus.FAILED,
                expected=self.config.max_tpot_ms,
                actual=tpot_ms,
                threshold=self.config.max_tpot_ms,
                message=f"TPOT {tpot_ms:.1f} ms {'<='}{'>'} {self.config.max_tpot_ms:.1f} ms",
            ))
        
        if p99_ms is not None and self.config.max_p99_latency_ms is not None:
            passed = p99_ms <= self.config.max_p99_latency_ms
            results.append(GateResult(
                name="p99_latency",
                status=GateStatus.PASSED if passed else GateStatus.FAILED,
                expected=self.config.max_p99_latency_ms,
                actual=p99_ms,
                threshold=self.config.max_p99_latency_ms,
                message=f"P99 {p99_ms:.1f} ms {'<='}{'>'} {self.config.max_p99_latency_ms:.1f} ms",
            ))
        
        self._results.extend(results)
        return results
    
    def check_regression(
        self,
        metric_name: str,
        baseline: float,
        current: float,
        higher_is_better: bool = True,
    ) -> GateResult:
        """æ£€æŸ¥æ€§èƒ½å›å½’
        
        Args:
            metric_name: æŒ‡æ ‡å
            baseline: åŸºå‡†å€¼
            current: å½“å‰å€¼
            higher_is_better: å€¼è¶Šå¤§è¶Šå¥½
        """
        if baseline == 0:
            return GateResult(
                name=f"regression_{metric_name}",
                status=GateStatus.SKIPPED,
                expected=baseline,
                actual=current,
                threshold=0,
                message="No baseline available",
            )
        
        if higher_is_better:
            regression_pct = ((baseline - current) / baseline) * 100
        else:
            regression_pct = ((current - baseline) / baseline) * 100
        
        passed = regression_pct <= self.config.max_regression_pct
        
        result = GateResult(
            name=f"regression_{metric_name}",
            status=GateStatus.PASSED if passed else GateStatus.FAILED,
            expected=baseline,
            actual=current,
            threshold=self.config.max_regression_pct,
            message=f"{metric_name} regression {regression_pct:.1f}% {'<='}{'>'} {self.config.max_regression_pct:.1f}%",
        )
        self._results.append(result)
        return result
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–æ‘˜è¦"""
        passed = sum(1 for r in self._results if r.status == GateStatus.PASSED)
        failed = sum(1 for r in self._results if r.status == GateStatus.FAILED)
        skipped = sum(1 for r in self._results if r.status == GateStatus.SKIPPED)
        
        return {
            "total_checks": len(self._results),
            "passed": passed,
            "failed": failed,
            "skipped": skipped,
            "all_passed": failed == 0,
            "results": [
                {
                    "name": r.name,
                    "status": r.status.name,
                    "message": r.message,
                }
                for r in self._results
            ],
        }
    
    def assert_all_passed(self) -> None:
        """æ–­è¨€æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ˆç”¨äº CIï¼‰"""
        summary = self.get_summary()
        if not summary["all_passed"]:
            failed_msgs = [
                r["message"]
                for r in summary["results"]
                if r["status"] == "FAILED"
            ]
            raise AssertionError(
                f"Performance gate failed:\n" + "\n".join(failed_msgs)
            )
```

### 7. æ§åˆ¶å°æŠ¥å‘Š (`reporters/console.py`)

```python
from typing import Dict, List, Any
from dataclasses import dataclass

from ..metrics import MetricValue, MetricSummary


@dataclass
class ConsoleReporterConfig:
    """æ§åˆ¶å°æŠ¥å‘Šé…ç½®"""
    use_color: bool = True
    show_percentiles: bool = True
    precision: int = 4


class ConsoleReporter:
    """æ§åˆ¶å°æŠ¥å‘Šå™¨"""
    
    COLORS = {
        "green": "\033[92m",
        "red": "\033[91m",
        "yellow": "\033[93m",
        "blue": "\033[94m",
        "reset": "\033[0m",
    }
    
    def __init__(self, config: ConsoleReporterConfig = None):
        self.config = config or ConsoleReporterConfig()
    
    def _color(self, text: str, color: str) -> str:
        if not self.config.use_color:
            return text
        return f"{self.COLORS.get(color, '')}{text}{self.COLORS['reset']}"
    
    def report_metric(self, metric: MetricValue) -> str:
        """æŠ¥å‘Šå•ä¸ªæŒ‡æ ‡"""
        return f"  {metric.name}: {metric.value:.{self.config.precision}f} {metric.unit}"
    
    def report_summary(self, summary: MetricSummary) -> str:
        """æŠ¥å‘ŠæŒ‡æ ‡æ‘˜è¦"""
        lines = [
            f"  {summary.name}:",
            f"    mean: {summary.mean:.{self.config.precision}f} {summary.unit}",
            f"    std:  {summary.std:.{self.config.precision}f} {summary.unit}",
            f"    min:  {summary.min:.{self.config.precision}f} {summary.unit}",
            f"    max:  {summary.max:.{self.config.precision}f} {summary.unit}",
        ]
        
        if self.config.show_percentiles:
            lines.extend([
                f"    p50:  {summary.p50:.{self.config.precision}f} {summary.unit}",
                f"    p90:  {summary.p90:.{self.config.precision}f} {summary.unit}",
                f"    p99:  {summary.p99:.{self.config.precision}f} {summary.unit}",
            ])
        
        lines.append(f"    count: {summary.count}")
        
        return "\n".join(lines)
    
    def report_benchmark(
        self,
        name: str,
        metrics: Dict[str, Any],
        duration_s: float,
    ) -> str:
        """æŠ¥å‘Šå®Œæ•´ benchmark ç»“æœ"""
        lines = [
            self._color(f"\n{'='*60}", "blue"),
            self._color(f"Benchmark: {name}", "blue"),
            self._color(f"{'='*60}", "blue"),
            f"Duration: {duration_s:.2f}s",
            "",
            "Metrics:",
        ]
        
        for key, value in metrics.items():
            if isinstance(value, MetricValue):
                lines.append(self.report_metric(value))
            elif isinstance(value, MetricSummary):
                lines.append(self.report_summary(value))
            elif isinstance(value, (int, float)):
                lines.append(f"  {key}: {value:.{self.config.precision}f}")
            else:
                lines.append(f"  {key}: {value}")
        
        lines.append(self._color(f"{'='*60}\n", "blue"))
        
        return "\n".join(lines)
    
    def report_gate_results(self, summary: Dict[str, Any]) -> str:
        """æŠ¥å‘Šé—¨æ§ç»“æœ"""
        lines = [
            self._color("\nPerformance Gate Results:", "blue"),
            f"  Total: {summary['total_checks']}",
        ]
        
        if summary['passed'] > 0:
            lines.append(self._color(f"  Passed: {summary['passed']}", "green"))
        if summary['failed'] > 0:
            lines.append(self._color(f"  Failed: {summary['failed']}", "red"))
        if summary['skipped'] > 0:
            lines.append(self._color(f"  Skipped: {summary['skipped']}", "yellow"))
        
        lines.append("")
        
        for result in summary['results']:
            if result['status'] == 'PASSED':
                icon = self._color("âœ“", "green")
            elif result['status'] == 'FAILED':
                icon = self._color("âœ—", "red")
            else:
                icon = self._color("-", "yellow")
            
            lines.append(f"  {icon} {result['message']}")
        
        return "\n".join(lines)
```

---

## å•å…ƒæµ‹è¯•è¦æ±‚

åˆ›å»º `tests/unit/test_benchmarks.py`ï¼š

```python
import pytest
import time
from sageLLM.benchmarks.metrics import MetricRegistry, MetricSummary
from sageLLM.benchmarks.metrics.throughput import ThroughputMetric
from sageLLM.benchmarks.metrics.latency import LatencyMetric
from sageLLM.benchmarks.metrics.mfu import MFUMetric
from sageLLM.benchmarks.ci.gates import PerformanceGate, PerformanceGateConfig


class TestThroughputMetric:
    """ååé‡æŒ‡æ ‡æµ‹è¯•"""
    
    def test_basic_throughput(self):
        """æµ‹è¯•åŸºæœ¬ååé‡è®¡ç®—"""
        metric = ThroughputMetric()
        metric.start()
        metric.record(tokens=1000, requests=10)
        time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        result = metric.compute()
        
        assert result.total_tokens == 1000
        assert result.total_requests == 10
        assert result.tokens_per_second > 0


class TestLatencyMetric:
    """å»¶è¿ŸæŒ‡æ ‡æµ‹è¯•"""
    
    def test_ttft(self):
        """æµ‹è¯• TTFT è®¡ç®—"""
        metric = LatencyMetric()
        metric.request_start()
        time.sleep(0.05)
        metric.first_token()
        metric.prefill_done()
        
        for _ in range(10):
            metric.token_generated()
            time.sleep(0.001)
        
        result = metric.compute()
        
        assert result.ttft_ms > 40  # è‡³å°‘ 50ms
        assert result.prefill_ms > 0


class TestMFUMetric:
    """MFU æŒ‡æ ‡æµ‹è¯•"""
    
    def test_mfu_calculation(self):
        """æµ‹è¯• MFU è®¡ç®—"""
        metric = MFUMetric()
        
        result = metric.compute(
            model_params=7_000_000_000,  # 7B
            tokens_processed=1024,
            duration_s=1.0,
            peak_tflops=312.0,  # A100
        )
        
        assert result.mfu > 0
        assert result.mfu <= 1.0
        assert result.achieved_tflops > 0


class TestPerformanceGate:
    """æ€§èƒ½é—¨æ§æµ‹è¯•"""
    
    def test_throughput_gate_pass(self):
        """æµ‹è¯•ååé‡é—¨æ§é€šè¿‡"""
        config = PerformanceGateConfig(min_throughput_tps=100.0)
        gate = PerformanceGate(config)
        
        result = gate.check_throughput(actual_tps=150.0)
        
        assert result.passed
    
    def test_throughput_gate_fail(self):
        """æµ‹è¯•ååé‡é—¨æ§å¤±è´¥"""
        config = PerformanceGateConfig(min_throughput_tps=100.0)
        gate = PerformanceGate(config)
        
        result = gate.check_throughput(actual_tps=50.0)
        
        assert not result.passed
    
    def test_regression_check(self):
        """æµ‹è¯•å›å½’æ£€æŸ¥"""
        config = PerformanceGateConfig(max_regression_pct=5.0)
        gate = PerformanceGate(config)
        
        # 3% å›å½’ï¼Œåº”è¯¥é€šè¿‡
        result = gate.check_regression(
            metric_name="throughput",
            baseline=100.0,
            current=97.0,
            higher_is_better=True,
        )
        assert result.passed
        
        # 10% å›å½’ï¼Œåº”è¯¥å¤±è´¥
        result = gate.check_regression(
            metric_name="throughput",
            baseline=100.0,
            current=90.0,
            higher_is_better=True,
        )
        assert not result.passed


class TestMetricSummary:
    """æŒ‡æ ‡æ‘˜è¦æµ‹è¯•"""
    
    def test_from_values(self):
        """æµ‹è¯•ä»å€¼åˆ—è¡¨åˆ›å»ºæ‘˜è¦"""
        values = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
        summary = MetricSummary.from_values("test", values, "ms")
        
        assert summary.mean == 55.0
        assert summary.min == 10.0
        assert summary.max == 100.0
        assert summary.count == 10
```

---

## æ¥å£çº¦å®š

### è¾“å…¥æ¥å£

| æ¥å£ | æ¥æº | è¯´æ˜ |
|------|------|------|
| æµ‹é‡æ•°æ® | runtime/scheduler | æ‰§è¡Œæ—¶é—´ã€token æ•°ç­‰ |
| KV ç»Ÿè®¡ | kv_runtime | å‘½ä¸­ç‡ã€è¿ç§»ç­‰ |
| ç¡¬ä»¶è§„æ ¼ | backends | å³°å€¼æ€§èƒ½ |

### è¾“å‡ºæ¥å£

| æ¥å£ | ç›®æ ‡ | è¯´æ˜ |
|------|------|------|
| `MetricValue` | reporters | å•æ¬¡æµ‹é‡ç»“æœ |
| `MetricSummary` | reporters | èšåˆç»“æœ |
| `GateResult` | CI | é—¨æ§æ£€æŸ¥ç»“æœ |

---

## éªŒæ”¶æ ‡å‡†

- [ ] ååé‡æŒ‡æ ‡ï¼šæ­£ç¡®è®¡ç®— TPS/QPS
- [ ] å»¶è¿ŸæŒ‡æ ‡ï¼šæ­£ç¡®è®¡ç®— TTFT/TPOT/P99
- [ ] MFU æŒ‡æ ‡ï¼šè®¡ç®—è¯¯å·® < 10%
- [ ] CI é—¨æ§ï¼šæ­£ç¡®åˆ¤æ–­ pass/fail
- [ ] æŠ¥å‘Šç”Ÿæˆï¼šæ§åˆ¶å° + JSON æ ¼å¼
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%

---

## è¾“å‡ºç‰©æ¸…å•

```
benchmarks/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ __init__.py           # âœ… åŸºç¡€å®šä¹‰
â”‚   â”œâ”€â”€ throughput.py         # âœ… ååé‡
â”‚   â”œâ”€â”€ latency.py            # âœ… å»¶è¿Ÿ
â”‚   â”œâ”€â”€ memory.py             # ï¼ˆå¯é€‰ï¼‰
â”‚   â”œâ”€â”€ kv_cache.py           # âœ… KV ç¼“å­˜
â”‚   â””â”€â”€ mfu.py                # âœ… MFU/MBU
â”œâ”€â”€ profiler/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ trace.py              # ï¼ˆåç»­æ·»åŠ ï¼‰
â”œâ”€â”€ ci/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ gates.py              # âœ… æ€§èƒ½é—¨æ§
â””â”€â”€ reporters/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ console.py            # âœ… æ§åˆ¶å°
    â””â”€â”€ json_reporter.py      # ï¼ˆå¯é€‰ï¼‰

tests/unit/
â””â”€â”€ test_benchmarks.py        # âœ… æµ‹è¯•æ–‡ä»¶
```
