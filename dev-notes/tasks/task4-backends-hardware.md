# Task 4: backends/ ç¡¬ä»¶åç«¯æŠ½è±¡

**çŠ¶æ€**: ğŸ”² å¾…å¼€å§‹  
**é¢„è®¡æ—¶é—´**: 4h  
**è¯¾é¢˜å¯¹åº”**: 4.1 + å›½äº§èŠ¯ç‰‡æ”¯æŒ  
**å¯å¹¶è¡Œ**: âœ… æ˜¯ï¼ˆä¸ Task 1-3, 5 å¹¶è¡Œï¼‰

---

## èƒŒæ™¯

è¯¾é¢˜ 4.1 è¦æ±‚æ”¯æŒå›½äº§èŠ¯ç‰‡ï¼ˆåä¸ºæ˜‡è…¾ã€å¯’æ­¦çºªã€æµ·å…‰ï¼‰ã€‚æœ¬ä»»åŠ¡åˆ›å»ºç»Ÿä¸€çš„ç¡¬ä»¶åç«¯æŠ½è±¡å±‚ï¼Œä½¿å¾— runtime å±‚ä»£ç ä¸å…·ä½“ç¡¬ä»¶è§£è€¦ã€‚

**è®¾è®¡åŸåˆ™**ï¼š
1. **ç»Ÿä¸€æ¥å£**ï¼šæ‰€æœ‰åç«¯å®ç°ç›¸åŒçš„åè®®
2. **è‡ªåŠ¨å‘ç°**ï¼šè¿è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹å¯ç”¨ç¡¬ä»¶
3. **ä¼˜é›…é™çº§**ï¼šç¡¬ä»¶ä¸å¯ç”¨æ—¶è‡ªåŠ¨å›é€€
4. **æ‰©å±•æ€§**ï¼šæ˜“äºæ·»åŠ æ–°ç¡¬ä»¶æ”¯æŒ

---

## å·¥ä½œç›®å½•

```
/home/shuhao/SAGE/packages/sage-common/src/sage/common/components/sage_llm/sageLLM/backends/
â”œâ”€â”€ __init__.py              # å¯¼å‡º + è‡ªåŠ¨å‘ç°
â”œâ”€â”€ protocols.py             # ç¡¬ä»¶åç«¯åè®®
â”œâ”€â”€ registry.py              # åç«¯æ³¨å†Œè¡¨
â”œâ”€â”€ cuda/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ backend.py           # CUDA åç«¯
â”œâ”€â”€ ascend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ backend.py           # åä¸ºæ˜‡è…¾åç«¯
â”œâ”€â”€ cambricon/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ backend.py           # å¯’æ­¦çºª MLU åç«¯
â””â”€â”€ hygon/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ backend.py           # æµ·å…‰ DCU åç«¯
```

---

## å‚è€ƒèµ„æ–™

- PyTorch Device API: https://pytorch.org/docs/stable/notes/cuda.html
- Ascend PyTorch (torch_npu): https://gitee.com/ascend/pytorch
- Cambricon PyTorch (torch_mlu): https://github.com/Cambricon/catch
- ROCm PyTorch: https://pytorch.org/docs/stable/notes/hip.html
- vLLM Platform Layer: https://github.com/vllm-project/vllm/tree/main/vllm/platforms

---

## ä»»åŠ¡æ¸…å•

### 1. å®šä¹‰ç¡¬ä»¶åç«¯åè®® (`protocols.py`)

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Dict, List, Optional, Tuple, Any, Union
import torch


class BackendType(Enum):
    """ç¡¬ä»¶åç«¯ç±»å‹"""
    CUDA = auto()        # NVIDIA CUDA
    ASCEND = auto()      # åä¸ºæ˜‡è…¾
    CAMBRICON = auto()   # å¯’æ­¦çºª MLU
    HYGON = auto()       # æµ·å…‰ DCU (ROCm-based)
    CPU = auto()         # CPU fallback


@dataclass
class DeviceInfo:
    """è®¾å¤‡ä¿¡æ¯"""
    backend: BackendType
    device_id: int
    name: str
    
    # è®¡ç®—èƒ½åŠ›
    compute_capability: Optional[str] = None  # e.g., "8.0" for A100
    
    # å†…å­˜
    total_memory_gb: float = 0.0
    free_memory_gb: float = 0.0
    
    # æ ¸å¿ƒæ•°
    num_cores: int = 0
    
    # é©±åŠ¨/SDK ç‰ˆæœ¬
    driver_version: Optional[str] = None
    sdk_version: Optional[str] = None
    
    # å…¶ä»–å±æ€§
    properties: Dict[str, Any] = field(default_factory=dict)


@dataclass
class KernelCapabilities:
    """å†…æ ¸èƒ½åŠ›"""
    # æ”¯æŒçš„ç²¾åº¦
    supports_fp32: bool = True
    supports_fp16: bool = True
    supports_bf16: bool = False
    supports_fp8: bool = False
    supports_int8: bool = False
    supports_int4: bool = False
    
    # ç¨€ç–æ”¯æŒ
    supports_sparse_2_4: bool = False
    
    # ç‰¹æ®Šç®—å­
    supports_flash_attention: bool = False
    supports_paged_attention: bool = False
    supports_fused_moe: bool = False
    
    # é€šä¿¡
    supports_nccl: bool = False
    supports_hccl: bool = False  # æ˜‡è…¾


class HardwareBackend(ABC):
    """ç¡¬ä»¶åç«¯æŠ½è±¡åŸºç±»
    
    å®šä¹‰æ‰€æœ‰ç¡¬ä»¶åç«¯å¿…é¡»å®ç°çš„æ¥å£ã€‚
    """
    
    @property
    @abstractmethod
    def backend_type(self) -> BackendType:
        """è¿”å›åç«¯ç±»å‹"""
        ...
    
    @abstractmethod
    def is_available(self) -> bool:
        """æ£€æŸ¥åç«¯æ˜¯å¦å¯ç”¨"""
        ...
    
    @abstractmethod
    def get_device_count(self) -> int:
        """è·å–å¯ç”¨è®¾å¤‡æ•°é‡"""
        ...
    
    @abstractmethod
    def get_device_info(self, device_id: int = 0) -> DeviceInfo:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        ...
    
    @abstractmethod
    def get_capabilities(self, device_id: int = 0) -> KernelCapabilities:
        """è·å–å†…æ ¸èƒ½åŠ›"""
        ...
    
    @abstractmethod
    def get_device(self, device_id: int = 0) -> torch.device:
        """è·å– PyTorch device å¯¹è±¡"""
        ...
    
    @abstractmethod
    def synchronize(self, device_id: Optional[int] = None) -> None:
        """åŒæ­¥è®¾å¤‡
        
        Args:
            device_id: è®¾å¤‡ IDï¼ŒNone è¡¨ç¤ºå½“å‰è®¾å¤‡
        """
        ...
    
    @abstractmethod
    def memory_stats(self, device_id: int = 0) -> Dict[str, float]:
        """è·å–å†…å­˜ç»Ÿè®¡
        
        Returns:
            åŒ…å« total_gb, used_gb, free_gb çš„å­—å…¸
        """
        ...
    
    @abstractmethod
    def empty_cache(self, device_id: Optional[int] = None) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        ...
    
    # === å¯é€‰æ–¹æ³•ï¼ˆæœ‰é»˜è®¤å®ç°ï¼‰===
    
    def set_device(self, device_id: int) -> None:
        """è®¾ç½®å½“å‰è®¾å¤‡"""
        torch.cuda.set_device(device_id)  # é»˜è®¤å®ç°
    
    def current_device(self) -> int:
        """è·å–å½“å‰è®¾å¤‡ ID"""
        return torch.cuda.current_device()  # é»˜è®¤å®ç°
    
    def allocate_tensor(
        self,
        shape: Tuple[int, ...],
        dtype: torch.dtype,
        device_id: int = 0,
    ) -> torch.Tensor:
        """åˆ†é…å¼ é‡
        
        æŸäº›åç«¯å¯èƒ½éœ€è¦ç‰¹æ®Šçš„å†…å­˜åˆ†é…ç­–ç•¥ã€‚
        """
        device = self.get_device(device_id)
        return torch.empty(shape, dtype=dtype, device=device)
    
    def copy_to_device(
        self,
        tensor: torch.Tensor,
        device_id: int = 0,
        non_blocking: bool = True,
    ) -> torch.Tensor:
        """å¤åˆ¶å¼ é‡åˆ°è®¾å¤‡"""
        device = self.get_device(device_id)
        return tensor.to(device, non_blocking=non_blocking)
    
    def copy_to_host(
        self,
        tensor: torch.Tensor,
        non_blocking: bool = True,
    ) -> torch.Tensor:
        """å¤åˆ¶å¼ é‡åˆ° CPU"""
        return tensor.cpu()


class CommunicationBackend(ABC):
    """é€šä¿¡åç«¯æŠ½è±¡
    
    ç”¨äºå¤šè®¾å¤‡/å¤šèŠ‚ç‚¹é€šä¿¡ã€‚
    """
    
    @abstractmethod
    def init_process_group(
        self,
        backend: str,
        world_size: int,
        rank: int,
        **kwargs,
    ) -> None:
        """åˆå§‹åŒ–è¿›ç¨‹ç»„"""
        ...
    
    @abstractmethod
    def all_reduce(
        self,
        tensor: torch.Tensor,
        op: str = "sum",
    ) -> torch.Tensor:
        """All-reduce æ“ä½œ"""
        ...
    
    @abstractmethod
    def all_gather(
        self,
        tensor: torch.Tensor,
        world_size: int,
    ) -> List[torch.Tensor]:
        """All-gather æ“ä½œ"""
        ...
    
    @abstractmethod
    def broadcast(
        self,
        tensor: torch.Tensor,
        src: int = 0,
    ) -> torch.Tensor:
        """å¹¿æ’­æ“ä½œ"""
        ...
    
    @abstractmethod
    def send(
        self,
        tensor: torch.Tensor,
        dst: int,
    ) -> None:
        """å‘é€å¼ é‡"""
        ...
    
    @abstractmethod
    def recv(
        self,
        shape: Tuple[int, ...],
        dtype: torch.dtype,
        src: int,
    ) -> torch.Tensor:
        """æ¥æ”¶å¼ é‡"""
        ...
```

### 2. å®ç°åç«¯æ³¨å†Œè¡¨ (`registry.py`)

```python
from typing import Dict, List, Optional, Type
import logging

from .protocols import HardwareBackend, BackendType, DeviceInfo

logger = logging.getLogger(__name__)


class BackendRegistry:
    """ç¡¬ä»¶åç«¯æ³¨å†Œè¡¨
    
    æä¾›ï¼š
    1. åç«¯æ³¨å†Œå’Œå‘ç°
    2. è‡ªåŠ¨æ£€æµ‹å¯ç”¨åç«¯
    3. ä¼˜é›…é™çº§ï¼ˆfallbackï¼‰
    """
    
    _backends: Dict[BackendType, Type[HardwareBackend]] = {}
    _instances: Dict[BackendType, HardwareBackend] = {}
    _default_backend: Optional[BackendType] = None
    
    @classmethod
    def register(cls, backend_type: BackendType):
        """è£…é¥°å™¨ï¼šæ³¨å†Œåç«¯
        
        Usage:
            @BackendRegistry.register(BackendType.CUDA)
            class CUDABackend(HardwareBackend):
                ...
        """
        def decorator(backend_cls: Type[HardwareBackend]):
            cls._backends[backend_type] = backend_cls
            logger.debug(f"Registered backend: {backend_type.name}")
            return backend_cls
        return decorator
    
    @classmethod
    def get(cls, backend_type: BackendType) -> Optional[HardwareBackend]:
        """è·å–åç«¯å®ä¾‹
        
        Args:
            backend_type: åç«¯ç±»å‹
            
        Returns:
            åç«¯å®ä¾‹ï¼Œå¦‚æœä¸å¯ç”¨è¿”å› None
        """
        # æ£€æŸ¥ç¼“å­˜
        if backend_type in cls._instances:
            return cls._instances[backend_type]
        
        # åˆ›å»ºå®ä¾‹
        if backend_type not in cls._backends:
            logger.warning(f"Backend {backend_type.name} not registered")
            return None
        
        try:
            instance = cls._backends[backend_type]()
            if instance.is_available():
                cls._instances[backend_type] = instance
                return instance
            else:
                logger.info(f"Backend {backend_type.name} not available")
                return None
        except Exception as e:
            logger.warning(f"Failed to initialize backend {backend_type.name}: {e}")
            return None
    
    @classmethod
    def get_default(cls) -> HardwareBackend:
        """è·å–é»˜è®¤åç«¯
        
        ä¼˜å…ˆçº§ï¼šCUDA > ASCEND > CAMBRICON > HYGON > CPU
        """
        if cls._default_backend:
            backend = cls.get(cls._default_backend)
            if backend:
                return backend
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•
        priority = [
            BackendType.CUDA,
            BackendType.ASCEND,
            BackendType.CAMBRICON,
            BackendType.HYGON,
            BackendType.CPU,
        ]
        
        for bt in priority:
            backend = cls.get(bt)
            if backend:
                cls._default_backend = bt
                return backend
        
        raise RuntimeError("No available hardware backend")
    
    @classmethod
    def set_default(cls, backend_type: BackendType) -> None:
        """è®¾ç½®é»˜è®¤åç«¯"""
        cls._default_backend = backend_type
    
    @classmethod
    def list_available(cls) -> List[BackendType]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨åç«¯"""
        available = []
        for bt in cls._backends:
            try:
                instance = cls._backends[bt]()
                if instance.is_available():
                    available.append(bt)
            except Exception:
                pass
        return available
    
    @classmethod
    def discover(cls) -> Dict[BackendType, DeviceInfo]:
        """å‘ç°æ‰€æœ‰å¯ç”¨è®¾å¤‡
        
        Returns:
            åç«¯ç±»å‹åˆ°è®¾å¤‡ä¿¡æ¯çš„æ˜ å°„
        """
        devices = {}
        for bt in cls.list_available():
            backend = cls.get(bt)
            if backend and backend.get_device_count() > 0:
                devices[bt] = backend.get_device_info(0)
        return devices
    
    @classmethod
    def reset(cls) -> None:
        """é‡ç½®æ³¨å†Œè¡¨ï¼ˆä¸»è¦ç”¨äºæµ‹è¯•ï¼‰"""
        cls._instances.clear()
        cls._default_backend = None
```

### 3. CUDA åç«¯ (`cuda/backend.py`)

```python
import torch
from typing import Dict, Optional, Tuple, Any
import logging

from ..protocols import (
    HardwareBackend, BackendType, DeviceInfo, KernelCapabilities
)
from ..registry import BackendRegistry

logger = logging.getLogger(__name__)


@BackendRegistry.register(BackendType.CUDA)
class CUDABackend(HardwareBackend):
    """NVIDIA CUDA åç«¯"""
    
    @property
    def backend_type(self) -> BackendType:
        return BackendType.CUDA
    
    def is_available(self) -> bool:
        return torch.cuda.is_available()
    
    def get_device_count(self) -> int:
        if not self.is_available():
            return 0
        return torch.cuda.device_count()
    
    def get_device_info(self, device_id: int = 0) -> DeviceInfo:
        if not self.is_available():
            raise RuntimeError("CUDA not available")
        
        props = torch.cuda.get_device_properties(device_id)
        
        # è·å–è®¡ç®—èƒ½åŠ›
        compute_capability = f"{props.major}.{props.minor}"
        
        # è·å–å†…å­˜ä¿¡æ¯
        total_memory = props.total_memory / (1024**3)
        free_memory = torch.cuda.mem_get_info(device_id)[0] / (1024**3)
        
        return DeviceInfo(
            backend=BackendType.CUDA,
            device_id=device_id,
            name=props.name,
            compute_capability=compute_capability,
            total_memory_gb=total_memory,
            free_memory_gb=free_memory,
            num_cores=props.multi_processor_count,
            driver_version=torch.version.cuda,
            properties={
                "max_threads_per_block": props.max_threads_per_block,
                "max_threads_per_multiprocessor": props.max_threads_per_multi_processor,
                "warp_size": props.warp_size,
            },
        )
    
    def get_capabilities(self, device_id: int = 0) -> KernelCapabilities:
        info = self.get_device_info(device_id)
        major, minor = map(int, info.compute_capability.split("."))
        
        # æ ¹æ®è®¡ç®—èƒ½åŠ›ç¡®å®šæ”¯æŒçš„ç‰¹æ€§
        supports_bf16 = major >= 8  # Ampere+
        supports_fp8 = major >= 9   # Hopper+
        supports_sparse_2_4 = major >= 8  # Ampere+
        supports_flash_attention = major >= 8
        
        return KernelCapabilities(
            supports_fp32=True,
            supports_fp16=True,
            supports_bf16=supports_bf16,
            supports_fp8=supports_fp8,
            supports_int8=True,
            supports_int4=True,
            supports_sparse_2_4=supports_sparse_2_4,
            supports_flash_attention=supports_flash_attention,
            supports_paged_attention=True,
            supports_fused_moe=True,
            supports_nccl=True,
            supports_hccl=False,
        )
    
    def get_device(self, device_id: int = 0) -> torch.device:
        return torch.device(f"cuda:{device_id}")
    
    def synchronize(self, device_id: Optional[int] = None) -> None:
        if device_id is not None:
            torch.cuda.synchronize(device_id)
        else:
            torch.cuda.synchronize()
    
    def memory_stats(self, device_id: int = 0) -> Dict[str, float]:
        free, total = torch.cuda.mem_get_info(device_id)
        return {
            "total_gb": total / (1024**3),
            "used_gb": (total - free) / (1024**3),
            "free_gb": free / (1024**3),
        }
    
    def empty_cache(self, device_id: Optional[int] = None) -> None:
        torch.cuda.empty_cache()
```

### 4. æ˜‡è…¾åç«¯ (`ascend/backend.py`)

```python
import torch
from typing import Dict, Optional, Tuple, Any
import logging

from ..protocols import (
    HardwareBackend, BackendType, DeviceInfo, KernelCapabilities
)
from ..registry import BackendRegistry

logger = logging.getLogger(__name__)


@BackendRegistry.register(BackendType.ASCEND)
class AscendBackend(HardwareBackend):
    """åä¸ºæ˜‡è…¾åç«¯
    
    ä¾èµ– torch_npu åŒ…ã€‚
    """
    
    def __init__(self):
        self._npu = None
        self._available = False
        self._init_npu()
    
    def _init_npu(self):
        """åˆå§‹åŒ– torch_npu"""
        try:
            import torch_npu
            self._npu = torch_npu
            self._available = torch_npu.npu.is_available()
            if self._available:
                logger.info("Ascend NPU available")
        except ImportError:
            logger.debug("torch_npu not installed")
            self._available = False
    
    @property
    def backend_type(self) -> BackendType:
        return BackendType.ASCEND
    
    def is_available(self) -> bool:
        return self._available
    
    def get_device_count(self) -> int:
        if not self.is_available():
            return 0
        return self._npu.npu.device_count()
    
    def get_device_info(self, device_id: int = 0) -> DeviceInfo:
        if not self.is_available():
            raise RuntimeError("Ascend NPU not available")
        
        # è·å–è®¾å¤‡å±æ€§ï¼ˆAPI å¯èƒ½ä¸ CUDA ä¸åŒï¼‰
        try:
            props = self._npu.npu.get_device_properties(device_id)
            name = props.name if hasattr(props, "name") else f"Ascend NPU {device_id}"
            total_memory = props.total_memory / (1024**3) if hasattr(props, "total_memory") else 0
        except Exception:
            name = f"Ascend NPU {device_id}"
            total_memory = 64.0  # é»˜è®¤å‡è®¾ 64GB
        
        # è·å–å¯ç”¨å†…å­˜
        try:
            free, total = self._npu.npu.mem_get_info(device_id)
            free_memory = free / (1024**3)
            total_memory = total / (1024**3)
        except Exception:
            free_memory = 0.0
        
        return DeviceInfo(
            backend=BackendType.ASCEND,
            device_id=device_id,
            name=name,
            compute_capability=None,  # Ascend æ²¡æœ‰ç±»ä¼¼æ¦‚å¿µ
            total_memory_gb=total_memory,
            free_memory_gb=free_memory,
            num_cores=0,  # éœ€è¦æŸ¥è¯¢å®é™…å€¼
            sdk_version=self._get_cann_version(),
        )
    
    def _get_cann_version(self) -> Optional[str]:
        """è·å– CANN ç‰ˆæœ¬"""
        try:
            return self._npu.version.cann
        except Exception:
            return None
    
    def get_capabilities(self, device_id: int = 0) -> KernelCapabilities:
        # æ˜‡è…¾çš„èƒ½åŠ›å› å‹å·è€Œå¼‚ï¼Œè¿™é‡Œç»™å‡º 910B çš„å…¸å‹èƒ½åŠ›
        return KernelCapabilities(
            supports_fp32=True,
            supports_fp16=True,
            supports_bf16=True,  # 910B æ”¯æŒ BF16
            supports_fp8=False,  # æš‚ä¸æ”¯æŒ
            supports_int8=True,
            supports_int4=False,  # éœ€è¦éªŒè¯
            supports_sparse_2_4=False,
            supports_flash_attention=True,  # é€šè¿‡ CANN æ”¯æŒ
            supports_paged_attention=True,  # vLLM-Ascend æ”¯æŒ
            supports_fused_moe=False,  # éœ€è¦éªŒè¯
            supports_nccl=False,
            supports_hccl=True,
        )
    
    def get_device(self, device_id: int = 0) -> torch.device:
        return torch.device(f"npu:{device_id}")
    
    def set_device(self, device_id: int) -> None:
        self._npu.npu.set_device(device_id)
    
    def current_device(self) -> int:
        return self._npu.npu.current_device()
    
    def synchronize(self, device_id: Optional[int] = None) -> None:
        if device_id is not None:
            self._npu.npu.synchronize(device_id)
        else:
            self._npu.npu.synchronize()
    
    def memory_stats(self, device_id: int = 0) -> Dict[str, float]:
        try:
            free, total = self._npu.npu.mem_get_info(device_id)
            return {
                "total_gb": total / (1024**3),
                "used_gb": (total - free) / (1024**3),
                "free_gb": free / (1024**3),
            }
        except Exception:
            return {"total_gb": 0, "used_gb": 0, "free_gb": 0}
    
    def empty_cache(self, device_id: Optional[int] = None) -> None:
        self._npu.npu.empty_cache()
```

### 5. å¯’æ­¦çºªåç«¯ (`cambricon/backend.py`)

```python
import torch
from typing import Dict, Optional, Tuple, Any
import logging

from ..protocols import (
    HardwareBackend, BackendType, DeviceInfo, KernelCapabilities
)
from ..registry import BackendRegistry

logger = logging.getLogger(__name__)


@BackendRegistry.register(BackendType.CAMBRICON)
class CambriconBackend(HardwareBackend):
    """å¯’æ­¦çºª MLU åç«¯
    
    ä¾èµ– torch_mlu (catch) åŒ…ã€‚
    """
    
    def __init__(self):
        self._mlu = None
        self._available = False
        self._init_mlu()
    
    def _init_mlu(self):
        """åˆå§‹åŒ– torch_mlu"""
        try:
            import torch_mlu
            self._mlu = torch_mlu
            self._available = torch_mlu.mlu.is_available()
            if self._available:
                logger.info("Cambricon MLU available")
        except ImportError:
            logger.debug("torch_mlu not installed")
            self._available = False
    
    @property
    def backend_type(self) -> BackendType:
        return BackendType.CAMBRICON
    
    def is_available(self) -> bool:
        return self._available
    
    def get_device_count(self) -> int:
        if not self.is_available():
            return 0
        return self._mlu.mlu.device_count()
    
    def get_device_info(self, device_id: int = 0) -> DeviceInfo:
        if not self.is_available():
            raise RuntimeError("Cambricon MLU not available")
        
        try:
            name = self._mlu.mlu.get_device_name(device_id)
        except Exception:
            name = f"MLU {device_id}"
        
        try:
            props = self._mlu.mlu.get_device_properties(device_id)
            total_memory = props.total_memory / (1024**3)
        except Exception:
            total_memory = 32.0  # é»˜è®¤å‡è®¾ 32GB
        
        return DeviceInfo(
            backend=BackendType.CAMBRICON,
            device_id=device_id,
            name=name,
            total_memory_gb=total_memory,
            free_memory_gb=0,  # éœ€è¦æŸ¥è¯¢
        )
    
    def get_capabilities(self, device_id: int = 0) -> KernelCapabilities:
        # MLU590 çš„å…¸å‹èƒ½åŠ›
        return KernelCapabilities(
            supports_fp32=True,
            supports_fp16=True,
            supports_bf16=False,  # éœ€è¦éªŒè¯
            supports_fp8=False,
            supports_int8=True,
            supports_int4=False,
            supports_sparse_2_4=False,
            supports_flash_attention=False,  # éœ€è¦éªŒè¯
            supports_paged_attention=False,
            supports_fused_moe=False,
            supports_nccl=False,
            supports_hccl=False,
        )
    
    def get_device(self, device_id: int = 0) -> torch.device:
        return torch.device(f"mlu:{device_id}")
    
    def set_device(self, device_id: int) -> None:
        self._mlu.mlu.set_device(device_id)
    
    def current_device(self) -> int:
        return self._mlu.mlu.current_device()
    
    def synchronize(self, device_id: Optional[int] = None) -> None:
        if device_id is not None:
            self._mlu.mlu.synchronize(device_id)
        else:
            self._mlu.mlu.synchronize()
    
    def memory_stats(self, device_id: int = 0) -> Dict[str, float]:
        try:
            # API å¯èƒ½ä¸åŒ
            allocated = self._mlu.mlu.memory_allocated(device_id)
            reserved = self._mlu.mlu.memory_reserved(device_id)
            total = self.get_device_info(device_id).total_memory_gb * (1024**3)
            return {
                "total_gb": total / (1024**3),
                "used_gb": allocated / (1024**3),
                "free_gb": (total - allocated) / (1024**3),
            }
        except Exception:
            return {"total_gb": 0, "used_gb": 0, "free_gb": 0}
    
    def empty_cache(self, device_id: Optional[int] = None) -> None:
        self._mlu.mlu.empty_cache()
```

### 6. æµ·å…‰åç«¯ (`hygon/backend.py`)

```python
import torch
from typing import Dict, Optional, Tuple, Any
import logging

from ..protocols import (
    HardwareBackend, BackendType, DeviceInfo, KernelCapabilities
)
from ..registry import BackendRegistry

logger = logging.getLogger(__name__)


@BackendRegistry.register(BackendType.HYGON)
class HygonBackend(HardwareBackend):
    """æµ·å…‰ DCU åç«¯
    
    åŸºäº ROCm/HIPï¼Œä¸ AMD GPU ç±»ä¼¼çš„ APIã€‚
    ä½¿ç”¨ PyTorch çš„ ROCm æ”¯æŒã€‚
    """
    
    def __init__(self):
        self._available = False
        self._init_dcu()
    
    def _init_dcu(self):
        """åˆå§‹åŒ–æµ·å…‰ DCU"""
        # æµ·å…‰ DCU ä½¿ç”¨ ROCmï¼ŒPyTorch é€šè¿‡ torch.cuda è®¿é—®ï¼ˆå¦‚æœæ˜¯ ROCm ç‰ˆæœ¬ï¼‰
        # æˆ–è€…å¯èƒ½æœ‰ä¸“é—¨çš„ torch_dcu
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯ ROCm ç‰ˆæœ¬çš„ PyTorch
            if torch.version.hip is not None:
                # ROCm ç‰ˆæœ¬ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æµ·å…‰è®¾å¤‡
                self._available = torch.cuda.is_available()
                if self._available:
                    # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦æ˜¯æµ·å…‰è®¾å¤‡
                    device_name = torch.cuda.get_device_name(0)
                    if "Hygon" in device_name or "DCU" in device_name:
                        logger.info(f"Hygon DCU available: {device_name}")
                    else:
                        # å¯èƒ½æ˜¯å…¶ä»– ROCm è®¾å¤‡ï¼ˆå¦‚ AMDï¼‰
                        self._available = False
            else:
                # å°è¯•å¯¼å…¥ä¸“é—¨çš„ torch_dcu
                try:
                    import torch_dcu
                    self._available = torch_dcu.dcu.is_available()
                except ImportError:
                    self._available = False
        except Exception:
            self._available = False
    
    @property
    def backend_type(self) -> BackendType:
        return BackendType.HYGON
    
    def is_available(self) -> bool:
        return self._available
    
    def get_device_count(self) -> int:
        if not self.is_available():
            return 0
        return torch.cuda.device_count()  # ROCm ä½¿ç”¨ cuda API
    
    def get_device_info(self, device_id: int = 0) -> DeviceInfo:
        if not self.is_available():
            raise RuntimeError("Hygon DCU not available")
        
        props = torch.cuda.get_device_properties(device_id)
        total_memory = props.total_memory / (1024**3)
        free, _ = torch.cuda.mem_get_info(device_id)
        free_memory = free / (1024**3)
        
        return DeviceInfo(
            backend=BackendType.HYGON,
            device_id=device_id,
            name=props.name,
            total_memory_gb=total_memory,
            free_memory_gb=free_memory,
            num_cores=props.multi_processor_count,
            sdk_version=torch.version.hip,
        )
    
    def get_capabilities(self, device_id: int = 0) -> KernelCapabilities:
        # æµ·å…‰ DCU çš„èƒ½åŠ›ï¼ˆåŸºäº ROCmï¼‰
        return KernelCapabilities(
            supports_fp32=True,
            supports_fp16=True,
            supports_bf16=True,  # è¾ƒæ–°ç‰ˆæœ¬æ”¯æŒ
            supports_fp8=False,
            supports_int8=True,
            supports_int4=False,
            supports_sparse_2_4=False,
            supports_flash_attention=True,  # ROCm æœ‰ Flash Attention
            supports_paged_attention=True,  # vLLM æ”¯æŒ ROCm
            supports_fused_moe=False,
            supports_nccl=True,  # ROCm NCCL (RCCL)
            supports_hccl=False,
        )
    
    def get_device(self, device_id: int = 0) -> torch.device:
        # ROCm ä½¿ç”¨ cuda è®¾å¤‡ç±»å‹
        return torch.device(f"cuda:{device_id}")
    
    def synchronize(self, device_id: Optional[int] = None) -> None:
        if device_id is not None:
            torch.cuda.synchronize(device_id)
        else:
            torch.cuda.synchronize()
    
    def memory_stats(self, device_id: int = 0) -> Dict[str, float]:
        free, total = torch.cuda.mem_get_info(device_id)
        return {
            "total_gb": total / (1024**3),
            "used_gb": (total - free) / (1024**3),
            "free_gb": free / (1024**3),
        }
    
    def empty_cache(self, device_id: Optional[int] = None) -> None:
        torch.cuda.empty_cache()
```

### 7. ä¸»æ¨¡å— (`__init__.py`)

```python
"""sageLLM ç¡¬ä»¶åç«¯æŠ½è±¡å±‚

æä¾›ç»Ÿä¸€çš„ç¡¬ä»¶è®¿é—®æ¥å£ï¼Œæ”¯æŒï¼š
- NVIDIA CUDA
- åä¸ºæ˜‡è…¾ (Ascend)
- å¯’æ­¦çºª (Cambricon MLU)
- æµ·å…‰ (Hygon DCU)

Usage:
    from sageLLM.backends import get_backend, BackendType
    
    # è‡ªåŠ¨æ£€æµ‹æœ€ä½³åç«¯
    backend = get_backend()
    
    # æŒ‡å®šåç«¯
    backend = get_backend(BackendType.ASCEND)
    
    # è·å–è®¾å¤‡ä¿¡æ¯
    info = backend.get_device_info()
    print(f"Using {info.name} with {info.total_memory_gb:.1f} GB memory")
"""

from .protocols import (
    HardwareBackend,
    BackendType,
    DeviceInfo,
    KernelCapabilities,
    CommunicationBackend,
)
from .registry import BackendRegistry

# å¯¼å…¥æ‰€æœ‰åç«¯ä»¥è§¦å‘æ³¨å†Œ
from .cuda import backend as _cuda_backend
from .ascend import backend as _ascend_backend
from .cambricon import backend as _cambricon_backend
from .hygon import backend as _hygon_backend


def get_backend(backend_type: BackendType = None) -> HardwareBackend:
    """è·å–ç¡¬ä»¶åç«¯
    
    Args:
        backend_type: åç«¯ç±»å‹ï¼ŒNone è¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹
        
    Returns:
        ç¡¬ä»¶åç«¯å®ä¾‹
    """
    if backend_type is None:
        return BackendRegistry.get_default()
    
    backend = BackendRegistry.get(backend_type)
    if backend is None:
        raise RuntimeError(f"Backend {backend_type.name} not available")
    return backend


def list_available_backends() -> list[BackendType]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨åç«¯"""
    return BackendRegistry.list_available()


def discover_devices() -> dict[BackendType, DeviceInfo]:
    """å‘ç°æ‰€æœ‰å¯ç”¨è®¾å¤‡"""
    return BackendRegistry.discover()


__all__ = [
    # åè®®
    "HardwareBackend",
    "BackendType",
    "DeviceInfo",
    "KernelCapabilities",
    "CommunicationBackend",
    # æ³¨å†Œè¡¨
    "BackendRegistry",
    # ä¾¿æ·å‡½æ•°
    "get_backend",
    "list_available_backends",
    "discover_devices",
]
```

---

## å•å…ƒæµ‹è¯•è¦æ±‚

åˆ›å»º `tests/unit/test_backends.py`ï¼š

```python
import pytest
import torch
from sageLLM.backends import (
    get_backend, list_available_backends, discover_devices,
    BackendType, BackendRegistry
)


class TestBackendRegistry:
    """åç«¯æ³¨å†Œè¡¨æµ‹è¯•"""
    
    def test_list_available(self):
        """æµ‹è¯•åˆ—å‡ºå¯ç”¨åç«¯"""
        available = list_available_backends()
        assert isinstance(available, list)
        # è‡³å°‘åº”è¯¥æœ‰ CPU æˆ– CUDA
        assert len(available) >= 0
    
    def test_get_default(self):
        """æµ‹è¯•è·å–é»˜è®¤åç«¯"""
        backend = get_backend()
        assert backend is not None
        assert backend.is_available()
    
    def test_discover_devices(self):
        """æµ‹è¯•è®¾å¤‡å‘ç°"""
        devices = discover_devices()
        assert isinstance(devices, dict)


@pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA not available")
class TestCUDABackend:
    """CUDA åç«¯æµ‹è¯•"""
    
    def test_is_available(self):
        """æµ‹è¯•å¯ç”¨æ€§æ£€æŸ¥"""
        backend = get_backend(BackendType.CUDA)
        assert backend.is_available()
    
    def test_device_info(self):
        """æµ‹è¯•è®¾å¤‡ä¿¡æ¯"""
        backend = get_backend(BackendType.CUDA)
        info = backend.get_device_info()
        
        assert info.backend == BackendType.CUDA
        assert info.total_memory_gb > 0
        assert info.name != ""
    
    def test_capabilities(self):
        """æµ‹è¯•èƒ½åŠ›æŸ¥è¯¢"""
        backend = get_backend(BackendType.CUDA)
        caps = backend.get_capabilities()
        
        assert caps.supports_fp16
        assert caps.supports_int8
    
    def test_memory_stats(self):
        """æµ‹è¯•å†…å­˜ç»Ÿè®¡"""
        backend = get_backend(BackendType.CUDA)
        stats = backend.memory_stats()
        
        assert "total_gb" in stats
        assert "used_gb" in stats
        assert "free_gb" in stats
        assert stats["total_gb"] > 0
    
    def test_allocate_tensor(self):
        """æµ‹è¯•å¼ é‡åˆ†é…"""
        backend = get_backend(BackendType.CUDA)
        tensor = backend.allocate_tensor(
            shape=(256, 256),
            dtype=torch.float16,
        )
        
        assert tensor.device.type == "cuda"
        assert tensor.dtype == torch.float16


class TestBackendFallback:
    """åç«¯é™çº§æµ‹è¯•"""
    
    def test_unavailable_backend_returns_none(self):
        """æµ‹è¯•ä¸å¯ç”¨åç«¯è¿”å› None"""
        # è¿™ä¸ªæµ‹è¯•å‡è®¾æŸäº›åç«¯ä¸å¯ç”¨
        # å…·ä½“è¡Œä¸ºå–å†³äºæµ‹è¯•ç¯å¢ƒ
        pass
    
    def test_default_fallback(self):
        """æµ‹è¯•é»˜è®¤åç«¯é™çº§"""
        # å³ä½¿æ²¡æœ‰ GPUï¼Œä¹Ÿåº”è¯¥èƒ½è·å–åˆ°æŸä¸ªåç«¯
        backend = get_backend()
        assert backend is not None
```

---

## æ¥å£çº¦å®š

### è¾“å…¥æ¥å£

| æ¥å£ | æ¥æº | è¯´æ˜ |
|------|------|------|
| ç¯å¢ƒå˜é‡ | OS | CUDA_VISIBLE_DEVICES, ASCEND_DEVICE_ID ç­‰ |
| PyTorch | torch | torch.cuda, torch_npu, torch_mlu |

### è¾“å‡ºæ¥å£

| æ¥å£ | ç›®æ ‡ | è¯´æ˜ |
|------|------|------|
| `HardwareBackend` | runtime | è®¾å¤‡æ“ä½œæ¥å£ |
| `DeviceInfo` | scheduler | è®¾å¤‡ä¿¡æ¯ |
| `KernelCapabilities` | accel | æ”¯æŒçš„ä¼˜åŒ–ç‰¹æ€§ |

---

## éªŒæ”¶æ ‡å‡†

- [ ] CUDA åç«¯ï¼šå®Œæ•´å®ç°ï¼Œé€šè¿‡æ‰€æœ‰æµ‹è¯•
- [ ] æ˜‡è…¾åç«¯ï¼šæ¡†æ¶å®ç°ï¼Œæœ‰ `torch_npu` æ—¶å¯ç”¨
- [ ] å¯’æ­¦çºªåç«¯ï¼šæ¡†æ¶å®ç°ï¼Œæœ‰ `torch_mlu` æ—¶å¯ç”¨
- [ ] æµ·å…‰åç«¯ï¼šæ¡†æ¶å®ç°ï¼ŒROCm ç¯å¢ƒå¯ç”¨
- [ ] è‡ªåŠ¨å‘ç°ï¼šæ­£ç¡®æ£€æµ‹æ‰€æœ‰å¯ç”¨åç«¯
- [ ] ä¼˜é›…é™çº§ï¼šç¡¬ä»¶ä¸å¯ç”¨æ—¶ä¸æŠ›å¼‚å¸¸
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%

---

## è¾“å‡ºç‰©æ¸…å•

```
backends/
â”œâ”€â”€ __init__.py           # âœ… å¯¼å‡º + è‡ªåŠ¨å‘ç°
â”œâ”€â”€ protocols.py          # âœ… åè®®å®šä¹‰
â”œâ”€â”€ registry.py           # âœ… æ³¨å†Œè¡¨
â”œâ”€â”€ cuda/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ backend.py        # âœ… CUDA åç«¯
â”œâ”€â”€ ascend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ backend.py        # âœ… æ˜‡è…¾åç«¯
â”œâ”€â”€ cambricon/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ backend.py        # âœ… å¯’æ­¦çºªåç«¯
â””â”€â”€ hygon/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ backend.py        # âœ… æµ·å…‰åç«¯

tests/unit/
â””â”€â”€ test_backends.py      # âœ… æµ‹è¯•æ–‡ä»¶
```
