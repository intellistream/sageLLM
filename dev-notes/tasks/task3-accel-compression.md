# Task 3: accel/ æ¨¡å‹å‹ç¼©ä¸åŠ é€Ÿ

**çŠ¶æ€**: ğŸ”² å¾…å¼€å§‹  
**é¢„è®¡æ—¶é—´**: 4h  
**è¯¾é¢˜å¯¹åº”**: 4.3 æ¨¡å‹å‹ç¼©ä¸æ¨ç†åŠ é€Ÿæ–¹æ³•  
**å¯å¹¶è¡Œ**: âœ… æ˜¯ï¼ˆä¸ Task 1-2, 4-5 å¹¶è¡Œï¼‰

---

## èƒŒæ™¯

è¯¾é¢˜ 4.3 è¦æ±‚ï¼š
- "FP8/INT4/INT8 é‡åŒ–"
- "ç»“æ„åŒ–ç¨€ç–ä¸å‰ªæ"
- "æ··åˆç²¾åº¦æ¨ç†"
- "æˆæœ¬æ¨¡å‹æŒ‡å¯¼çš„åŠ é€Ÿç­–ç•¥é€‰æ‹©"

æœ¬ä»»åŠ¡åˆ›å»º `accel/` æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿèƒ½åŠ›ã€‚

---

## å·¥ä½œç›®å½•

```
/home/shuhao/SAGE/packages/sage-common/src/sage/common/components/sage_llm/sageLLM/accel/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ quantize/                # é‡åŒ–
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fp8.py              # FP8 é‡åŒ–
â”‚   â”œâ”€â”€ int4.py             # INT4 é‡åŒ–
â”‚   â””â”€â”€ mixed_precision.py  # æ··åˆç²¾åº¦
â”œâ”€â”€ sparsity/               # ç¨€ç–
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ structured.py       # ç»“æ„åŒ–ç¨€ç–
â””â”€â”€ cost_model/             # æˆæœ¬æ¨¡å‹
    â”œâ”€â”€ __init__.py
    â””â”€â”€ estimator.py        # æˆæœ¬ä¼°ç®—
```

---

## å‚è€ƒèµ„æ–™

- vLLM Quantization: https://docs.vllm.ai/en/latest/quantization/supported_hardware.html
- GPTQ: https://arxiv.org/abs/2210.17323
- AWQ: https://arxiv.org/abs/2306.00978
- SparseGPT: https://arxiv.org/abs/2301.00774
- FP8 Training: https://arxiv.org/abs/2209.05433
- llm.c: https://github.com/karpathy/llm.c (æ··åˆç²¾åº¦å‚è€ƒ)

---

## ä»»åŠ¡æ¸…å•

### 1. é‡åŒ–åè®®å®šä¹‰ (`quantize/__init__.py`)

```python
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum, auto
from typing import Dict, List, Optional, Tuple, Any
import torch


class QuantizationType(Enum):
    """é‡åŒ–ç±»å‹"""
    NONE = auto()       # æ— é‡åŒ– (FP16/BF16)
    FP8_E4M3 = auto()   # FP8 (E4M3 æ ¼å¼)
    FP8_E5M2 = auto()   # FP8 (E5M2 æ ¼å¼)
    INT8 = auto()       # INT8 å¯¹ç§°é‡åŒ–
    INT4 = auto()       # INT4 åˆ†ç»„é‡åŒ–
    NF4 = auto()        # NormalFloat 4-bit (QLoRA)


class QuantizationGranularity(Enum):
    """é‡åŒ–ç²’åº¦"""
    PER_TENSOR = auto()     # æ•´ä¸ªå¼ é‡å…±äº«ä¸€ä¸ª scale
    PER_CHANNEL = auto()    # æ¯ä¸ªé€šé“ä¸€ä¸ª scale
    PER_GROUP = auto()      # åˆ†ç»„é‡åŒ–ï¼ˆå¦‚æ¯ 128 ä¸ªå…ƒç´ ï¼‰
    PER_TOKEN = auto()      # æ¯ä¸ª token ä¸€ä¸ª scaleï¼ˆç”¨äºæ¿€æ´»ï¼‰


@dataclass
class QuantizationConfig:
    """é‡åŒ–é…ç½®"""
    quant_type: QuantizationType
    granularity: QuantizationGranularity = QuantizationGranularity.PER_TENSOR
    
    # åˆ†ç»„é‡åŒ–å‚æ•°
    group_size: int = 128
    
    # æ ¡å‡†å‚æ•°
    calibration_samples: int = 128
    
    # æ··åˆç²¾åº¦å‚æ•°
    sensitive_layers: List[str] = None  # ä¿æŒé«˜ç²¾åº¦çš„å±‚
    
    # ç®—æ³•å‚æ•°
    use_symmetric: bool = True          # å¯¹ç§°é‡åŒ–
    clip_ratio: float = 1.0             # è£å‰ªæ¯”ä¾‹
    
    def __post_init__(self):
        if self.sensitive_layers is None:
            self.sensitive_layers = []


@dataclass
class QuantizationOutput:
    """é‡åŒ–è¾“å‡º"""
    quantized_weight: torch.Tensor
    scales: torch.Tensor
    zeros: Optional[torch.Tensor] = None  # éå¯¹ç§°é‡åŒ–çš„é›¶ç‚¹
    group_size: int = 128
    quant_type: QuantizationType = QuantizationType.INT8


class Quantizer(ABC):
    """é‡åŒ–å™¨åŸºç±»"""
    
    @property
    @abstractmethod
    def quant_type(self) -> QuantizationType:
        """è¿”å›é‡åŒ–ç±»å‹"""
        ...
    
    @abstractmethod
    def quantize(
        self,
        weight: torch.Tensor,
        config: QuantizationConfig,
    ) -> QuantizationOutput:
        """é‡åŒ–æƒé‡
        
        Args:
            weight: åŸå§‹æƒé‡ [out_features, in_features]
            config: é‡åŒ–é…ç½®
            
        Returns:
            é‡åŒ–è¾“å‡º
        """
        ...
    
    @abstractmethod
    def dequantize(
        self,
        output: QuantizationOutput,
    ) -> torch.Tensor:
        """åé‡åŒ–æƒé‡
        
        Args:
            output: é‡åŒ–è¾“å‡º
            
        Returns:
            åé‡åŒ–åçš„æƒé‡
        """
        ...


class QuantizerRegistry:
    """é‡åŒ–å™¨æ³¨å†Œè¡¨"""
    
    _quantizers: Dict[QuantizationType, type] = {}
    
    @classmethod
    def register(cls, quant_type: QuantizationType):
        """è£…é¥°å™¨ï¼šæ³¨å†Œé‡åŒ–å™¨"""
        def decorator(quantizer_cls):
            cls._quantizers[quant_type] = quantizer_cls
            return quantizer_cls
        return decorator
    
    @classmethod
    def get(cls, quant_type: QuantizationType) -> Quantizer:
        """è·å–é‡åŒ–å™¨å®ä¾‹"""
        if quant_type not in cls._quantizers:
            raise ValueError(f"Unknown quantization type: {quant_type}")
        return cls._quantizers[quant_type]()
    
    @classmethod
    def list_available(cls) -> List[QuantizationType]:
        """åˆ—å‡ºå¯ç”¨çš„é‡åŒ–ç±»å‹"""
        return list(cls._quantizers.keys())
```

### 2. FP8 é‡åŒ– (`quantize/fp8.py`)

```python
import torch
from dataclasses import dataclass
from typing import Optional, Tuple

from . import (
    Quantizer, QuantizerRegistry, QuantizationType, 
    QuantizationConfig, QuantizationOutput, QuantizationGranularity
)


@dataclass
class FP8Format:
    """FP8 æ ¼å¼å®šä¹‰"""
    name: str
    exponent_bits: int
    mantissa_bits: int
    exponent_bias: int
    max_value: float
    min_value: float  # æœ€å°æ­£å€¼


# E4M3: 4 ä½æŒ‡æ•°, 3 ä½å°¾æ•°
FP8_E4M3 = FP8Format(
    name="E4M3",
    exponent_bits=4,
    mantissa_bits=3,
    exponent_bias=7,
    max_value=448.0,      # 2^8 * (1 + 7/8)
    min_value=2**-9,      # æœ€å°éé›¶æ­£å€¼
)

# E5M2: 5 ä½æŒ‡æ•°, 2 ä½å°¾æ•°
FP8_E5M2 = FP8Format(
    name="E5M2",
    exponent_bits=5,
    mantissa_bits=2,
    exponent_bias=15,
    max_value=57344.0,    # 2^15 * (1 + 3/4)
    min_value=2**-16,     # æœ€å°éé›¶æ­£å€¼
)


@QuantizerRegistry.register(QuantizationType.FP8_E4M3)
class FP8E4M3Quantizer(Quantizer):
    """FP8 E4M3 é‡åŒ–å™¨
    
    E4M3 æ›´é€‚åˆæƒé‡é‡åŒ–ï¼š
    - æ›´å¤§çš„åŠ¨æ€èŒƒå›´
    - æ›´é«˜çš„ç²¾åº¦ï¼ˆå¯¹äº [-1, 1] èŒƒå›´å†…çš„å€¼ï¼‰
    """
    
    def __init__(self):
        self.format = FP8_E4M3
    
    @property
    def quant_type(self) -> QuantizationType:
        return QuantizationType.FP8_E4M3
    
    def quantize(
        self,
        weight: torch.Tensor,
        config: QuantizationConfig,
    ) -> QuantizationOutput:
        """FP8 E4M3 é‡åŒ–
        
        å®ç°æ­¥éª¤ï¼š
        1. è®¡ç®— scale ä½¿å¾— weight/scale åœ¨ FP8 èŒƒå›´å†…
        2. å°† weight è½¬æ¢ä¸º FP8 è¡¨ç¤º
        """
        # è®¡ç®—ç¼©æ”¾å› å­
        if config.granularity == QuantizationGranularity.PER_TENSOR:
            scales = self._compute_scale_per_tensor(weight)
        elif config.granularity == QuantizationGranularity.PER_CHANNEL:
            scales = self._compute_scale_per_channel(weight)
        elif config.granularity == QuantizationGranularity.PER_GROUP:
            scales = self._compute_scale_per_group(weight, config.group_size)
        else:
            raise ValueError(f"Unsupported granularity: {config.granularity}")
        
        # ç¼©æ”¾
        scaled_weight = weight / scales.view(-1, 1)
        
        # è£å‰ªåˆ° FP8 èŒƒå›´
        clipped = torch.clamp(
            scaled_weight, 
            -self.format.max_value * config.clip_ratio,
            self.format.max_value * config.clip_ratio,
        )
        
        # æ¨¡æ‹Ÿ FP8 é‡åŒ–ï¼ˆå®é™…ç¡¬ä»¶æ”¯æŒæ—¶ç›´æ¥è½¬æ¢ï¼‰
        # è¿™é‡Œä½¿ç”¨ round-to-nearest æ¨¡æ‹Ÿ
        quantized = self._simulate_fp8_rounding(clipped)
        
        return QuantizationOutput(
            quantized_weight=quantized.to(torch.float16),  # å­˜å‚¨ä¸º FP16ï¼ˆç¡¬ä»¶ä¸æ”¯æŒ FP8 æ—¶ï¼‰
            scales=scales,
            quant_type=self.quant_type,
        )
    
    def _compute_scale_per_tensor(self, weight: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—å¼ é‡çº§ scale"""
        abs_max = weight.abs().max()
        scale = abs_max / self.format.max_value
        return scale.clamp(min=1e-8)
    
    def _compute_scale_per_channel(self, weight: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—é€šé“çº§ scale"""
        abs_max = weight.abs().amax(dim=1)
        scale = abs_max / self.format.max_value
        return scale.clamp(min=1e-8)
    
    def _compute_scale_per_group(
        self, 
        weight: torch.Tensor, 
        group_size: int,
    ) -> torch.Tensor:
        """è®¡ç®—åˆ†ç»„ scale"""
        out_features, in_features = weight.shape
        num_groups = (in_features + group_size - 1) // group_size
        
        # Pad if needed
        if in_features % group_size != 0:
            pad_size = group_size - (in_features % group_size)
            weight = torch.nn.functional.pad(weight, (0, pad_size))
        
        # Reshape to [out_features, num_groups, group_size]
        grouped = weight.view(out_features, num_groups, group_size)
        
        # Compute scale per group
        abs_max = grouped.abs().amax(dim=-1)
        scale = abs_max / self.format.max_value
        return scale.clamp(min=1e-8)
    
    def _simulate_fp8_rounding(self, x: torch.Tensor) -> torch.Tensor:
        """æ¨¡æ‹Ÿ FP8 èˆå…¥
        
        åœ¨æ²¡æœ‰åŸç”Ÿ FP8 æ”¯æŒçš„ç¡¬ä»¶ä¸Šï¼Œä½¿ç”¨ FP16 æ¨¡æ‹Ÿ FP8 ç²¾åº¦ã€‚
        """
        # å¯¹äº E4M3ï¼Œå°¾æ•°æœ‰ 3 ä½ï¼Œç²¾åº¦çº¦ä¸º 1/8
        precision = 2 ** (-self.format.mantissa_bits)
        
        # Round to nearest
        rounded = torch.round(x / precision) * precision
        return rounded
    
    def dequantize(self, output: QuantizationOutput) -> torch.Tensor:
        """åé‡åŒ–"""
        return output.quantized_weight * output.scales.view(-1, 1)


@QuantizerRegistry.register(QuantizationType.FP8_E5M2)
class FP8E5M2Quantizer(Quantizer):
    """FP8 E5M2 é‡åŒ–å™¨
    
    E5M2 æ›´é€‚åˆæ¿€æ´»é‡åŒ–ï¼š
    - æ›´å¤§çš„åŠ¨æ€èŒƒå›´
    - ä¸ FP16 æ›´å…¼å®¹ï¼ˆç›¸åŒçš„æŒ‡æ•°ä½æ•°ï¼‰
    """
    
    def __init__(self):
        self.format = FP8_E5M2
    
    @property
    def quant_type(self) -> QuantizationType:
        return QuantizationType.FP8_E5M2
    
    def quantize(
        self,
        weight: torch.Tensor,
        config: QuantizationConfig,
    ) -> QuantizationOutput:
        # ä¸ E4M3 ç±»ä¼¼ï¼Œä½†ä½¿ç”¨ E5M2 æ ¼å¼
        scales = self._compute_scale_per_tensor(weight)
        scaled_weight = weight / scales
        clipped = torch.clamp(scaled_weight, -self.format.max_value, self.format.max_value)
        quantized = self._simulate_fp8_rounding(clipped)
        
        return QuantizationOutput(
            quantized_weight=quantized.to(torch.float16),
            scales=scales,
            quant_type=self.quant_type,
        )
    
    def _compute_scale_per_tensor(self, weight: torch.Tensor) -> torch.Tensor:
        abs_max = weight.abs().max()
        scale = abs_max / self.format.max_value
        return scale.clamp(min=1e-8)
    
    def _simulate_fp8_rounding(self, x: torch.Tensor) -> torch.Tensor:
        precision = 2 ** (-self.format.mantissa_bits)
        rounded = torch.round(x / precision) * precision
        return rounded
    
    def dequantize(self, output: QuantizationOutput) -> torch.Tensor:
        return output.quantized_weight * output.scales
```

### 3. INT4 é‡åŒ– (`quantize/int4.py`)

```python
import torch
from typing import Optional, Tuple

from . import (
    Quantizer, QuantizerRegistry, QuantizationType,
    QuantizationConfig, QuantizationOutput, QuantizationGranularity
)


@QuantizerRegistry.register(QuantizationType.INT4)
class INT4Quantizer(Quantizer):
    """INT4 åˆ†ç»„é‡åŒ–å™¨
    
    å®ç° GPTQ/AWQ é£æ ¼çš„ INT4 é‡åŒ–ï¼š
    - åˆ†ç»„é‡åŒ–ï¼ˆé»˜è®¤ group_size=128ï¼‰
    - æ”¯æŒå¯¹ç§°å’Œéå¯¹ç§°é‡åŒ–
    - æ”¯æŒ zero-pointï¼ˆéå¯¹ç§°ï¼‰
    """
    
    INT4_MIN = -8
    INT4_MAX = 7
    
    @property
    def quant_type(self) -> QuantizationType:
        return QuantizationType.INT4
    
    def quantize(
        self,
        weight: torch.Tensor,
        config: QuantizationConfig,
    ) -> QuantizationOutput:
        """INT4 åˆ†ç»„é‡åŒ–
        
        Args:
            weight: æƒé‡å¼ é‡ [out_features, in_features]
            config: é‡åŒ–é…ç½®
            
        Returns:
            é‡åŒ–è¾“å‡ºï¼ŒåŒ…å«æ‰“åŒ…åçš„ INT4 æƒé‡
        """
        out_features, in_features = weight.shape
        group_size = config.group_size
        
        # ç¡®ä¿å¯ä»¥æ•´é™¤
        assert in_features % group_size == 0, \
            f"in_features ({in_features}) must be divisible by group_size ({group_size})"
        
        num_groups = in_features // group_size
        
        # Reshape to [out_features, num_groups, group_size]
        grouped = weight.view(out_features, num_groups, group_size)
        
        if config.use_symmetric:
            # å¯¹ç§°é‡åŒ–
            scales, zeros = self._compute_symmetric_params(grouped)
            quantized = self._quantize_symmetric(grouped, scales)
        else:
            # éå¯¹ç§°é‡åŒ–
            scales, zeros = self._compute_asymmetric_params(grouped)
            quantized = self._quantize_asymmetric(grouped, scales, zeros)
        
        # æ‰“åŒ… INT4ï¼ˆ2 ä¸ª INT4 æ‰“åŒ…åˆ° 1 ä¸ª INT8ï¼‰
        packed = self._pack_int4(quantized)
        
        return QuantizationOutput(
            quantized_weight=packed,
            scales=scales,
            zeros=zeros,
            group_size=group_size,
            quant_type=self.quant_type,
        )
    
    def _compute_symmetric_params(
        self,
        grouped: torch.Tensor,
    ) -> Tuple[torch.Tensor, None]:
        """è®¡ç®—å¯¹ç§°é‡åŒ–å‚æ•°
        
        Args:
            grouped: [out_features, num_groups, group_size]
            
        Returns:
            scales: [out_features, num_groups]
            zeros: None (å¯¹ç§°é‡åŒ–ä¸éœ€è¦)
        """
        abs_max = grouped.abs().amax(dim=-1)
        scales = abs_max / self.INT4_MAX
        scales = scales.clamp(min=1e-8)
        return scales, None
    
    def _compute_asymmetric_params(
        self,
        grouped: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """è®¡ç®—éå¯¹ç§°é‡åŒ–å‚æ•°
        
        Args:
            grouped: [out_features, num_groups, group_size]
            
        Returns:
            scales: [out_features, num_groups]
            zeros: [out_features, num_groups] (é›¶ç‚¹ï¼ŒINT4 èŒƒå›´å†…)
        """
        min_val = grouped.amin(dim=-1)
        max_val = grouped.amax(dim=-1)
        
        # scale = (max - min) / 15 (INT4 æœ‰ 16 ä¸ªå€¼)
        scales = (max_val - min_val) / 15.0
        scales = scales.clamp(min=1e-8)
        
        # zero point: round((-min) / scale)
        zeros = torch.round(-min_val / scales).clamp(0, 15).to(torch.int8)
        
        return scales, zeros
    
    def _quantize_symmetric(
        self,
        grouped: torch.Tensor,
        scales: torch.Tensor,
    ) -> torch.Tensor:
        """å¯¹ç§°é‡åŒ–"""
        # [out, groups, group_size] / [out, groups, 1]
        scaled = grouped / scales.unsqueeze(-1)
        quantized = torch.round(scaled).clamp(self.INT4_MIN, self.INT4_MAX)
        return quantized.to(torch.int8)
    
    def _quantize_asymmetric(
        self,
        grouped: torch.Tensor,
        scales: torch.Tensor,
        zeros: torch.Tensor,
    ) -> torch.Tensor:
        """éå¯¹ç§°é‡åŒ–"""
        scaled = grouped / scales.unsqueeze(-1)
        quantized = torch.round(scaled + zeros.unsqueeze(-1).float())
        quantized = quantized.clamp(0, 15)
        return quantized.to(torch.int8)
    
    def _pack_int4(self, quantized: torch.Tensor) -> torch.Tensor:
        """æ‰“åŒ… INT4 åˆ° INT8
        
        2 ä¸ª INT4 å€¼æ‰“åŒ…åˆ° 1 ä¸ª INT8ï¼š
        - ä½ 4 ä½ï¼šç¬¬ä¸€ä¸ª INT4
        - é«˜ 4 ä½ï¼šç¬¬äºŒä¸ª INT4
        """
        out_features, num_groups, group_size = quantized.shape
        assert group_size % 2 == 0
        
        # Reshape to pair up elements
        paired = quantized.view(out_features, num_groups, group_size // 2, 2)
        
        # Pack: low | (high << 4)
        # å…ˆè½¬æ¢åˆ° 0-15 èŒƒå›´ï¼ˆå¯¹äºå¯¹ç§°é‡åŒ–éœ€è¦åŠ  8ï¼‰
        low = (paired[..., 0] + 8) & 0xF
        high = (paired[..., 1] + 8) & 0xF
        
        packed = low | (high << 4)
        return packed.to(torch.uint8)
    
    def dequantize(self, output: QuantizationOutput) -> torch.Tensor:
        """åé‡åŒ– INT4"""
        # è§£åŒ…
        unpacked = self._unpack_int4(output.quantized_weight)
        
        # è·å–å½¢çŠ¶
        scales = output.scales
        out_features, num_groups = scales.shape
        group_size = output.group_size
        
        # Reshape
        unpacked = unpacked.view(out_features, num_groups, group_size).float()
        
        # åé‡åŒ–
        if output.zeros is None:
            # å¯¹ç§°é‡åŒ–
            dequantized = unpacked * scales.unsqueeze(-1)
        else:
            # éå¯¹ç§°é‡åŒ–
            dequantized = (unpacked - output.zeros.unsqueeze(-1).float()) * scales.unsqueeze(-1)
        
        # Reshape back
        return dequantized.view(out_features, -1)
    
    def _unpack_int4(self, packed: torch.Tensor) -> torch.Tensor:
        """è§£åŒ… INT4"""
        # Extract low and high nibbles
        low = (packed & 0xF).to(torch.int8) - 8
        high = ((packed >> 4) & 0xF).to(torch.int8) - 8
        
        # Interleave
        unpacked = torch.stack([low, high], dim=-1).flatten(start_dim=-2)
        return unpacked
```

### 4. æ··åˆç²¾åº¦æ¨ç† (`quantize/mixed_precision.py`)

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Set
import torch
import torch.nn as nn

from . import QuantizationType, QuantizationConfig


@dataclass
class LayerPrecision:
    """å±‚ç²¾åº¦é…ç½®"""
    layer_name: str
    weight_precision: QuantizationType
    activation_precision: QuantizationType
    
    # æ˜¯å¦ä¿æŒé«˜ç²¾åº¦ï¼ˆç”¨äºæ•æ„Ÿå±‚ï¼‰
    keep_high_precision: bool = False


class MixedPrecisionConfig:
    """æ··åˆç²¾åº¦é…ç½®
    
    æ”¯æŒä¸åŒå±‚ä½¿ç”¨ä¸åŒç²¾åº¦ï¼š
    - æ•æ„Ÿå±‚ï¼ˆå¦‚ embedding, lm_headï¼‰ä½¿ç”¨é«˜ç²¾åº¦
    - ä¸­é—´å±‚ä½¿ç”¨ä½ç²¾åº¦
    """
    
    # é»˜è®¤æ•æ„Ÿå±‚æ¨¡å¼
    DEFAULT_SENSITIVE_PATTERNS = [
        "embed",
        "lm_head",
        "norm",
        "layernorm",
    ]
    
    def __init__(
        self,
        default_weight_precision: QuantizationType = QuantizationType.INT4,
        default_activation_precision: QuantizationType = QuantizationType.FP8_E5M2,
        sensitive_patterns: Optional[List[str]] = None,
        layer_configs: Optional[Dict[str, LayerPrecision]] = None,
    ):
        self.default_weight_precision = default_weight_precision
        self.default_activation_precision = default_activation_precision
        self.sensitive_patterns = sensitive_patterns or self.DEFAULT_SENSITIVE_PATTERNS
        self.layer_configs = layer_configs or {}
    
    def get_layer_precision(self, layer_name: str) -> LayerPrecision:
        """è·å–å±‚çš„ç²¾åº¦é…ç½®"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾å¼é…ç½®
        if layer_name in self.layer_configs:
            return self.layer_configs[layer_name]
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…æ•æ„Ÿå±‚æ¨¡å¼
        layer_name_lower = layer_name.lower()
        for pattern in self.sensitive_patterns:
            if pattern in layer_name_lower:
                return LayerPrecision(
                    layer_name=layer_name,
                    weight_precision=QuantizationType.NONE,
                    activation_precision=QuantizationType.NONE,
                    keep_high_precision=True,
                )
        
        # è¿”å›é»˜è®¤ç²¾åº¦
        return LayerPrecision(
            layer_name=layer_name,
            weight_precision=self.default_weight_precision,
            activation_precision=self.default_activation_precision,
            keep_high_precision=False,
        )
    
    def set_layer_precision(
        self,
        layer_name: str,
        weight_precision: QuantizationType,
        activation_precision: Optional[QuantizationType] = None,
    ) -> None:
        """è®¾ç½®ç‰¹å®šå±‚çš„ç²¾åº¦"""
        self.layer_configs[layer_name] = LayerPrecision(
            layer_name=layer_name,
            weight_precision=weight_precision,
            activation_precision=activation_precision or self.default_activation_precision,
        )


class MixedPrecisionQuantizer:
    """æ··åˆç²¾åº¦é‡åŒ–å™¨
    
    å¯¹æ•´ä¸ªæ¨¡å‹åº”ç”¨æ··åˆç²¾åº¦é‡åŒ–ã€‚
    """
    
    def __init__(self, config: MixedPrecisionConfig):
        self.config = config
        self._quantized_layers: Set[str] = set()
    
    def quantize_model(
        self,
        model: nn.Module,
        calibration_data: Optional[torch.Tensor] = None,
    ) -> nn.Module:
        """é‡åŒ–æ•´ä¸ªæ¨¡å‹
        
        Args:
            model: åŸå§‹æ¨¡å‹
            calibration_data: æ ¡å‡†æ•°æ®ï¼ˆç”¨äºæŸäº›é‡åŒ–æ–¹æ³•ï¼‰
            
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        from . import QuantizerRegistry
        
        for name, module in model.named_modules():
            if isinstance(module, nn.Linear):
                precision = self.config.get_layer_precision(name)
                
                if precision.keep_high_precision:
                    continue
                
                # è·å–é‡åŒ–å™¨
                if precision.weight_precision != QuantizationType.NONE:
                    quantizer = QuantizerRegistry.get(precision.weight_precision)
                    
                    # é‡åŒ–æƒé‡
                    quant_config = QuantizationConfig(
                        quant_type=precision.weight_precision,
                    )
                    quant_output = quantizer.quantize(module.weight.data, quant_config)
                    
                    # æ›¿æ¢ä¸ºé‡åŒ–åçš„æƒé‡
                    # æ³¨æ„ï¼šå®é™…å®ç°éœ€è¦åŒ…è£…ä¸ºæ”¯æŒé‡åŒ–æ¨ç†çš„ module
                    module.weight.data = quantizer.dequantize(quant_output)
                    
                    self._quantized_layers.add(name)
        
        return model
    
    def get_quantization_summary(self) -> Dict:
        """è·å–é‡åŒ–æ‘˜è¦"""
        return {
            "total_quantized_layers": len(self._quantized_layers),
            "quantized_layers": list(self._quantized_layers),
            "default_weight_precision": self.config.default_weight_precision.name,
            "default_activation_precision": self.config.default_activation_precision.name,
        }
```

### 5. ç»“æ„åŒ–ç¨€ç– (`sparsity/structured.py`)

```python
from dataclasses import dataclass
from enum import Enum, auto
from typing import Dict, List, Optional, Tuple, Any
import torch
import torch.nn as nn


class SparsityPattern(Enum):
    """ç¨€ç–æ¨¡å¼"""
    UNSTRUCTURED = auto()     # éç»“æ„åŒ–ï¼ˆä»»æ„ä½ç½®ï¼‰
    N_M = auto()              # N:M ç¨€ç–ï¼ˆå¦‚ 2:4ï¼‰
    BLOCK = auto()            # å—ç¨€ç–
    CHANNEL = auto()          # é€šé“ç¨€ç–ï¼ˆå‰ªææ•´ä¸ªé€šé“ï¼‰
    HEAD = auto()             # æ³¨æ„åŠ›å¤´ç¨€ç–


@dataclass
class SparsityConfig:
    """ç¨€ç–é…ç½®"""
    pattern: SparsityPattern
    target_sparsity: float = 0.5     # ç›®æ ‡ç¨€ç–åº¦
    
    # N:M ç¨€ç–å‚æ•°
    n: int = 2  # æ¯ M ä¸ªä¸­ä¿ç•™ N ä¸ª
    m: int = 4
    
    # å—ç¨€ç–å‚æ•°
    block_size: Tuple[int, int] = (32, 32)
    
    # å‰ªæå‚æ•°
    importance_metric: str = "magnitude"  # magnitude, gradient, taylor


@dataclass
class SparsityOutput:
    """ç¨€ç–è¾“å‡º"""
    sparse_weight: torch.Tensor
    mask: torch.Tensor
    actual_sparsity: float
    pattern: SparsityPattern


class StructuredSparseTransform:
    """ç»“æ„åŒ–ç¨€ç–å˜æ¢
    
    æ”¯æŒå¤šç§ç¨€ç–æ¨¡å¼ï¼š
    - N:M ç¨€ç–ï¼šNVIDIA Ampere+ æ”¯æŒçš„ 2:4 ç¨€ç–
    - å—ç¨€ç–ï¼šæ•´å—å‰ªæ
    - é€šé“ç¨€ç–ï¼šå‰ªææ•´ä¸ªè¾“å‡ºé€šé“
    """
    
    def __init__(self, config: SparsityConfig):
        self.config = config
    
    def apply(self, weight: torch.Tensor) -> SparsityOutput:
        """åº”ç”¨ç¨€ç–å˜æ¢
        
        Args:
            weight: æƒé‡å¼ é‡ [out_features, in_features]
            
        Returns:
            ç¨€ç–è¾“å‡º
        """
        if self.config.pattern == SparsityPattern.N_M:
            return self._apply_n_m_sparsity(weight)
        elif self.config.pattern == SparsityPattern.BLOCK:
            return self._apply_block_sparsity(weight)
        elif self.config.pattern == SparsityPattern.CHANNEL:
            return self._apply_channel_sparsity(weight)
        else:
            return self._apply_unstructured_sparsity(weight)
    
    def _apply_n_m_sparsity(self, weight: torch.Tensor) -> SparsityOutput:
        """åº”ç”¨ N:M ç¨€ç–
        
        ä¿ç•™æ¯ M ä¸ªå…ƒç´ ä¸­ç»å¯¹å€¼æœ€å¤§çš„ N ä¸ªã€‚
        """
        n, m = self.config.n, self.config.m
        out_features, in_features = weight.shape
        
        # ç¡®ä¿ in_features å¯è¢« m æ•´é™¤
        assert in_features % m == 0, f"in_features ({in_features}) must be divisible by m ({m})"
        
        # Reshape to [out_features, num_groups, m]
        grouped = weight.view(out_features, -1, m)
        
        # è·å–æ¯ç»„ä¸­ç»å¯¹å€¼æœ€å¤§çš„ n ä¸ªä½ç½®
        _, indices = torch.topk(grouped.abs(), k=n, dim=-1)
        
        # åˆ›å»º mask
        mask = torch.zeros_like(grouped)
        mask.scatter_(-1, indices, 1.0)
        mask = mask.view(out_features, in_features)
        
        # åº”ç”¨ mask
        sparse_weight = weight * mask
        
        actual_sparsity = 1.0 - (mask.sum() / mask.numel()).item()
        
        return SparsityOutput(
            sparse_weight=sparse_weight,
            mask=mask,
            actual_sparsity=actual_sparsity,
            pattern=SparsityPattern.N_M,
        )
    
    def _apply_block_sparsity(self, weight: torch.Tensor) -> SparsityOutput:
        """åº”ç”¨å—ç¨€ç–
        
        ä»¥å›ºå®šå¤§å°çš„å—ä¸ºå•ä½è¿›è¡Œå‰ªæã€‚
        """
        block_h, block_w = self.config.block_size
        out_features, in_features = weight.shape
        
        # è®¡ç®—å—æ•°é‡
        num_blocks_h = out_features // block_h
        num_blocks_w = in_features // block_w
        total_blocks = num_blocks_h * num_blocks_w
        
        # è®¡ç®—æ¯ä¸ªå—çš„é‡è¦æ€§ï¼ˆä½¿ç”¨ L2 èŒƒæ•°ï¼‰
        block_importance = torch.zeros(num_blocks_h, num_blocks_w)
        for i in range(num_blocks_h):
            for j in range(num_blocks_w):
                block = weight[
                    i*block_h:(i+1)*block_h,
                    j*block_w:(j+1)*block_w
                ]
                block_importance[i, j] = block.norm()
        
        # ç¡®å®šè¦ä¿ç•™çš„å—æ•°é‡
        num_keep = int(total_blocks * (1 - self.config.target_sparsity))
        
        # è·å–æœ€é‡è¦çš„å—
        flat_importance = block_importance.view(-1)
        _, top_indices = torch.topk(flat_importance, k=num_keep)
        
        # åˆ›å»º mask
        mask = torch.zeros(out_features, in_features)
        for idx in top_indices:
            i = idx // num_blocks_w
            j = idx % num_blocks_w
            mask[
                i*block_h:(i+1)*block_h,
                j*block_w:(j+1)*block_w
            ] = 1.0
        
        sparse_weight = weight * mask
        actual_sparsity = 1.0 - (mask.sum() / mask.numel()).item()
        
        return SparsityOutput(
            sparse_weight=sparse_weight,
            mask=mask,
            actual_sparsity=actual_sparsity,
            pattern=SparsityPattern.BLOCK,
        )
    
    def _apply_channel_sparsity(self, weight: torch.Tensor) -> SparsityOutput:
        """åº”ç”¨é€šé“ç¨€ç–
        
        å‰ªææ•´ä¸ªè¾“å‡ºé€šé“ã€‚
        """
        out_features, in_features = weight.shape
        
        # è®¡ç®—æ¯ä¸ªè¾“å‡ºé€šé“çš„é‡è¦æ€§
        channel_importance = weight.abs().sum(dim=1)
        
        # ç¡®å®šè¦ä¿ç•™çš„é€šé“æ•°é‡
        num_keep = int(out_features * (1 - self.config.target_sparsity))
        
        # è·å–æœ€é‡è¦çš„é€šé“
        _, top_indices = torch.topk(channel_importance, k=num_keep)
        
        # åˆ›å»º mask
        mask = torch.zeros(out_features, 1)
        mask[top_indices] = 1.0
        mask = mask.expand(-1, in_features)
        
        sparse_weight = weight * mask
        actual_sparsity = 1.0 - (mask.sum() / mask.numel()).item()
        
        return SparsityOutput(
            sparse_weight=sparse_weight,
            mask=mask,
            actual_sparsity=actual_sparsity,
            pattern=SparsityPattern.CHANNEL,
        )
    
    def _apply_unstructured_sparsity(self, weight: torch.Tensor) -> SparsityOutput:
        """åº”ç”¨éç»“æ„åŒ–ç¨€ç–"""
        # è®¡ç®—é˜ˆå€¼
        flat = weight.abs().view(-1)
        k = int(flat.numel() * self.config.target_sparsity)
        threshold = torch.kthvalue(flat, k).values
        
        # åˆ›å»º mask
        mask = (weight.abs() >= threshold).float()
        
        sparse_weight = weight * mask
        actual_sparsity = 1.0 - (mask.sum() / mask.numel()).item()
        
        return SparsityOutput(
            sparse_weight=sparse_weight,
            mask=mask,
            actual_sparsity=actual_sparsity,
            pattern=SparsityPattern.UNSTRUCTURED,
        )
```

### 6. æˆæœ¬æ¨¡å‹ (`cost_model/estimator.py`)

```python
from dataclasses import dataclass
from typing import Dict, List, Optional, Any
from enum import Enum, auto

from ..quantize import QuantizationType
from ..sparsity.structured import SparsityPattern


class AcceleratorType(Enum):
    """åŠ é€Ÿå™¨ç±»å‹"""
    NVIDIA_A100 = auto()
    NVIDIA_H100 = auto()
    HUAWEI_ASCEND_910B = auto()
    CAMBRICON_MLU590 = auto()
    HYGON_DCU = auto()


@dataclass
class AcceleratorSpec:
    """åŠ é€Ÿå™¨è§„æ ¼"""
    name: str
    type: AcceleratorType
    
    # è®¡ç®—èƒ½åŠ›
    fp32_tflops: float
    fp16_tflops: float
    bf16_tflops: float
    int8_tops: float
    fp8_tflops: Optional[float] = None
    
    # å†…å­˜
    hbm_gb: float
    hbm_bandwidth_gbps: float
    
    # ç¨€ç–æ”¯æŒ
    supports_2_4_sparsity: bool = False
    sparse_speedup: float = 1.0  # 2:4 ç¨€ç–åŠ é€Ÿæ¯”
    
    # é‡åŒ–æ”¯æŒ
    supported_quant_types: List[QuantizationType] = None
    
    def __post_init__(self):
        if self.supported_quant_types is None:
            self.supported_quant_types = [
                QuantizationType.NONE,
                QuantizationType.INT8,
            ]


# é¢„å®šä¹‰åŠ é€Ÿå™¨è§„æ ¼
ACCELERATOR_SPECS = {
    AcceleratorType.NVIDIA_A100: AcceleratorSpec(
        name="NVIDIA A100 80GB",
        type=AcceleratorType.NVIDIA_A100,
        fp32_tflops=19.5,
        fp16_tflops=312,
        bf16_tflops=312,
        int8_tops=624,
        fp8_tflops=None,
        hbm_gb=80,
        hbm_bandwidth_gbps=2039,
        supports_2_4_sparsity=True,
        sparse_speedup=2.0,
        supported_quant_types=[
            QuantizationType.NONE,
            QuantizationType.INT8,
            QuantizationType.INT4,
        ],
    ),
    AcceleratorType.NVIDIA_H100: AcceleratorSpec(
        name="NVIDIA H100 80GB",
        type=AcceleratorType.NVIDIA_H100,
        fp32_tflops=67,
        fp16_tflops=990,
        bf16_tflops=990,
        int8_tops=1980,
        fp8_tflops=1980,
        hbm_gb=80,
        hbm_bandwidth_gbps=3350,
        supports_2_4_sparsity=True,
        sparse_speedup=2.0,
        supported_quant_types=[
            QuantizationType.NONE,
            QuantizationType.FP8_E4M3,
            QuantizationType.FP8_E5M2,
            QuantizationType.INT8,
            QuantizationType.INT4,
        ],
    ),
    AcceleratorType.HUAWEI_ASCEND_910B: AcceleratorSpec(
        name="Huawei Ascend 910B",
        type=AcceleratorType.HUAWEI_ASCEND_910B,
        fp32_tflops=8,  # ä¼°è®¡å€¼
        fp16_tflops=320,
        bf16_tflops=320,
        int8_tops=640,
        hbm_gb=64,
        hbm_bandwidth_gbps=1200,
        supports_2_4_sparsity=False,
        supported_quant_types=[
            QuantizationType.NONE,
            QuantizationType.INT8,
        ],
    ),
}


@dataclass
class InferenceCost:
    """æ¨ç†æˆæœ¬ä¼°ç®—"""
    # æ—¶é—´æˆæœ¬
    compute_time_ms: float
    memory_time_ms: float
    total_time_ms: float
    
    # èµ„æºåˆ©ç”¨
    compute_utilization: float  # è®¡ç®—åˆ©ç”¨ç‡ (0-1)
    memory_bandwidth_utilization: float  # å¸¦å®½åˆ©ç”¨ç‡ (0-1)
    
    # å†…å­˜å ç”¨
    weight_memory_mb: float
    activation_memory_mb: float
    kv_cache_memory_mb: float
    total_memory_mb: float
    
    # ååé‡
    tokens_per_second: float


class CostEstimator:
    """æˆæœ¬ä¼°ç®—å™¨
    
    ä¼°ç®—ä¸åŒé…ç½®ä¸‹çš„æ¨ç†æˆæœ¬ï¼Œç”¨äºæŒ‡å¯¼ä¼˜åŒ–ç­–ç•¥é€‰æ‹©ã€‚
    """
    
    def __init__(self, accelerator: AcceleratorSpec):
        self.accelerator = accelerator
    
    def estimate_linear_layer(
        self,
        in_features: int,
        out_features: int,
        batch_size: int,
        seq_len: int,
        quant_type: QuantizationType = QuantizationType.NONE,
        sparsity: float = 0.0,
        sparsity_pattern: Optional[SparsityPattern] = None,
    ) -> InferenceCost:
        """ä¼°ç®—çº¿æ€§å±‚æ¨ç†æˆæœ¬
        
        Args:
            in_features: è¾“å…¥ç‰¹å¾æ•°
            out_features: è¾“å‡ºç‰¹å¾æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            seq_len: åºåˆ—é•¿åº¦
            quant_type: é‡åŒ–ç±»å‹
            sparsity: ç¨€ç–åº¦ (0-1)
            sparsity_pattern: ç¨€ç–æ¨¡å¼
            
        Returns:
            æ¨ç†æˆæœ¬ä¼°ç®—
        """
        # è®¡ç®— FLOPs
        total_tokens = batch_size * seq_len
        flops = 2 * total_tokens * in_features * out_features
        
        # åº”ç”¨ç¨€ç–åŠ é€Ÿ
        effective_sparsity = sparsity
        if sparsity_pattern == SparsityPattern.N_M and self.accelerator.supports_2_4_sparsity:
            flops = flops * (1 - sparsity) / self.accelerator.sparse_speedup
        else:
            flops = flops * (1 - sparsity)
        
        # è·å–è®¡ç®—ååé‡
        compute_tflops = self._get_compute_tflops(quant_type)
        
        # è®¡ç®—æ—¶é—´
        compute_time_ms = (flops / (compute_tflops * 1e12)) * 1000
        
        # å†…å­˜å ç”¨
        weight_bytes = self._get_weight_bytes(in_features, out_features, quant_type)
        weight_memory_mb = weight_bytes / (1024 ** 2)
        
        # æ¿€æ´»å†…å­˜
        activation_bytes = total_tokens * (in_features + out_features) * 2  # FP16
        activation_memory_mb = activation_bytes / (1024 ** 2)
        
        # å†…å­˜è®¿é—®æ—¶é—´
        total_memory_bytes = weight_bytes + activation_bytes
        memory_time_ms = (total_memory_bytes / (self.accelerator.hbm_bandwidth_gbps * 1e9)) * 1000
        
        # æ€»æ—¶é—´ï¼ˆå–è®¡ç®—å’Œå†…å­˜çš„æœ€å¤§å€¼ï¼Œå› ä¸ºå¯èƒ½é‡å ï¼‰
        total_time_ms = max(compute_time_ms, memory_time_ms)
        
        # åˆ©ç”¨ç‡
        compute_utilization = compute_time_ms / total_time_ms if total_time_ms > 0 else 0
        memory_bandwidth_utilization = memory_time_ms / total_time_ms if total_time_ms > 0 else 0
        
        return InferenceCost(
            compute_time_ms=compute_time_ms,
            memory_time_ms=memory_time_ms,
            total_time_ms=total_time_ms,
            compute_utilization=compute_utilization,
            memory_bandwidth_utilization=memory_bandwidth_utilization,
            weight_memory_mb=weight_memory_mb,
            activation_memory_mb=activation_memory_mb,
            kv_cache_memory_mb=0,  # çº¿æ€§å±‚ä¸æ¶‰åŠ KV cache
            total_memory_mb=weight_memory_mb + activation_memory_mb,
            tokens_per_second=(total_tokens / total_time_ms * 1000) if total_time_ms > 0 else 0,
        )
    
    def _get_compute_tflops(self, quant_type: QuantizationType) -> float:
        """è·å–æŒ‡å®šé‡åŒ–ç±»å‹çš„è®¡ç®—ååé‡"""
        if quant_type == QuantizationType.NONE:
            return self.accelerator.fp16_tflops
        elif quant_type in (QuantizationType.FP8_E4M3, QuantizationType.FP8_E5M2):
            return self.accelerator.fp8_tflops or self.accelerator.fp16_tflops
        elif quant_type in (QuantizationType.INT8, QuantizationType.INT4):
            return self.accelerator.int8_tops
        else:
            return self.accelerator.fp16_tflops
    
    def _get_weight_bytes(
        self,
        in_features: int,
        out_features: int,
        quant_type: QuantizationType,
    ) -> int:
        """è®¡ç®—æƒé‡å†…å­˜å ç”¨"""
        num_elements = in_features * out_features
        
        bytes_per_element = {
            QuantizationType.NONE: 2,      # FP16
            QuantizationType.FP8_E4M3: 1,
            QuantizationType.FP8_E5M2: 1,
            QuantizationType.INT8: 1,
            QuantizationType.INT4: 0.5,
            QuantizationType.NF4: 0.5,
        }.get(quant_type, 2)
        
        return int(num_elements * bytes_per_element)
    
    def recommend_configuration(
        self,
        model_size_params: int,
        max_batch_size: int,
        max_seq_len: int,
        target_latency_ms: Optional[float] = None,
        target_throughput_tps: Optional[float] = None,
    ) -> Dict[str, Any]:
        """æ¨èæœ€ä½³é…ç½®
        
        æ ¹æ®ç›®æ ‡å»¶è¿Ÿ/ååé‡æ¨èé‡åŒ–å’Œç¨€ç–é…ç½®ã€‚
        """
        recommendations = {
            "accelerator": self.accelerator.name,
            "supported_quant_types": [q.name for q in self.accelerator.supported_quant_types],
            "supports_2_4_sparsity": self.accelerator.supports_2_4_sparsity,
        }
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒ FP8
        if QuantizationType.FP8_E4M3 in self.accelerator.supported_quant_types:
            recommendations["recommended_weight_quant"] = "FP8_E4M3"
            recommendations["recommended_activation_quant"] = "FP8_E5M2"
        elif QuantizationType.INT4 in self.accelerator.supported_quant_types:
            recommendations["recommended_weight_quant"] = "INT4"
            recommendations["recommended_activation_quant"] = "INT8"
        else:
            recommendations["recommended_weight_quant"] = "INT8"
            recommendations["recommended_activation_quant"] = "NONE"
        
        # æ£€æŸ¥ç¨€ç–
        if self.accelerator.supports_2_4_sparsity:
            recommendations["recommended_sparsity"] = "N_M (2:4)"
            recommendations["expected_speedup"] = "~2x"
        else:
            recommendations["recommended_sparsity"] = "NONE"
        
        return recommendations
```

---

## å•å…ƒæµ‹è¯•è¦æ±‚

åˆ›å»º `tests/unit/test_accel.py`ï¼š

```python
import pytest
import torch
from sageLLM.accel.quantize import (
    QuantizerRegistry, QuantizationType, QuantizationConfig,
    QuantizationGranularity
)
from sageLLM.accel.quantize.fp8 import FP8E4M3Quantizer
from sageLLM.accel.quantize.int4 import INT4Quantizer
from sageLLM.accel.sparsity.structured import (
    StructuredSparseTransform, SparsityConfig, SparsityPattern
)
from sageLLM.accel.cost_model.estimator import (
    CostEstimator, ACCELERATOR_SPECS, AcceleratorType
)


class TestFP8Quantizer:
    """FP8 é‡åŒ–æµ‹è¯•"""
    
    def test_fp8_e4m3_quantize_dequantize(self):
        """æµ‹è¯• FP8 E4M3 é‡åŒ–/åé‡åŒ–"""
        quantizer = FP8E4M3Quantizer()
        weight = torch.randn(256, 512)
        
        config = QuantizationConfig(
            quant_type=QuantizationType.FP8_E4M3,
            granularity=QuantizationGranularity.PER_TENSOR,
        )
        
        output = quantizer.quantize(weight, config)
        reconstructed = quantizer.dequantize(output)
        
        # æ£€æŸ¥é‡å»ºè¯¯å·®
        error = (weight - reconstructed).abs().mean()
        assert error < 0.1  # å…è®¸ä¸€å®šè¯¯å·®


class TestINT4Quantizer:
    """INT4 é‡åŒ–æµ‹è¯•"""
    
    def test_int4_symmetric_quantize(self):
        """æµ‹è¯• INT4 å¯¹ç§°é‡åŒ–"""
        quantizer = INT4Quantizer()
        weight = torch.randn(256, 512)
        
        config = QuantizationConfig(
            quant_type=QuantizationType.INT4,
            group_size=128,
            use_symmetric=True,
        )
        
        output = quantizer.quantize(weight, config)
        
        assert output.quantized_weight.dtype == torch.uint8
        assert output.zeros is None  # å¯¹ç§°é‡åŒ–æ—  zero point
    
    def test_int4_asymmetric_quantize(self):
        """æµ‹è¯• INT4 éå¯¹ç§°é‡åŒ–"""
        quantizer = INT4Quantizer()
        weight = torch.randn(256, 512) + 1.0  # éå¯¹ç§°åˆ†å¸ƒ
        
        config = QuantizationConfig(
            quant_type=QuantizationType.INT4,
            group_size=128,
            use_symmetric=False,
        )
        
        output = quantizer.quantize(weight, config)
        
        assert output.zeros is not None


class TestStructuredSparsity:
    """ç»“æ„åŒ–ç¨€ç–æµ‹è¯•"""
    
    def test_2_4_sparsity(self):
        """æµ‹è¯• 2:4 ç¨€ç–"""
        config = SparsityConfig(
            pattern=SparsityPattern.N_M,
            n=2,
            m=4,
        )
        transform = StructuredSparseTransform(config)
        
        weight = torch.randn(256, 512)
        output = transform.apply(weight)
        
        # æ£€æŸ¥ç¨€ç–åº¦çº¦ä¸º 50%
        assert abs(output.actual_sparsity - 0.5) < 0.01
    
    def test_block_sparsity(self):
        """æµ‹è¯•å—ç¨€ç–"""
        config = SparsityConfig(
            pattern=SparsityPattern.BLOCK,
            target_sparsity=0.75,
            block_size=(32, 32),
        )
        transform = StructuredSparseTransform(config)
        
        weight = torch.randn(256, 512)
        output = transform.apply(weight)
        
        # æ£€æŸ¥ç¨€ç–åº¦æ¥è¿‘ç›®æ ‡
        assert abs(output.actual_sparsity - 0.75) < 0.1


class TestCostEstimator:
    """æˆæœ¬ä¼°ç®—æµ‹è¯•"""
    
    def test_h100_linear_cost(self):
        """æµ‹è¯• H100 çº¿æ€§å±‚æˆæœ¬ä¼°ç®—"""
        spec = ACCELERATOR_SPECS[AcceleratorType.NVIDIA_H100]
        estimator = CostEstimator(spec)
        
        cost = estimator.estimate_linear_layer(
            in_features=4096,
            out_features=4096,
            batch_size=32,
            seq_len=1024,
            quant_type=QuantizationType.NONE,
        )
        
        assert cost.total_time_ms > 0
        assert cost.tokens_per_second > 0
    
    def test_recommendation(self):
        """æµ‹è¯•é…ç½®æ¨è"""
        spec = ACCELERATOR_SPECS[AcceleratorType.NVIDIA_H100]
        estimator = CostEstimator(spec)
        
        rec = estimator.recommend_configuration(
            model_size_params=7_000_000_000,
            max_batch_size=32,
            max_seq_len=4096,
        )
        
        assert "FP8" in rec["recommended_weight_quant"]
```

---

## æ¥å£çº¦å®š

### è¾“å…¥æ¥å£

| æ¥å£ | æ¥æº | è¯´æ˜ |
|------|------|------|
| `torch.Tensor` | æ¨¡å‹æƒé‡ | åŸå§‹ FP16/BF16 æƒé‡ |
| `AcceleratorSpec` | backends | ç¡¬ä»¶è§„æ ¼ |

### è¾“å‡ºæ¥å£

| æ¥å£ | ç›®æ ‡ | è¯´æ˜ |
|------|------|------|
| `QuantizationOutput` | runtime | é‡åŒ–åçš„æƒé‡ |
| `SparsityOutput` | runtime | ç¨€ç–æƒé‡+mask |
| `InferenceCost` | scheduler | æˆæœ¬ä¼°ç®—ç»“æœ |

---

## éªŒæ”¶æ ‡å‡†

- [ ] FP8 é‡åŒ–ï¼šE4M3/E5M2 æ ¼å¼æ­£ç¡®å®ç°
- [ ] INT4 é‡åŒ–ï¼šå¯¹ç§°/éå¯¹ç§°é‡åŒ–è¯¯å·® < 5%ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ > 0.95ï¼‰
- [ ] 2:4 ç¨€ç–ï¼šç²¾ç¡®å®ç° 50% ç¨€ç–åº¦
- [ ] æˆæœ¬æ¨¡å‹ï¼šä¼°ç®—è¯¯å·® < 20%ï¼ˆä¸å®é™…æµ‹é‡æ¯”è¾ƒï¼‰
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%
- [ ] ä»£ç é€šè¿‡ `ruff check` å’Œ `mypy`

---

## è¾“å‡ºç‰©æ¸…å•

```
accel/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ quantize/
â”‚   â”œâ”€â”€ __init__.py           # âœ… åè®®å®šä¹‰
â”‚   â”œâ”€â”€ fp8.py                # âœ… å®Œæ•´å®ç°
â”‚   â”œâ”€â”€ int4.py               # âœ… å®Œæ•´å®ç°
â”‚   â””â”€â”€ mixed_precision.py    # âœ… å®Œæ•´å®ç°
â”œâ”€â”€ sparsity/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ structured.py         # âœ… å®Œæ•´å®ç°
â””â”€â”€ cost_model/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ estimator.py          # âœ… å®Œæ•´å®ç°

tests/unit/
â””â”€â”€ test_accel.py             # âœ… æµ‹è¯•æ–‡ä»¶
```
