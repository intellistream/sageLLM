#!/usr/bin/env python3
# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the SAGE project

"""
å®Œæ•´çš„ Control Plane æ¼”ç¤ºç¤ºä¾‹

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å®Œæ•´çš„ä½¿ç”¨æµç¨‹ï¼Œä¸éœ€è¦å®é™…çš„ vLLM å®ä¾‹è¿è¡Œã€‚
ä½¿ç”¨ mock æ¥æ¨¡æ‹Ÿ vLLM HTTP å“åº”ã€‚
"""

import asyncio
import logging

from control_plane import (
    ControlPlaneManager,
    ExecutionInstance,
    ExecutionInstanceType,
    RequestMetadata,
    RequestPriority,
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

logger = logging.getLogger(__name__)


async def demo_basic_usage():
    """æ¼”ç¤º 1: åŸºç¡€ä½¿ç”¨æµç¨‹"""
    logger.info("\n" + "=" * 70)
    logger.info("æ¼”ç¤º 1: åŸºç¡€ Control Plane ä½¿ç”¨æµç¨‹")
    logger.info("=" * 70)

    # 1. åˆ›å»º Control Plane
    cp = ControlPlaneManager(
        scheduling_policy="fifo",
        routing_strategy="load_balanced",
        enable_pd_separation=False,
    )

    # 2. æ³¨å†Œ vLLM å®ä¾‹ï¼ˆæ¨¡æ‹Ÿæœ¬åœ° GPUï¼‰
    for i in range(2):
        instance = ExecutionInstance(
            instance_id=f"gpu-{i}",
            host="localhost",
            port=8000 + i,
            model_name="meta-llama/Llama-2-7b",
            tensor_parallel_size=1,
            gpu_count=1,
        )
        cp.register_instance(instance)

    logger.info(f"âœ“ å·²æ³¨å†Œ {len(cp.executor.get_all_instances())} ä¸ªå®ä¾‹")

    # 3. æäº¤è¯·æ±‚
    requests = []
    for i in range(5):
        req = RequestMetadata(
            request_id=f"demo-req-{i}",
            prompt=f"æ¼”ç¤ºè¯·æ±‚ {i}: è§£é‡Šäººå·¥æ™ºèƒ½",
            max_tokens=100,
            priority=RequestPriority.NORMAL,
        )
        requests.append(req)
        await cp.submit_request(req)

    logger.info(f"âœ“ å·²æäº¤ {len(requests)} ä¸ªè¯·æ±‚åˆ°é˜Ÿåˆ—")
    logger.info(f"âœ“ é˜Ÿåˆ—å¤§å°: {len(cp.pending_queue)}")

    # 4. æŸ¥çœ‹æ’é˜Ÿçš„è¯·æ±‚
    logger.info("\næ’é˜Ÿçš„è¯·æ±‚:")
    for i, req in enumerate(list(cp.pending_queue)[:3], 1):
        logger.info(f"  {i}. {req.request_id} (ä¼˜å…ˆçº§: {req.priority.name})")

    # 5. è·å–æŒ‡æ ‡
    metrics = cp.get_metrics()
    logger.info("\nå½“å‰æŒ‡æ ‡:")
    logger.info(f"  - æ´»è·ƒè¯·æ±‚: {metrics.active_requests}")
    logger.info(f"  - æ’é˜Ÿè¯·æ±‚: {metrics.queued_requests}")
    logger.info(f"  - å·²å®Œæˆ: {metrics.completed_requests}")
    logger.info(f"  - SLO è¾¾æ ‡ç‡: {metrics.slo_compliance_rate:.1f}%")


async def demo_priority_scheduling():
    """æ¼”ç¤º 2: ä¼˜å…ˆçº§è°ƒåº¦"""
    logger.info("\n" + "=" * 70)
    logger.info("æ¼”ç¤º 2: ä¼˜å…ˆçº§è°ƒåº¦ç­–ç•¥")
    logger.info("=" * 70)

    cp = ControlPlaneManager(
        scheduling_policy="priority",
        enable_pd_separation=False,
    )

    # æ³¨å†Œå®ä¾‹
    instance = ExecutionInstance(
        instance_id="priority-gpu",
        host="localhost",
        port=8000,
        model_name="meta-llama/Llama-2-7b",
        gpu_count=1,
    )
    cp.register_instance(instance)

    # æäº¤ä¸åŒä¼˜å…ˆçº§çš„è¯·æ±‚
    priorities = [
        (RequestPriority.LOW, "ä½ä¼˜å…ˆçº§ä»»åŠ¡"),
        (RequestPriority.CRITICAL, "ç´§æ€¥ä»»åŠ¡"),
        (RequestPriority.NORMAL, "æ™®é€šä»»åŠ¡"),
        (RequestPriority.HIGH, "é«˜ä¼˜å…ˆçº§ä»»åŠ¡"),
    ]

    for priority, desc in priorities:
        req = RequestMetadata(
            request_id=f"priority-{priority.name}",
            prompt=desc,
            priority=priority,
            max_tokens=50,
        )
        await cp.submit_request(req)
        logger.info(f"âœ“ æäº¤: {desc} (ä¼˜å…ˆçº§: {priority.name})")

    # å±•ç¤ºè°ƒåº¦é¡ºåº
    logger.info("\næ’é˜Ÿçš„è¯·æ±‚åˆ—è¡¨:")
    for i, req in enumerate(list(cp.pending_queue), 1):
        logger.info(f"  {i}. {req.request_id} - ä¼˜å…ˆçº§: {req.priority.name}")

    # ä½¿ç”¨ prioritize æ–¹æ³•æ’åº
    logger.info("\nç»è¿‡ä¼˜å…ˆçº§æ’åºå:")
    sorted_reqs = cp.scheduling_policy.prioritize(list(cp.pending_queue))
    for i, req in enumerate(sorted_reqs, 1):
        logger.info(f"  {i}. {req.request_id} - {req.priority.name}")


async def demo_slo_aware_scheduling():
    """æ¼”ç¤º 3: SLO æ„ŸçŸ¥è°ƒåº¦"""
    logger.info("\n" + "=" * 70)
    logger.info("æ¼”ç¤º 3: SLO æ„ŸçŸ¥è°ƒåº¦ï¼ˆå»¶è¿Ÿä¿è¯ï¼‰")
    logger.info("=" * 70)

    cp = ControlPlaneManager(
        scheduling_policy="slo_aware",
        enable_pd_separation=False,
    )

    instance = ExecutionInstance(
        instance_id="slo-gpu",
        host="localhost",
        port=8000,
        model_name="meta-llama/Llama-2-7b",
        gpu_count=1,
    )
    cp.register_instance(instance)

    # æäº¤å¸¦ SLO çš„è¯·æ±‚
    slo_configs = [
        (500, "è¶…ä½å»¶è¿Ÿè¦æ±‚"),
        (2000, "æ™®é€šå»¶è¿Ÿè¦æ±‚"),
        (1000, "ä¸­ç­‰å»¶è¿Ÿè¦æ±‚"),
        (None, "æ—  SLO è¦æ±‚"),
    ]

    for slo_ms, desc in slo_configs:
        req = RequestMetadata(
            request_id=f"slo-{slo_ms or 'none'}",
            prompt=desc,
            slo_deadline_ms=slo_ms,
            max_tokens=100,
        )
        await cp.submit_request(req)
        slo_str = f"{slo_ms}ms" if slo_ms else "æ— é™åˆ¶"
        logger.info(f"âœ“ æäº¤: {desc} (SLO: {slo_str})")

    logger.info("\nç»è¿‡ SLO æ’åºåï¼ˆç´§æ€¥ä¼˜å…ˆï¼‰:")
    sorted_reqs = cp.scheduling_policy.prioritize(list(cp.pending_queue))
    for i, req in enumerate(sorted_reqs, 1):
        slo_str = f"{req.slo_deadline_ms}ms" if req.slo_deadline_ms else "æ— "
        logger.info(f"  {i}. {req.request_id} - SLO: {slo_str}")


async def demo_pd_separation():
    """æ¼”ç¤º 4: Prefilling/Decoding åˆ†ç¦»ä¼˜åŒ–"""
    logger.info("\n" + "=" * 70)
    logger.info("æ¼”ç¤º 4: PD åˆ†ç¦»ä¼˜åŒ–ï¼ˆæå‡ 50-80% ååï¼‰")
    logger.info("=" * 70)

    cp = ControlPlaneManager(
        scheduling_policy="adaptive",
        enable_pd_separation=True,
    )

    # æ³¨å†Œä¸“é—¨çš„ Prefilling å®ä¾‹ï¼ˆé«˜ TPï¼‰
    prefill_instance = ExecutionInstance(
        instance_id="prefill-tp4",
        host="localhost",
        port=8001,
        model_name="meta-llama/Llama-2-7b",
        tensor_parallel_size=4,
        gpu_count=4,
        instance_type=ExecutionInstanceType.PREFILLING,
    )
    cp.register_instance(prefill_instance)
    logger.info("âœ“ æ³¨å†Œ Prefilling å®ä¾‹ (TP=4, ä¼˜åŒ–ååé‡)")

    # æ³¨å†Œä¸“é—¨çš„ Decoding å®ä¾‹ï¼ˆä½ TPï¼‰
    decode_instance = ExecutionInstance(
        instance_id="decode-tp1",
        host="localhost",
        port=8002,
        model_name="meta-llama/Llama-2-7b",
        tensor_parallel_size=1,
        gpu_count=1,
        instance_type=ExecutionInstanceType.DECODING,
    )
    cp.register_instance(decode_instance)
    logger.info("âœ“ æ³¨å†Œ Decoding å®ä¾‹ (TP=1, ä¼˜åŒ–å»¶è¿Ÿ)")

    # æäº¤ä¸åŒç±»å‹çš„è¯·æ±‚
    requests_config = [
        ("é•¿æ–‡æ¡£åˆ†æ", "A" * 2000, "é•¿è¾“å…¥ â†’ Prefilling å®ä¾‹"),
        ("ç®€çŸ­èŠå¤©", "ä½ å¥½", "çŸ­è¾“å…¥ â†’ Decoding å®ä¾‹"),
        ("ä»£ç å®¡æŸ¥", "B" * 1500, "ä¸­é•¿è¾“å…¥ â†’ Prefilling å®ä¾‹"),
        ("å¿«é€Ÿé—®ç­”", "ä»€ä¹ˆæ˜¯AI?", "çŸ­è¾“å…¥ â†’ Decoding å®ä¾‹"),
    ]

    logger.info("\nè¯·æ±‚è·¯ç”±å†³ç­–:")
    for desc, prompt, expected in requests_config:
        req = RequestMetadata(
            request_id=f"pd-{desc}",
            prompt=prompt,
            max_tokens=100,
        )

        # ä½¿ç”¨ PD router å†³å®šè·¯ç”±ç±»å‹
        if cp.pd_router:
            phase = cp.pd_router.determine_request_phase(req)
            input_len = len(prompt) if prompt else 0
            logger.info(f"  {desc}: è¾“å…¥é•¿åº¦={input_len}")
            logger.info(f"    è·¯ç”±å†³ç­–: {phase.value if phase else 'GENERAL'}")
            logger.info(f"    é¢„æœŸ: {expected}")


async def demo_multi_instance():
    """æ¼”ç¤º 5: å¤šå®ä¾‹è´Ÿè½½å‡è¡¡"""
    logger.info("\n" + "=" * 70)
    logger.info("æ¼”ç¤º 5: å¤šå®ä¾‹è´Ÿè½½å‡è¡¡")
    logger.info("=" * 70)

    cp = ControlPlaneManager(
        scheduling_policy="adaptive",
        routing_strategy="load_balanced",
        enable_pd_separation=False,
    )

    # æ³¨å†Œå¤šä¸ªä¸åŒé…ç½®çš„å®ä¾‹
    instances_config = [
        ("local-gpu-0", "localhost", 8000, 1, "æœ¬åœ° GPU 0"),
        ("local-gpu-1", "localhost", 8001, 1, "æœ¬åœ° GPU 1"),
        ("remote-gpu-0", "192.168.1.100", 8000, 2, "è¿œç¨‹ GPU (TP=2)"),
        ("remote-gpu-1", "192.168.1.100", 8001, 4, "è¿œç¨‹ GPU (TP=4)"),
    ]

    for inst_id, host, port, tp, desc in instances_config:
        instance = ExecutionInstance(
            instance_id=inst_id,
            host=host,
            port=port,
            model_name="meta-llama/Llama-2-7b",
            tensor_parallel_size=tp,
            gpu_count=tp,
        )
        cp.register_instance(instance)
        logger.info(f"âœ“ æ³¨å†Œ: {desc} ({host}:{port}, TP={tp})")

    # å±•ç¤ºæ‰€æœ‰å®ä¾‹çŠ¶æ€
    logger.info("\nå®ä¾‹åˆ—è¡¨:")
    for inst in cp.executor.get_all_instances():
        logger.info(f"  - {inst.instance_id}: {inst.host}:{inst.port}")
        logger.info(f"    TP={inst.tensor_parallel_size}, GPUs={inst.gpu_count}")
        logger.info(f"    è´Ÿè½½={inst.current_load:.1%}, æ´»è·ƒè¯·æ±‚={inst.active_requests}")

    # æ¨¡æ‹Ÿæ‰¹é‡è¯·æ±‚
    logger.info("\næäº¤ 10 ä¸ªè¯·æ±‚ï¼Œè§‚å¯Ÿè´Ÿè½½å‡è¡¡...")
    for i in range(10):
        req = RequestMetadata(
            request_id=f"batch-{i}",
            prompt=f"è¯·æ±‚ {i}",
            max_tokens=100,
        )
        await cp.submit_request(req)

    logger.info(f"âœ“ æ€»å…± {len(cp.pending_queue)} ä¸ªè¯·æ±‚åœ¨é˜Ÿåˆ—ä¸­")


async def main():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("\n" + "ğŸš€" * 35)
    print("   sageLLM Control Plane å®Œæ•´æ¼”ç¤º")
    print("ğŸš€" * 35 + "\n")

    # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
    await demo_basic_usage()
    await asyncio.sleep(0.5)

    await demo_priority_scheduling()
    await asyncio.sleep(0.5)

    await demo_slo_aware_scheduling()
    await asyncio.sleep(0.5)

    await demo_pd_separation()
    await asyncio.sleep(0.5)

    await demo_multi_instance()

    print("\n" + "âœ…" * 35)
    print("   æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
    print("âœ…" * 35 + "\n")

    print("\nğŸ“ å…³é”®è¦ç‚¹:")
    print("  1. Control Plane æ˜¯ HTTP å®¢æˆ·ç«¯ï¼Œç»Ÿä¸€ç®¡ç†æ‰€æœ‰ vLLM å®ä¾‹")
    print("  2. æ”¯æŒ 5 ç§è°ƒåº¦ç­–ç•¥: FIFO, Priority, SLO, Cost, Adaptive")
    print("  3. PD åˆ†ç¦»å¯æå‡ 50-80% ååï¼Œé™ä½ 50-60% å»¶è¿Ÿ")
    print("  4. æ”¯æŒå¤šå®ä¾‹è´Ÿè½½å‡è¡¡å’Œæ™ºèƒ½è·¯ç”±")
    print("  5. æœ¬åœ°å’Œè¿œç¨‹ GPU ç»Ÿä¸€é€æ˜è®¿é—®\n")


if __name__ == "__main__":
    asyncio.run(main())
